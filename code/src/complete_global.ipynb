{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b29f50",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e92367ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from data_ingestion import DataIngestion\n",
    "from preprocessing import Preprocessor\n",
    "from prediction_manager import PredictionManager\n",
    "from local_model_manager import LocalModelManager\n",
    "from evaluation import Evaluator\n",
    "from utilities import show_compact\n",
    "import time\n",
    "import json\n",
    "from random import sample\n",
    "from dtaidistance import dtw\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbee63",
   "metadata": {},
   "source": [
    "# CREATE SPARK SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fdb3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"testingglobal\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed2f1b",
   "metadata": {},
   "source": [
    "# CREATE CLASS INSTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "183c6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"databricks_data_path\": \"/mnt/2025-team6/fulldataset_ECG5000.csv\",\n",
    "    \"local_data_path\": \"/fulldataset_ECG5000.csv\",\n",
    "    \"label_col\": \"label\",\n",
    "    \"data_percentage\": 1.0,\n",
    "    \"min_number_iterarations\": 2,\n",
    "\n",
    "    \"local_model_config\": {\n",
    "        \"test_local_model\" : True,\n",
    "        \"num_partitions\": 10,  \n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": None,  \n",
    "            \"min_samples_split\": 5,  # From ProximityTree default\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "        \"forest_params\": {\n",
    "            \"random_state\": 123,\n",
    "            \"n_jobs\": -1  # Use all available cores\n",
    "            }\n",
    "    },\n",
    "    \"global_model_config\": {\n",
    "        \"test_local_model\" : False,\n",
    "        \"num_partitions\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "ingestion_config = {\n",
    "                \"data_path\":r\"C:\\Users\\benat\\OneDrive\\0. MSc MLiS\\0. GitHub Repositories\\BigDataProject_repo\\fulldataset_ECG5000.csv\",\n",
    "                \"data_percentage\": config.get(\"data_percentage\", 0.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4290bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion = DataIngestion(spark=spark, config=ingestion_config)\n",
    "preprocessor = Preprocessor(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f2097",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44ebe289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repartition_sparkdf(df, num_partitions):\n",
    "    rdd = df.rdd\n",
    "    rdd = rdd.repartition(num_partitions)\n",
    "    rdd = rdd.mapPartitionsWithIndex(\n",
    "            lambda idx, iter: [{**row.asDict(), \"partition_id\": idx} for row in iter]\n",
    "        )\n",
    "    return rdd\n",
    "\n",
    "def choose_exemplars(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    if not partition_data:\n",
    "        return iter([])\n",
    "    \n",
    "    # Group data by class\n",
    "    grouped_data_by_class = {}\n",
    "    for row in partition_data:\n",
    "        label = row.get('label')\n",
    "        if label is not None:\n",
    "            grouped_data_by_class.setdefault(label, []).append(row)\n",
    "    \n",
    "    # Select one exemplar per class and track exemplar-to-label mapping\n",
    "    chosen_exemplars = []\n",
    "    exemplar_labels = {}  # Map to track exemplar labels\n",
    "    \n",
    "    for label, instances in grouped_data_by_class.items():\n",
    "        if instances:  # Ensure there are instances for the class\n",
    "            exemplar = sample(instances, 1)[0]\n",
    "            chosen_exemplars.append((exemplar['time_series'], label))  # Store tuple of (time_series, label)\n",
    "    \n",
    "    # Remove chosen exemplars from the working data\n",
    "    exemplar_time_series = [ex[0] for ex in chosen_exemplars]\n",
    "    filtered_partition = [\n",
    "        row for row in partition_data\n",
    "        if row['time_series'] not in exemplar_time_series\n",
    "    ]\n",
    "    \n",
    "    # Return rows with individual exemplar columns and their labels\n",
    "    result = []\n",
    "    for row in filtered_partition:\n",
    "        new_row = {**row}\n",
    "        # Add each exemplar and its label as columns\n",
    "        for i, (exemplar, label) in enumerate(chosen_exemplars, 1):\n",
    "            new_row[f\"exemplar_{i}\"] = exemplar\n",
    "            new_row[f\"exemplar_{i}_label\"] = label\n",
    "        result.append(new_row)\n",
    "    \n",
    "    return iter(result)\n",
    "\n",
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row.get('time_series', [])\n",
    "        \n",
    "        # Check for individual exemplar columns (exemplar_1, exemplar_2, etc.)\n",
    "        exemplar_columns = {k: v for k, v in row.items() \n",
    "                           if k.startswith('exemplar_') and not k.endswith('_label') and isinstance(v, list)}\n",
    "        \n",
    "        if not exemplar_columns:\n",
    "            # Try to get exemplars from the 'exemplars' list if individual columns aren't found\n",
    "            exemplars = row.get('exemplars', [])\n",
    "            if not exemplars:\n",
    "                continue  # Skip if no exemplars found\n",
    "            \n",
    "            # Calculate DTW distances for each exemplar in the list\n",
    "            updated_row = {**row}\n",
    "            for i, exemplar in enumerate(exemplars):\n",
    "                dtw_distance = dtw.distance(time_series, exemplar)\n",
    "                updated_row[f\"dtw_distance_exemplar_{i+1}\"] = dtw_distance\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "        else:\n",
    "            # Calculate DTW distances for each exemplar column\n",
    "            updated_row = {**row}\n",
    "            for col_name, exemplar in exemplar_columns.items():\n",
    "                # Extract index from column name (e.g., \"exemplar_1\" -> \"1\")\n",
    "                idx = col_name.split('_')[1]\n",
    "                dtw_distance = dtw.distance(time_series, exemplar)\n",
    "                updated_row[f\"dtw_distance_exemplar_{idx}\"] = dtw_distance\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "def assign_closest_exemplar(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        # Get all DTW distances\n",
    "        dtw_distances = {k: v for k, v in row.items() if k.startswith('dtw_distance_exemplar_')}\n",
    "        \n",
    "        if not dtw_distances:\n",
    "            # Create a simplified row without exemplar columns\n",
    "            simplified_row = {k: v for k, v in row.items() \n",
    "                             if not k.startswith('exemplar_')}\n",
    "            updated_rows.append(simplified_row)\n",
    "            continue\n",
    "        \n",
    "        # Find the closest exemplar based on the minimum DTW distance\n",
    "        closest_exemplar_key = min(dtw_distances, key=dtw_distances.get)\n",
    "        min_distance = dtw_distances[closest_exemplar_key]\n",
    "        \n",
    "        # Extract exemplar number from the key (e.g., \"dtw_distance_exemplar_1\" -> \"1\")\n",
    "        exemplar_num = closest_exemplar_key.split('_')[-1]\n",
    "        \n",
    "        # Get the corresponding exemplar time series data and its original label\n",
    "        exemplar_key = f'exemplar_{exemplar_num}'\n",
    "        exemplar_label_key = f'exemplar_{exemplar_num}_label'\n",
    "        exemplar_time_series = row.get(exemplar_key, None)\n",
    "        exemplar_original_label = row.get(exemplar_label_key, None)\n",
    "        \n",
    "        # Create a new row without the DTW distance columns and exemplar columns\n",
    "        updated_row = {k: v for k, v in row.items() \n",
    "                      if not k.startswith('dtw_distance_exemplar_') and not k.startswith('exemplar_')}\n",
    "        \n",
    "        # Add information about the closest exemplar\n",
    "        updated_row['closest_exemplar_id'] = closest_exemplar_key\n",
    "        updated_row['closest_exemplar_data'] = exemplar_time_series\n",
    "        updated_row['closest_exemplar_original_label'] = exemplar_original_label\n",
    "        \n",
    "        updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "def calculate_gini(labels):\n",
    "    if not labels:\n",
    "        return 0\n",
    "    label_counts = {}\n",
    "    for label in labels:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    total = sum(label_counts.values())\n",
    "    gini = 1 - sum((count / total) ** 2 for count in label_counts.values()) if total > 0 else 0\n",
    "    return gini\n",
    "\n",
    "# Define tree node structure\n",
    "class TreeNode:\n",
    "    def __init__(self, node_id=None, split_exemplar_id=None, split_exemplar_data=None, \n",
    "                 split_exemplar_label=None, is_leaf=False, predicted_label=None, gini_reduction=None,\n",
    "                 partition_id=None):\n",
    "        self.node_id = node_id\n",
    "        self.split_exemplar_id = split_exemplar_id\n",
    "        self.split_exemplar_data = split_exemplar_data\n",
    "        self.split_exemplar_label = split_exemplar_label\n",
    "        self.gini_reduction = gini_reduction\n",
    "        self.is_leaf = is_leaf\n",
    "        self.predicted_label = predicted_label\n",
    "        self.yes_child = None  # Instances closest to this exemplar\n",
    "        self.no_child = None   # Instances closest to other exemplars\n",
    "        self.partition_id = partition_id  # Track which partition built this tree\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert tree to dictionary for serialization\"\"\"\n",
    "        result = {\n",
    "            'node_id': self.node_id,\n",
    "            'split_exemplar_id': self.split_exemplar_id,\n",
    "            'split_exemplar_data': self.split_exemplar_data,\n",
    "            'split_exemplar_label': self.split_exemplar_label,\n",
    "            'gini_reduction': self.gini_reduction,\n",
    "            'is_leaf': self.is_leaf,\n",
    "            'predicted_label': self.predicted_label,\n",
    "            'partition_id': self.partition_id,\n",
    "        }\n",
    "        \n",
    "        if self.yes_child:\n",
    "            result['yes_child'] = self.yes_child.to_dict()\n",
    "        if self.no_child:\n",
    "            result['no_child'] = self.no_child.to_dict()\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Find the best split from all evaluated splits (within a single partition)\n",
    "def find_best_split(all_splits):\n",
    "    if not all_splits:\n",
    "        return None\n",
    "    return max(all_splits, key=lambda x: x['gini_reduction'])\n",
    "\n",
    "# Build a tree for a single partition (locally, not distributed)\n",
    "def build_partition_tree(partition_data, partition_id, max_depth=3, min_samples=2, node_id=1):\n",
    "    # If too few samples or max depth reached, create a leaf node\n",
    "    if len(partition_data) < min_samples or max_depth <= 0:\n",
    "        # Determine the majority class for prediction\n",
    "        label_counts = collections.Counter([row.get('label') for row in partition_data if row.get('label') is not None])\n",
    "        if not label_counts:\n",
    "            return TreeNode(node_id=node_id, is_leaf=True, predicted_label=None, partition_id=partition_id)\n",
    "        \n",
    "        majority_class = label_counts.most_common(1)[0][0]\n",
    "        return TreeNode(node_id=node_id, is_leaf=True, predicted_label=majority_class, partition_id=partition_id)\n",
    "    \n",
    "    # Evaluate all possible splits (reuse your existing function logic)\n",
    "    labels = [row.get('label') for row in partition_data if row.get('label') is not None]\n",
    "    before_split_gini = calculate_gini(labels)\n",
    "    \n",
    "    # Get all unique exemplars in the partition\n",
    "    unique_exemplars = set(\n",
    "        row['closest_exemplar_id'] for row in partition_data\n",
    "        if row.get('closest_exemplar_id') is not None\n",
    "    )\n",
    "    \n",
    "    # Create a mapping of exemplar_id to exemplar_data and exemplar_label\n",
    "    exemplar_data_map = {}\n",
    "    exemplar_label_map = {}\n",
    "    \n",
    "    for row in partition_data:\n",
    "        exemplar_id = row.get('closest_exemplar_id')\n",
    "        if exemplar_id and exemplar_id not in exemplar_data_map:\n",
    "            exemplar_data_map[exemplar_id] = row.get('closest_exemplar_data')\n",
    "            exemplar_label_map[exemplar_id] = row.get('closest_exemplar_original_label')\n",
    "    \n",
    "    # Evaluate all possible splits\n",
    "    splits = []\n",
    "    for exemplar_id in unique_exemplars:\n",
    "        # Split the data based on the current exemplar\n",
    "        yes_split = [r for r in partition_data if r.get('closest_exemplar_id') == exemplar_id]\n",
    "        no_split = [r for r in partition_data if r.get('closest_exemplar_id') != exemplar_id]\n",
    "        \n",
    "        # Calculate Gini for each daughter node\n",
    "        yes_labels = [r.get('label') for r in yes_split if r.get('label') is not None]\n",
    "        no_labels = [r.get('label') for r in no_split if r.get('label') is not None]\n",
    "        \n",
    "        yes_gini = calculate_gini(yes_labels)\n",
    "        no_gini = calculate_gini(no_labels)\n",
    "        \n",
    "        # Calculate weighted Gini after split\n",
    "        total_size = len(yes_split) + len(no_split)\n",
    "        weighted_gini = (yes_gini * len(yes_split) / total_size + no_gini * len(no_split) / total_size) if total_size > 0 else float('inf')\n",
    "        \n",
    "        # Calculate Gini reduction\n",
    "        gini_reduction = before_split_gini - weighted_gini if total_size > 0 else float('-inf')\n",
    "        \n",
    "        # Get the exemplar data and label for this exemplar_id\n",
    "        exemplar_data = exemplar_data_map.get(exemplar_id)\n",
    "        exemplar_label = exemplar_label_map.get(exemplar_id)\n",
    "        \n",
    "        # Add this split evaluation to results\n",
    "        splits.append({\n",
    "            \"partition_id\": partition_id,\n",
    "            \"exemplar_id\": exemplar_id,\n",
    "            \"exemplar_data\": exemplar_data,\n",
    "            \"exemplar_label\": exemplar_label,\n",
    "            \"gini_reduction\": gini_reduction   \n",
    "        })\n",
    "    \n",
    "    # Find best split\n",
    "    best_split = find_best_split(splits)\n",
    "    \n",
    "    # If no good split found, create a leaf node\n",
    "    if not best_split or best_split['gini_reduction'] <= 0:\n",
    "        label_counts = collections.Counter([row.get('label') for row in partition_data if row.get('label') is not None])\n",
    "        if not label_counts:\n",
    "            return TreeNode(node_id=node_id, is_leaf=True, predicted_label=None, partition_id=partition_id)\n",
    "        \n",
    "        majority_class = label_counts.most_common(1)[0][0]\n",
    "        return TreeNode(node_id=node_id, is_leaf=True, predicted_label=majority_class, partition_id=partition_id)\n",
    "    \n",
    "    # Create a decision node\n",
    "    node = TreeNode(\n",
    "        node_id=node_id,\n",
    "        split_exemplar_id=best_split['exemplar_id'],\n",
    "        split_exemplar_data=best_split['exemplar_data'],\n",
    "        split_exemplar_label=best_split['exemplar_label'],\n",
    "        gini_reduction=best_split['gini_reduction'],\n",
    "        partition_id=partition_id\n",
    "    )\n",
    "    \n",
    "    # Split the data based on the best exemplar\n",
    "    yes_data = [r for r in partition_data if r.get('closest_exemplar_id') == best_split['exemplar_id']]\n",
    "    no_data = [r for r in partition_data if r.get('closest_exemplar_id') != best_split['exemplar_id']]\n",
    "    \n",
    "    # Recursively build the subtrees\n",
    "    node.yes_child = build_partition_tree(yes_data, partition_id, max_depth-1, min_samples, node_id=node_id*2)\n",
    "    node.no_child = build_partition_tree(no_data, partition_id, max_depth-1, min_samples, node_id=node_id*2+1)\n",
    "    \n",
    "    return node\n",
    "\n",
    "# Function to visualize a single tree\n",
    "def visualize_tree(node, indent=\"\"):\n",
    "    if node is None:\n",
    "        return\n",
    "    \n",
    "    if node.is_leaf:\n",
    "        print(f\"{indent}Leaf: Predict class {node.predicted_label}\")\n",
    "    else:\n",
    "        # Format the exemplar data nicely for display\n",
    "        exemplar_data_str = \"[\" + \", \".join(f\"{val:.1f}\" for val in node.split_exemplar_data) + \"]\"\n",
    "        \n",
    "        print(f\"{indent}Is closest to exemplar {node.split_exemplar_id} (class {node.split_exemplar_label})?\")\n",
    "        print(f\"{indent}Data: {exemplar_data_str}, Gini reduction: {node.gini_reduction:.4f}\")\n",
    "        \n",
    "        print(f\"{indent}Yes ->\")\n",
    "        visualize_tree(node.yes_child, indent + \"  \")\n",
    "        \n",
    "        print(f\"{indent}No ->\")\n",
    "        visualize_tree(node.no_child, indent + \"  \")\n",
    "\n",
    "# Function to predict with a single tree\n",
    "def predict_with_tree(row, tree):\n",
    "    current_node = tree\n",
    "    \n",
    "    while not current_node.is_leaf:\n",
    "        closest_exemplar = row.get('closest_exemplar_id')\n",
    "        \n",
    "        if closest_exemplar == current_node.split_exemplar_id:\n",
    "            current_node = current_node.yes_child\n",
    "        else:\n",
    "            current_node = current_node.no_child\n",
    "    \n",
    "    return current_node.predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c212b02",
   "metadata": {},
   "source": [
    "# RUNNING THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24dd69a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path: C:\\Users\\benat\\OneDrive\\0. MSc MLiS\\0. GitHub Repositories\\BigDataProject_repo\\fulldataset_ECG5000.csv\n",
      "Loading 100.0% of data\n",
      "Data size: 5000\n",
      "\n",
      "Repartitioning to 10 workers\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 41.0 failed 1 times, most recent failure: Lost task 3.0 in stage 41.0 (TID 68) (BenAtkinson-Dell-Inspiron3505.Home executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"C:\\Users\\benat\\AppData\\Local\\Temp\\ipykernel_30224\\2873670571.py\", line 28, in choose_exemplars\nKeyError: 'time_series'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"C:\\Users\\benat\\AppData\\Local\\Temp\\ipykernel_30224\\2873670571.py\", line 28, in choose_exemplars\nKeyError: 'time_series'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 74\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[20], line 24\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m rdd_with_closest_exemplar \u001b[38;5;241m=\u001b[39m rdd_with_dtw\u001b[38;5;241m.\u001b[39mmapPartitions(assign_closest_exemplar)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Collect all data (for our small test dataset, this is fine)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m all_data \u001b[38;5;241m=\u001b[39m rdd_with_closest_exemplar\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Group by partition\u001b[39;00m\n\u001b[0;32m     27\u001b[0m partitions \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 41.0 failed 1 times, most recent failure: Lost task 3.0 in stage 41.0 (TID 68) (BenAtkinson-Dell-Inspiron3505.Home executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"C:\\Users\\benat\\AppData\\Local\\Temp\\ipykernel_30224\\2873670571.py\", line 28, in choose_exemplars\nKeyError: 'time_series'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"C:\\Users\\benat\\AppData\\Local\\Temp\\ipykernel_30224\\2873670571.py\", line 28, in choose_exemplars\nKeyError: 'time_series'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Main function to run the full pipeline\n",
    "def main():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"ExemplarTreeEnsemble\").getOrCreate()\n",
    "    \n",
    "    # load + preprocess data\n",
    "    df = ingestion.load_data()\n",
    "    df = preprocessor.run_preprocessing(df)\n",
    "    \n",
    "    # Repartition data\n",
    "    num_partitions = 10\n",
    "    rdd = repartition_sparkdf(df, num_partitions)\n",
    "    \n",
    "    # Choose exemplars\n",
    "    rdd_with_exemplars = rdd.mapPartitions(choose_exemplars)\n",
    "    \n",
    "    # Calculate DTW distances\n",
    "    rdd_with_dtw = rdd_with_exemplars.mapPartitions(calc_dtw_distance)\n",
    "    \n",
    "    # Assign closest exemplars\n",
    "    rdd_with_closest_exemplar = rdd_with_dtw.mapPartitions(assign_closest_exemplar)\n",
    "    \n",
    "    # Collect all data (for our small test dataset, this is fine)\n",
    "    all_data = rdd_with_closest_exemplar.collect()\n",
    "    \n",
    "    # Group by partition\n",
    "    partitions = {}\n",
    "    for row in all_data:\n",
    "        pid = row['partition_id']\n",
    "        if pid not in partitions:\n",
    "            partitions[pid] = []\n",
    "        partitions[pid].append(row)\n",
    "    \n",
    "    # Build trees for each partition\n",
    "    trees = []\n",
    "    for pid, data in partitions.items():\n",
    "        if data:  # Make sure there's data in this partition\n",
    "            tree = build_partition_tree(data, pid, max_depth=3, min_samples=2)\n",
    "            trees.append(tree)\n",
    "    \n",
    "    # Visualize each tree\n",
    "    for i, tree in enumerate(trees):\n",
    "        print(f\"\\nDecision Tree for Partition {tree.partition_id}:\")\n",
    "        visualize_tree(tree)\n",
    "    \n",
    "    # Make predictions and evaluate accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for row in all_data:\n",
    "        # Get predictions from all trees\n",
    "        predictions = [predict_with_tree(row, tree) for tree in trees]\n",
    "        # Take majority vote\n",
    "        if predictions:\n",
    "            counter = collections.Counter(predictions)\n",
    "            ensemble_prediction = counter.most_common(1)[0][0]\n",
    "            \n",
    "            # Update accuracy counts\n",
    "            total += 1\n",
    "            if ensemble_prediction == row.get('label'):\n",
    "                correct += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\nEnsemble Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "    \n",
    "    # Save the ensemble for future use (optional)\n",
    "    with open(\"exemplar_tree_ensemble.json\", \"w\") as f:\n",
    "        json.dump([tree.to_dict() for tree in trees], f, indent=2)\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bff83e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "366e03cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in current directory:\n",
      "  - complete_global.ipynb\n",
      "  - config.py\n",
      "  - controller_loop.py\n",
      "  - data_ingestion.py\n",
      "  - distance_measures.py\n",
      "  - evaluation.py\n",
      "  - exemplar_tree.json\n",
      "  - exemplar_tree_ensemble.json\n",
      "  - global.ipynb\n",
      "  - global_model_manager.py\n",
      "  - local_model_manager.py\n",
      "  - main.py\n",
      "  - partition_metrics\n",
      "  - prediction_manager.py\n",
      "  - preprocessing.py\n",
      "  - RDD_BA_2.ipynb\n",
      "  - rdd_global_trialling_stuff.ipynb\n",
      "  - reports\n",
      "  - test.ipynb\n",
      "  - test_pipeline_local_model.ipynb\n",
      "  - Tyler_global.py\n",
      "  - utilities.py\n",
      "  - visualization.py\n",
      "  - __pycache__\n",
      "Step 1: Loading and preprocessing data...\n",
      "Data Path: C:\\Users\\benat\\OneDrive\\0. MSc MLiS\\0. GitHub Repositories\\BigDataProject_repo\\fulldataset_ECG5000.csv\n",
      "Loading 100.0% of data\n",
      "Data size: 5000\n",
      "\n",
      "Repartitioning to 10 workers\n",
      "Step 2: Repartitioning data...\n",
      "Step 3: Choosing exemplars in each partition...\n",
      "Step 4: Calculating DTW distances...\n",
      "Step 5: Assigning closest exemplars...\n",
      "Step 6: Collecting results...\n",
      "Collected 500 rows\n",
      "Sample processed row structure:\n",
      "Keys: ['_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13', '_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c33', '_c34', '_c35', '_c36', '_c37', '_c38', '_c39', '_c40', '_c41', '_c42', '_c43', '_c44', '_c45', '_c46', '_c47', '_c48', '_c49', '_c50', '_c51', '_c52', '_c53', '_c54', '_c55', '_c56', '_c57', '_c58', '_c59', '_c60', '_c61', '_c62', '_c63', '_c64', '_c65', '_c66', '_c67', '_c68', '_c69', '_c70', '_c71', '_c72', '_c73', '_c74', '_c75', '_c76', '_c77', '_c78', '_c79', '_c80', '_c81', '_c82', '_c83', '_c84', '_c85', '_c86', '_c87', '_c88', '_c89', '_c90', '_c91', '_c92', '_c93', '_c94', '_c95', '_c96', '_c97', '_c98', '_c99', '_c100', '_c101', '_c102', '_c103', '_c104', '_c105', '_c106', '_c107', '_c108', '_c109', '_c110', '_c111', '_c112', '_c113', '_c114', '_c115', '_c116', '_c117', '_c118', '_c119', '_c120', '_c121', '_c122', '_c123', '_c124', '_c125', '_c126', '_c127', '_c128', '_c129', '_c130', '_c131', '_c132', '_c133', '_c134', '_c135', '_c136', '_c137', '_c138', '_c139', '_c140', 'label', 'partition_id']\n",
      "_c1: 0.5072229812860525\n",
      "_c2: 0.3535007852555048\n",
      "_c3: 0.11612508325039414\n",
      "_c4: 0.17323854326775248\n",
      "_c5: 0.24395069343177278\n",
      "_c6: 0.3917682975340598\n",
      "_c7: 0.4488408116802839\n",
      "_c8: 0.4433684149525536\n",
      "_c9: 0.5494590007351752\n",
      "_c10: 0.6230764541100388\n",
      "_c11: 0.6031499575716445\n",
      "_c12: 0.5782289772746197\n",
      "_c13: 0.5176795075243095\n",
      "_c14: 0.5245288242960986\n",
      "_c15: 0.5413122252868816\n",
      "_c16: 0.5241500971623825\n",
      "_c17: 0.48729648698061834\n",
      "_c18: 0.500638418685819\n",
      "_c19: 0.5327849429611166\n",
      "_c20: 0.5763026609270397\n",
      "_c21: 0.5998039539609593\n",
      "_c22: 0.5717285808326625\n",
      "_c23: 0.5530525060440286\n",
      "_c24: 0.5619178644571786\n",
      "_c25: 0.5449850764752977\n",
      "_c26: 0.5348551114554391\n",
      "_c27: 0.5220067243665437\n",
      "_c28: 0.491128457040868\n",
      "_c29: 0.4966465328531728\n",
      "_c30: 0.49865890721585454\n",
      "_c31: 0.5531979442518453\n",
      "_c32: 0.5355095111797283\n",
      "_c33: 0.5340724160527553\n",
      "_c34: 0.49673620599634666\n",
      "_c35: 0.4108716919311342\n",
      "_c36: 0.4966002360627651\n",
      "_c37: 0.5553606512178253\n",
      "_c38: 0.5588045419651013\n",
      "_c39: 0.5206801457725858\n",
      "_c40: 0.5841956495772394\n",
      "_c41: 0.6638257111704959\n",
      "_c42: 0.6582433004284042\n",
      "_c43: 0.5988545247674667\n",
      "_c44: 0.6728485023856844\n",
      "_c45: 0.6508614580853155\n",
      "_c46: 0.6372708793727173\n",
      "_c47: 0.5973198821336987\n",
      "_c48: 0.6222760845851223\n",
      "_c49: 0.6501936153470174\n",
      "_c50: 0.6688511401294546\n",
      "_c51: 0.6195292190016757\n",
      "_c52: 0.6035574302544673\n",
      "_c53: 0.5897608998517493\n",
      "_c54: 0.5806191403877434\n",
      "_c55: 0.5872615060682287\n",
      "_c56: 0.580813312559101\n",
      "_c57: 0.5713653598940044\n",
      "_c58: 0.6174458098672083\n",
      "_c59: 0.6959035945405277\n",
      "_c60: 0.6922929334970482\n",
      "_c61: 0.6625743979489584\n",
      "_c62: 0.6700378484590428\n",
      "_c63: 0.6965009883304645\n",
      "_c64: 0.7154184728050134\n",
      "_c65: 0.7268604246658857\n",
      "_c66: 0.6762911627366538\n",
      "_c67: 0.668149904136303\n",
      "_c68: 0.7120276740320102\n",
      "_c69: 0.7390257121585683\n",
      "_c70: 0.7653192543974033\n",
      "_c71: 0.7629660728679049\n",
      "_c72: 0.7034498771199216\n",
      "_c73: 0.6599365841368241\n",
      "_c74: 0.6782397632781145\n",
      "_c75: 0.7277402803540741\n",
      "_c76: 0.7276642972154792\n",
      "_c77: 0.6874462805837518\n",
      "_c78: 0.6598741708747757\n",
      "_c79: 0.6834101846184558\n",
      "_c80: 0.7081929065220935\n",
      "_c81: 0.7306186618256824\n",
      "_c82: 0.7065569824200707\n",
      "_c83: 0.65325960294745\n",
      "_c84: 0.6112989861175366\n",
      "_c85: 0.5389080800644098\n",
      "_c86: 0.5340974916138346\n",
      "_c87: 0.5543069540406806\n",
      "_c88: 0.5934820498766584\n",
      "_c89: 0.6265320673810003\n",
      "_c90: 0.6343979964218718\n",
      "_c91: 0.5954958046372534\n",
      "_c92: 0.5805679092207502\n",
      "_c93: 0.5940710815916105\n",
      "_c94: 0.6106594746610738\n",
      "_c95: 0.5920836930237843\n",
      "_c96: 0.5783065480301296\n",
      "_c97: 0.6203720509356312\n",
      "_c98: 0.6823284385830324\n",
      "_c99: 0.7200561749785365\n",
      "_c100: 0.669490128988786\n",
      "_c101: 0.6681195753997633\n",
      "_c102: 0.7138289081293231\n",
      "_c103: 0.7485803840235769\n",
      "_c104: 0.7884673648966367\n",
      "_c105: 0.8377589775305025\n",
      "_c106: 0.7625079272191554\n",
      "_c107: 0.7570742636688602\n",
      "_c108: 0.7919978237096212\n",
      "_c109: 0.7708015227675351\n",
      "_c110: 0.7569676013989849\n",
      "_c111: 0.6993882147472722\n",
      "_c112: 0.6041455723278858\n",
      "_c113: 0.49429815170175745\n",
      "_c114: 0.44867620851662243\n",
      "_c115: 0.4670792422019903\n",
      "_c116: 0.5222673577641134\n",
      "_c117: 0.5270115176734198\n",
      "_c118: 0.5416197210521265\n",
      "_c119: 0.553647559950702\n",
      "_c120: 0.5871369366523465\n",
      "_c121: 0.5906246902467638\n",
      "_c122: 0.5383417020568059\n",
      "_c123: 0.516860275252664\n",
      "_c124: 0.48214649489815964\n",
      "_c125: 0.47750519803873626\n",
      "_c126: 0.5361281233535983\n",
      "_c127: 0.5885074018334894\n",
      "_c128: 0.5793670085205661\n",
      "_c129: 0.5526453630299505\n",
      "_c130: 0.6242922610363736\n",
      "_c131: 0.7339997005200425\n",
      "_c132: 0.7527763667133095\n",
      "_c133: 0.7975512045488442\n",
      "_c134: 0.8360000778193095\n",
      "_c135: 0.8427602780419479\n",
      "_c136: 0.7682290281500406\n",
      "_c137: 0.7367908138721068\n",
      "_c138: 0.7916340837219027\n",
      "_c139: 0.7647054206390188\n",
      "_c140: 0.43843150493577676\n",
      "label: 1\n",
      "partition_id: 0\n",
      "Step 7: Grouping by partition...\n",
      "Number of partitions with data: 1\n",
      "Step 8: Building trees for each partition...\n",
      "Building tree for partition 0 with 500 rows...\n",
      "Built 1 trees\n",
      "Step 9: Visualizing sample trees...\n",
      "\n",
      "Decision Tree for Partition 0:\n",
      "Leaf: Predict class 1\n",
      "Step 10: Making predictions and evaluating accuracy...\n",
      "\n",
      "Ensemble Accuracy: 0.6000 (300/500)\n",
      "Step 11: Saving the ensemble model...\n",
      "Model saved successfully to exemplar_tree_ensemble.json\n",
      "Stopping Spark session...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import array, col\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import collections\n",
    "from dtaidistance import dtw\n",
    "import traceback\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"ExemplarTreeEnsemble\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Function to convert wide format data to features array\n",
    "def convert_wide_format_to_features(df):\n",
    "    \"\"\"\n",
    "    Convert a wide-format DataFrame with columns _c1, _c2, etc. to a DataFrame with 'features' array column\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame in wide format (each time point is a separate column)\n",
    "        \n",
    "    Returns:\n",
    "        PySpark DataFrame with 'features' and 'label' columns\n",
    "    \"\"\"\n",
    "    # Get all feature columns (excluding label)\n",
    "    feature_cols = [col_name for col_name in df.columns if col_name != 'label']\n",
    "    \n",
    "    # Create a features array column by combining all individual columns\n",
    "    df_with_features = df.select(\n",
    "        array(*[col(c) for c in feature_cols]).alias(\"features\"),\n",
    "        col(\"label\")\n",
    "    )\n",
    "    \n",
    "    return df_with_features\n",
    "\n",
    "def repartition_sparkdf(df, num_partitions):\n",
    "    \"\"\"Repartition a DataFrame and add partition_id\"\"\"\n",
    "    # First convert to RDD and repartition\n",
    "    rdd = df.rdd.repartition(num_partitions)\n",
    "    \n",
    "    # Add partition_id to each row\n",
    "    def add_partition_id(idx, iterator):\n",
    "        return ({**row.asDict(), \"partition_id\": idx} for row in iterator)\n",
    "    \n",
    "    return rdd.mapPartitionsWithIndex(add_partition_id)\n",
    "\n",
    "def choose_exemplars(iterator):\n",
    "    \"\"\"Select one exemplar per class in each partition\"\"\"\n",
    "    partition_data = list(iterator)\n",
    "    if not partition_data:\n",
    "        return iter([])\n",
    "    \n",
    "    # Check the structure of the data to find the correct time series field\n",
    "    # It could be 'features', or another field\n",
    "    if partition_data and len(partition_data) > 0:\n",
    "        sample_row = partition_data[0]\n",
    "        time_series_field = None\n",
    "        \n",
    "        # Find which field contains the time series data\n",
    "        for key, value in sample_row.items():\n",
    "            if isinstance(value, (list, np.ndarray)) and key != 'partition_id':\n",
    "                time_series_field = key\n",
    "                break\n",
    "        \n",
    "        if time_series_field is None:\n",
    "            print(f\"Warning: Cannot find time series data in row: {sample_row}\")\n",
    "            # If we can't identify the time series field, return the data unchanged\n",
    "            return iter(partition_data)\n",
    "    else:\n",
    "        return iter([])\n",
    "    \n",
    "    # Group data by class\n",
    "    grouped_data_by_class = {}\n",
    "    for row in partition_data:\n",
    "        label = row.get('label')\n",
    "        if label is not None:\n",
    "            if label not in grouped_data_by_class:\n",
    "                grouped_data_by_class[label] = []\n",
    "            grouped_data_by_class[label].append(row)\n",
    "    \n",
    "    # Select one exemplar per class\n",
    "    chosen_exemplars = []\n",
    "    \n",
    "    for label, instances in grouped_data_by_class.items():\n",
    "        if instances:  # Ensure there are instances for the class\n",
    "            # Using first instance as exemplar (more deterministic than random sampling)\n",
    "            exemplar = instances[0]\n",
    "            time_series_data = exemplar.get(time_series_field)\n",
    "            chosen_exemplars.append((time_series_data, label))\n",
    "    \n",
    "    # Store exemplar time_series for filtering\n",
    "    exemplar_time_series = [ex[0] for ex in chosen_exemplars]\n",
    "    \n",
    "    # Remove chosen exemplars from the working data\n",
    "    filtered_partition = []\n",
    "    for row in partition_data:\n",
    "        row_time_series = row.get(time_series_field)\n",
    "        keep = True\n",
    "        for ex_ts in exemplar_time_series:\n",
    "            # Compare time series - we need to handle both list and numpy array formats\n",
    "            if isinstance(row_time_series, (list, np.ndarray)) and isinstance(ex_ts, (list, np.ndarray)):\n",
    "                # For lists, check if they have the same values\n",
    "                if len(row_time_series) == len(ex_ts) and all(x == y for x, y in zip(row_time_series, ex_ts)):\n",
    "                    keep = False\n",
    "                    break\n",
    "        if keep:\n",
    "            filtered_partition.append(row)\n",
    "    \n",
    "    # Return rows with individual exemplar columns and their labels\n",
    "    result = []\n",
    "    for row in filtered_partition:\n",
    "        new_row = {**row}\n",
    "        # Add each exemplar and its label as columns\n",
    "        for i, (exemplar, label) in enumerate(chosen_exemplars, 1):\n",
    "            new_row[f\"exemplar_{i}\"] = exemplar\n",
    "            new_row[f\"exemplar_{i}_label\"] = label\n",
    "        result.append(new_row)\n",
    "    \n",
    "    return iter(result)\n",
    "\n",
    "def calc_dtw_distance(iterator):\n",
    "    \"\"\"Calculate DTW distance between time series and exemplars\"\"\"\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        # Find the time series field (could be 'features' or other field name)\n",
    "        time_series_field = None\n",
    "        time_series = None\n",
    "        \n",
    "        for key, value in row.items():\n",
    "            if isinstance(value, (list, np.ndarray)) and key != 'partition_id' and not key.startswith('exemplar_'):\n",
    "                time_series_field = key\n",
    "                time_series = value\n",
    "                break\n",
    "        \n",
    "        if time_series_field is None or time_series is None:\n",
    "            # If we can't find the time series data, keep the row but skip distance calculation\n",
    "            updated_rows.append(row)\n",
    "            continue\n",
    "        \n",
    "        # Check for individual exemplar columns\n",
    "        exemplar_columns = {k: v for k, v in row.items() \n",
    "                          if k.startswith('exemplar_') and not k.endswith('_label') and isinstance(v, (list, np.ndarray))}\n",
    "        \n",
    "        if not exemplar_columns:\n",
    "            # If no exemplars found, skip distance calculation but keep the row\n",
    "            updated_rows.append(row)\n",
    "            continue\n",
    "        \n",
    "        # Calculate DTW distances for each exemplar column\n",
    "        updated_row = {**row}\n",
    "        for col_name, exemplar in exemplar_columns.items():\n",
    "            # Extract index from column name (e.g., \"exemplar_1\" -> \"1\")\n",
    "            idx = col_name.split('_')[1]\n",
    "            \n",
    "            # Safely calculate DTW distance with error handling\n",
    "            try:\n",
    "                # Ensure both arrays have same dimensions and data types\n",
    "                ts1 = np.array(time_series, dtype=float)\n",
    "                ts2 = np.array(exemplar, dtype=float)\n",
    "                \n",
    "                # Handle potential empty arrays\n",
    "                if len(ts1) == 0 or len(ts2) == 0:\n",
    "                    dtw_distance = float('inf')\n",
    "                else:\n",
    "                    # Use DTW distance function from dtaidistance\n",
    "                    dtw_distance = dtw.distance(ts1, ts2)\n",
    "                    \n",
    "                updated_row[f\"dtw_distance_exemplar_{idx}\"] = dtw_distance\n",
    "            except Exception as e:\n",
    "                # Handle any errors gracefully\n",
    "                updated_row[f\"dtw_distance_exemplar_{idx}\"] = float('inf')\n",
    "                print(f\"DTW calculation error for {time_series_field}: {e}\")\n",
    "        \n",
    "        updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "def assign_closest_exemplar(iterator):\n",
    "    \"\"\"Assign each instance to its closest exemplar\"\"\"\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        # Get all DTW distances\n",
    "        dtw_distances = {k: v for k, v in row.items() if k.startswith('dtw_distance_exemplar_')}\n",
    "        \n",
    "        if not dtw_distances:\n",
    "            # Create a simplified row without exemplar columns\n",
    "            simplified_row = {k: v for k, v in row.items() \n",
    "                             if not k.startswith('exemplar_')}\n",
    "            updated_rows.append(simplified_row)\n",
    "            continue\n",
    "        \n",
    "        # Find the closest exemplar based on the minimum DTW distance\n",
    "        try:\n",
    "            closest_exemplar_key = min(dtw_distances.items(), key=lambda x: x[1])[0]\n",
    "            min_distance = dtw_distances[closest_exemplar_key]\n",
    "        except ValueError:\n",
    "            # Handle case with no valid distances\n",
    "            simplified_row = {k: v for k, v in row.items() \n",
    "                             if not k.startswith('exemplar_') and not k.startswith('dtw_distance_exemplar_')}\n",
    "            updated_rows.append(simplified_row)\n",
    "            continue\n",
    "        \n",
    "        # Extract exemplar number from the key\n",
    "        exemplar_num = closest_exemplar_key.split('_')[-1]\n",
    "        \n",
    "        # Get the corresponding exemplar time series data and its original label\n",
    "        exemplar_key = f'exemplar_{exemplar_num}'\n",
    "        exemplar_label_key = f'exemplar_{exemplar_num}_label'\n",
    "        exemplar_time_series = row.get(exemplar_key, None)\n",
    "        exemplar_original_label = row.get(exemplar_label_key, None)\n",
    "        \n",
    "        # Create a new row without the DTW distance columns and exemplar columns\n",
    "        updated_row = {k: v for k, v in row.items() \n",
    "                      if not k.startswith('dtw_distance_exemplar_') and not k.startswith('exemplar_')}\n",
    "        \n",
    "        # Add information about the closest exemplar\n",
    "        updated_row['closest_exemplar_id'] = exemplar_key\n",
    "        updated_row['closest_exemplar_data'] = exemplar_time_series\n",
    "        updated_row['closest_exemplar_original_label'] = exemplar_original_label\n",
    "        updated_row['min_dtw_distance'] = min_distance\n",
    "        \n",
    "        updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "def calculate_gini(labels):\n",
    "    \"\"\"Calculate Gini impurity for a list of labels\"\"\"\n",
    "    if not labels:\n",
    "        return 0\n",
    "    label_counts = {}\n",
    "    for label in labels:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    total = sum(label_counts.values())\n",
    "    gini = 1 - sum((count / total) ** 2 for count in label_counts.values()) if total > 0 else 0\n",
    "    return gini\n",
    "\n",
    "# Define tree node structure\n",
    "class TreeNode:\n",
    "    def __init__(self, node_id=None, split_exemplar_id=None, split_exemplar_data=None, \n",
    "                 split_exemplar_label=None, is_leaf=False, predicted_label=None, gini_reduction=None,\n",
    "                 partition_id=None):\n",
    "        self.node_id = node_id\n",
    "        self.split_exemplar_id = split_exemplar_id\n",
    "        self.split_exemplar_data = split_exemplar_data\n",
    "        self.split_exemplar_label = split_exemplar_label\n",
    "        self.gini_reduction = gini_reduction\n",
    "        self.is_leaf = is_leaf\n",
    "        self.predicted_label = predicted_label\n",
    "        self.yes_child = None  # Instances closest to this exemplar\n",
    "        self.no_child = None   # Instances closest to other exemplars\n",
    "        self.partition_id = partition_id  # Track which partition built this tree\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert tree to dictionary for serialization\"\"\"\n",
    "        result = {\n",
    "            'node_id': self.node_id,\n",
    "            'split_exemplar_id': self.split_exemplar_id,\n",
    "            'split_exemplar_label': self.split_exemplar_label,\n",
    "            'gini_reduction': self.gini_reduction,\n",
    "            'is_leaf': self.is_leaf,\n",
    "            'predicted_label': self.predicted_label,\n",
    "            'partition_id': self.partition_id,\n",
    "        }\n",
    "        \n",
    "        # Handle split_exemplar_data separately to make it JSON serializable\n",
    "        if self.split_exemplar_data is not None:\n",
    "            if isinstance(self.split_exemplar_data, list):\n",
    "                result['split_exemplar_data'] = self.split_exemplar_data\n",
    "            else:\n",
    "                # Convert numpy arrays or other types to list\n",
    "                try:\n",
    "                    result['split_exemplar_data'] = list(self.split_exemplar_data)\n",
    "                except:\n",
    "                    result['split_exemplar_data'] = None\n",
    "        \n",
    "        if self.yes_child:\n",
    "            result['yes_child'] = self.yes_child.to_dict()\n",
    "        if self.no_child:\n",
    "            result['no_child'] = self.no_child.to_dict()\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Find the best split from all evaluated splits (within a single partition)\n",
    "def find_best_split(all_splits):\n",
    "    if not all_splits:\n",
    "        return None\n",
    "    return max(all_splits, key=lambda x: x['gini_reduction'])\n",
    "\n",
    "# Build a tree for a single partition (locally, not distributed)\n",
    "def build_partition_tree(partition_data, partition_id, max_depth=3, min_samples=2, node_id=1):\n",
    "    # If too few samples or max depth reached, create a leaf node\n",
    "    if len(partition_data) < min_samples or max_depth <= 0:\n",
    "        # Determine the majority class for prediction\n",
    "        label_counts = collections.Counter([row.get('label') for row in partition_data if row.get('label') is not None])\n",
    "        if not label_counts:\n",
    "            return TreeNode(node_id=node_id, is_leaf=True, predicted_label=None, partition_id=partition_id)\n",
    "        \n",
    "        majority_class = label_counts.most_common(1)[0][0]\n",
    "        return TreeNode(node_id=node_id, is_leaf=True, predicted_label=majority_class, partition_id=partition_id)\n",
    "    \n",
    "    # Evaluate all possible splits\n",
    "    labels = [row.get('label') for row in partition_data if row.get('label') is not None]\n",
    "    before_split_gini = calculate_gini(labels)\n",
    "    \n",
    "    # Get all unique exemplars in the partition\n",
    "    unique_exemplars = set(\n",
    "        row['closest_exemplar_id'] for row in partition_data\n",
    "        if row.get('closest_exemplar_id') is not None\n",
    "    )\n",
    "    \n",
    "    # Create a mapping of exemplar_id to exemplar_data and exemplar_label\n",
    "    exemplar_data_map = {}\n",
    "    exemplar_label_map = {}\n",
    "    \n",
    "    for row in partition_data:\n",
    "        exemplar_id = row.get('closest_exemplar_id')\n",
    "        if exemplar_id and exemplar_id not in exemplar_data_map:\n",
    "            exemplar_data_map[exemplar_id] = row.get('closest_exemplar_data')\n",
    "            exemplar_label_map[exemplar_id] = row.get('closest_exemplar_original_label')\n",
    "    \n",
    "    # Evaluate all possible splits\n",
    "    splits = []\n",
    "    for exemplar_id in unique_exemplars:\n",
    "        # Split the data based on the current exemplar\n",
    "        yes_split = [r for r in partition_data if r.get('closest_exemplar_id') == exemplar_id]\n",
    "        no_split = [r for r in partition_data if r.get('closest_exemplar_id') != exemplar_id]\n",
    "        \n",
    "        # Calculate Gini for each daughter node\n",
    "        yes_labels = [r.get('label') for r in yes_split if r.get('label') is not None]\n",
    "        no_labels = [r.get('label') for r in no_split if r.get('label') is not None]\n",
    "        \n",
    "        yes_gini = calculate_gini(yes_labels)\n",
    "        no_gini = calculate_gini(no_labels)\n",
    "        \n",
    "        # Calculate weighted Gini after split\n",
    "        total_size = len(yes_split) + len(no_split)\n",
    "        weighted_gini = (yes_gini * len(yes_split) / total_size + \n",
    "                         no_gini * len(no_split) / total_size) if total_size > 0 else float('inf')\n",
    "        \n",
    "        # Calculate Gini reduction\n",
    "        gini_reduction = before_split_gini - weighted_gini if total_size > 0 else float('-inf')\n",
    "        \n",
    "        # Get the exemplar data and label for this exemplar_id\n",
    "        exemplar_data = exemplar_data_map.get(exemplar_id)\n",
    "        exemplar_label = exemplar_label_map.get(exemplar_id)\n",
    "        \n",
    "        # Add this split evaluation to results\n",
    "        splits.append({\n",
    "            \"partition_id\": partition_id,\n",
    "            \"exemplar_id\": exemplar_id,\n",
    "            \"exemplar_data\": exemplar_data,\n",
    "            \"exemplar_label\": exemplar_label,\n",
    "            \"gini_reduction\": gini_reduction   \n",
    "        })\n",
    "    \n",
    "    # Find best split\n",
    "    best_split = find_best_split(splits)\n",
    "    \n",
    "    # If no good split found, create a leaf node\n",
    "    if not best_split or best_split['gini_reduction'] <= 0:\n",
    "        label_counts = collections.Counter([row.get('label') for row in partition_data if row.get('label') is not None])\n",
    "        if not label_counts:\n",
    "            return TreeNode(node_id=node_id, is_leaf=True, predicted_label=None, partition_id=partition_id)\n",
    "        \n",
    "        majority_class = label_counts.most_common(1)[0][0]\n",
    "        return TreeNode(node_id=node_id, is_leaf=True, predicted_label=majority_class, partition_id=partition_id)\n",
    "    \n",
    "    # Create a decision node\n",
    "    node = TreeNode(\n",
    "        node_id=node_id,\n",
    "        split_exemplar_id=best_split['exemplar_id'],\n",
    "        split_exemplar_data=best_split['exemplar_data'],\n",
    "        split_exemplar_label=best_split['exemplar_label'],\n",
    "        gini_reduction=best_split['gini_reduction'],\n",
    "        partition_id=partition_id\n",
    "    )\n",
    "    \n",
    "    # Split the data based on the best exemplar\n",
    "    yes_data = [r for r in partition_data if r.get('closest_exemplar_id') == best_split['exemplar_id']]\n",
    "    no_data = [r for r in partition_data if r.get('closest_exemplar_id') != best_split['exemplar_id']]\n",
    "    \n",
    "    # Recursively build the subtrees\n",
    "    node.yes_child = build_partition_tree(yes_data, partition_id, max_depth-1, min_samples, node_id=node_id*2)\n",
    "    node.no_child = build_partition_tree(no_data, partition_id, max_depth-1, min_samples, node_id=node_id*2+1)\n",
    "    \n",
    "    return node\n",
    "\n",
    "# Function to visualize a single tree\n",
    "def visualize_tree(node, indent=\"\"):\n",
    "    if node is None:\n",
    "        return\n",
    "    \n",
    "    if node.is_leaf:\n",
    "        print(f\"{indent}Leaf: Predict class {node.predicted_label}\")\n",
    "    else:\n",
    "        # Format the exemplar data nicely for display\n",
    "        exemplar_data_str = \"[...]\"  # Simplified display for potentially large time series\n",
    "        if node.split_exemplar_data and len(node.split_exemplar_data) <= 10:\n",
    "            exemplar_data_str = \"[\" + \", \".join(f\"{val:.1f}\" for val in node.split_exemplar_data[:10]) + \"]\"\n",
    "        \n",
    "        print(f\"{indent}Is closest to exemplar {node.split_exemplar_id} (class {node.split_exemplar_label})?\")\n",
    "        print(f\"{indent}Gini reduction: {node.gini_reduction:.4f}\")\n",
    "        \n",
    "        print(f\"{indent}Yes ->\")\n",
    "        visualize_tree(node.yes_child, indent + \"  \")\n",
    "        \n",
    "        print(f\"{indent}No ->\")\n",
    "        visualize_tree(node.no_child, indent + \"  \")\n",
    "\n",
    "# Function to predict with a single tree\n",
    "def predict_with_tree(row, tree):\n",
    "    current_node = tree\n",
    "    \n",
    "    while current_node and not current_node.is_leaf:\n",
    "        closest_exemplar = row.get('closest_exemplar_id')\n",
    "        \n",
    "        if closest_exemplar == current_node.split_exemplar_id:\n",
    "            current_node = current_node.yes_child\n",
    "        else:\n",
    "            current_node = current_node.no_child\n",
    "        \n",
    "        # Safety check in case the tree structure is incomplete\n",
    "        if current_node is None:\n",
    "            return None\n",
    "    \n",
    "    return current_node.predicted_label if current_node else None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize configuration\n",
    "        config = {\n",
    "            \"data_path\": \"fulldataset_ECG5000.csv\",  # Update with your actual path\n",
    "            \"label_col\": \"label\",\n",
    "            \"num_partitions\": 5,\n",
    "            \"max_depth\": 3,\n",
    "            \"min_samples_split\": 2\n",
    "        }\n",
    "        \n",
    "        # Print all available files in the current directory to help debug file path issues\n",
    "        import os\n",
    "        print(\"Files in current directory:\")\n",
    "        for file in os.listdir('.'):\n",
    "            print(f\"  - {file}\")\n",
    "        \n",
    "        print(\"Step 1: Loading and preprocessing data...\")\n",
    "        \n",
    "        df = ingestion.load_data()\n",
    "        df = preprocessor.run_preprocessing(df)\n",
    "            \n",
    "        print(\"Step 2: Repartitioning data...\")\n",
    "        # Prepare data for exemplar tree algorithm\n",
    "        rdd = repartition_sparkdf(df, config[\"num_partitions\"])\n",
    "        \n",
    "        print(\"Step 3: Choosing exemplars in each partition...\")\n",
    "        # Choose exemplars\n",
    "        rdd_with_exemplars = rdd.mapPartitions(choose_exemplars)\n",
    "        \n",
    "        print(\"Step 4: Calculating DTW distances...\")\n",
    "        # Calculate DTW distances\n",
    "        rdd_with_dtw = rdd_with_exemplars.mapPartitions(calc_dtw_distance)\n",
    "        \n",
    "        print(\"Step 5: Assigning closest exemplars...\")\n",
    "        # Assign closest exemplars\n",
    "        rdd_with_closest_exemplar = rdd_with_dtw.mapPartitions(assign_closest_exemplar)\n",
    "        \n",
    "        print(\"Step 6: Collecting results...\")\n",
    "        # Apply take() instead of collect() to limit the amount of data transferred\n",
    "        # This helps avoid memory issues with large datasets\n",
    "        all_data = rdd_with_closest_exemplar.take(500)  # Take a sample first to test\n",
    "        \n",
    "        print(f\"Collected {len(all_data)} rows\")\n",
    "        \n",
    "        if len(all_data) == 0:\n",
    "            print(\"WARNING: No data was collected after processing. Check for errors in the previous steps.\")\n",
    "            return\n",
    "            \n",
    "        # Print a sample row to help debug\n",
    "        if all_data:\n",
    "            print(\"Sample processed row structure:\")\n",
    "            sample_row = all_data[0]\n",
    "            print(f\"Keys: {list(sample_row.keys())}\")\n",
    "            for key, value in sample_row.items():\n",
    "                if isinstance(value, (list, np.ndarray)):\n",
    "                    print(f\"{key}: [Array of length {len(value)}]\")\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "        \n",
    "        print(\"Step 7: Grouping by partition...\")\n",
    "        # Group by partition\n",
    "        partitions = {}\n",
    "        for row in all_data:\n",
    "            pid = row.get('partition_id')\n",
    "            if pid is not None:\n",
    "                if pid not in partitions:\n",
    "                    partitions[pid] = []\n",
    "                partitions[pid].append(row)\n",
    "        \n",
    "        print(f\"Number of partitions with data: {len(partitions)}\")\n",
    "        \n",
    "        print(\"Step 8: Building trees for each partition...\")\n",
    "        # Build trees for each partition\n",
    "        trees = []\n",
    "        for pid, data in partitions.items():\n",
    "            if data:  # Make sure there's data in this partition\n",
    "                print(f\"Building tree for partition {pid} with {len(data)} rows...\")\n",
    "                tree = build_partition_tree(\n",
    "                    data, \n",
    "                    pid, \n",
    "                    max_depth=config[\"max_depth\"], \n",
    "                    min_samples=config[\"min_samples_split\"]\n",
    "                )\n",
    "                trees.append(tree)\n",
    "        \n",
    "        print(f\"Built {len(trees)} trees\")\n",
    "        \n",
    "        print(\"Step 9: Visualizing sample trees...\")\n",
    "        # Visualize each tree (limit to first 2 trees for brevity)\n",
    "        for i, tree in enumerate(trees[:2]):\n",
    "            print(f\"\\nDecision Tree for Partition {tree.partition_id}:\")\n",
    "            visualize_tree(tree)\n",
    "        \n",
    "        print(\"Step 10: Making predictions and evaluating accuracy...\")\n",
    "        # Make predictions and evaluate accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for row in all_data:\n",
    "            # Get predictions from all trees\n",
    "            predictions = [predict_with_tree(row, tree) for tree in trees]\n",
    "            # Remove None predictions\n",
    "            predictions = [p for p in predictions if p is not None]\n",
    "            \n",
    "            # Take majority vote if we have predictions\n",
    "            if predictions:\n",
    "                counter = collections.Counter(predictions)\n",
    "                ensemble_prediction = counter.most_common(1)[0][0]\n",
    "                \n",
    "                # Update accuracy counts\n",
    "                actual_label = row.get('label')\n",
    "                if actual_label is not None:\n",
    "                    total += 1\n",
    "                    if ensemble_prediction == actual_label:\n",
    "                        correct += 1\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        print(f\"\\nEnsemble Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "        \n",
    "        print(\"Step 11: Saving the ensemble model...\")\n",
    "        # Save the ensemble for future use (optional)\n",
    "        try:\n",
    "            with open(\"exemplar_tree_ensemble.json\", \"w\") as f:\n",
    "                json.dump([tree.to_dict() for tree in trees], f)\n",
    "            print(\"Model saved successfully to exemplar_tree_ensemble.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        print(\"Stopping Spark session...\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da365d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
