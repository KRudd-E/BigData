{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc7c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from data_ingestion import DataIngestion\n",
    "from preprocessing import Preprocessor\n",
    "from prediction_manager import PredictionManager\n",
    "from local_model_manager import LocalModelManager\n",
    "from evaluation import Evaluator\n",
    "from utilities import show_compact\n",
    "import time\n",
    "import json\n",
    "from random import sample\n",
    "from dtaidistance import dtw\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"GenericRDD\").getOrCreate()\n",
    "\n",
    "# Access the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f30b2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]},\n",
    "    {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
    "    {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
    "    {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
    "    {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
    "    {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
    "    {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
    "    {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
    "    {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
    "    {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.4, 1.9, 2.4, 2.9]},\n",
    "    {'label': 1, 'time_series': [1.4, 2.0, 2.6, 3.2, 3.8]},\n",
    "    {'label': 2, 'time_series': [2.2, 3.1, 4.0, 4.9, 5.8]},\n",
    "    {'label': 2, 'time_series': [2.6, 3.2, 3.8, 4.4, 5.0]},\n",
    "    {'label': 3, 'time_series': [1.2, 2.0, 2.8, 3.6, 4.4]},\n",
    "    {'label': 3, 'time_series': [0.6, 1.3, 2.0, 2.7, 3.4]},\n",
    "    {'label': 4, 'time_series': [6.3, 6.5, 6.7, 6.9, 7.1]},\n",
    "    {'label': 4, 'time_series': [7.0, 7.8, 8.6, 9.4, 10.2]},\n",
    "    {'label': 4, 'time_series': [6.5, 7.0, 7.5, 8.0, 8.5]},\n",
    "    {'label': 1, 'time_series': [0.5, 1.0, 1.5, 2.0, 2.5]},\n",
    "    {'label': 2, 'time_series': [0.6, 1.4, 1.3, 2.1, 2.5]},\n",
    "    {'label': 3, 'time_series': [0.3, 1.7, 1.6, 2.2, 2.6]},\n",
    "    {'label': 4, 'time_series': [0.2, 1.9, 1.6, 2.3, 2.7]},\n",
    "    {'label': 4, 'time_series': [0.9, 1.7, 1.2, 2.4, 2.8]}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(tsdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669a74f",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ee4551f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5], 'partition_id': 0},\n",
       " {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5], 'partition_id': 0},\n",
       " {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [0.9, 1.4, 1.9, 2.4, 2.9], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [1.4, 2.0, 2.6, 3.2, 3.8], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [2.2, 3.1, 4.0, 4.9, 5.8], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [2.6, 3.2, 3.8, 4.4, 5.0], 'partition_id': 0},\n",
       " {'label': 3, 'time_series': [1.2, 2.0, 2.8, 3.6, 4.4], 'partition_id': 0},\n",
       " {'label': 3, 'time_series': [0.6, 1.3, 2.0, 2.7, 3.4], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [6.3, 6.5, 6.7, 6.9, 7.1], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [7.0, 7.8, 8.6, 9.4, 10.2], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0], 'partition_id': 1},\n",
       " {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9], 'partition_id': 1},\n",
       " {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2], 'partition_id': 1},\n",
       " {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5], 'partition_id': 1},\n",
       " {'label': 4, 'time_series': [6.5, 7.0, 7.5, 8.0, 8.5], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [0.5, 1.0, 1.5, 2.0, 2.5], 'partition_id': 1},\n",
       " {'label': 2, 'time_series': [0.6, 1.4, 1.3, 2.1, 2.5], 'partition_id': 1},\n",
       " {'label': 3, 'time_series': [0.3, 1.7, 1.6, 2.2, 2.6], 'partition_id': 1},\n",
       " {'label': 4, 'time_series': [0.2, 1.9, 1.6, 2.3, 2.7], 'partition_id': 1},\n",
       " {'label': 4, 'time_series': [0.9, 1.7, 1.2, 2.4, 2.8], 'partition_id': 1}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repartition_sparkdf(df, num_partitions):\n",
    "    rdd = df.rdd\n",
    "    rdd = rdd.repartition(num_partitions)\n",
    "    rdd = rdd.mapPartitionsWithIndex(\n",
    "            lambda idx, iter: [{**row.asDict(), \"partition_id\": idx} for row in iter]\n",
    "        )\n",
    "    return rdd\n",
    "\n",
    "# example usage\n",
    "rdd = repartition_sparkdf(df, 2)\n",
    "print(rdd.getNumPartitions())  # should print 1\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be92f01",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ee6bc902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 3,\n",
       "  'time_series': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 4,\n",
       "  'time_series': [5.5, 6.6, 7.7, 8.8, 9.9],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 4,\n",
       "  'time_series': [6.1, 6.2, 6.3, 6.4, 6.5],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 1,\n",
       "  'time_series': [0.7, 1.3, 1.9, 2.5, 3.1],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 1,\n",
       "  'time_series': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 1,\n",
       "  'time_series': [0.6, 1.2, 1.8, 2.4, 3.0],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 2,\n",
       "  'time_series': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 3,\n",
       "  'time_series': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 1,\n",
       "  'time_series': [1.3, 2.3, 3.3, 4.3, 5.3],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 1,\n",
       "  'time_series': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 2,\n",
       "  'time_series': [2.2, 3.1, 4.0, 4.9, 5.8],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 2,\n",
       "  'time_series': [2.6, 3.2, 3.8, 4.4, 5.0],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 3,\n",
       "  'time_series': [1.2, 2.0, 2.8, 3.6, 4.4],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 3,\n",
       "  'time_series': [0.6, 1.3, 2.0, 2.7, 3.4],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 4,\n",
       "  'time_series': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 4,\n",
       "  'time_series': [7.0, 7.8, 8.6, 9.4, 10.2],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2},\n",
       " {'label': 1,\n",
       "  'time_series': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_3_label': 4,\n",
       "  'exemplar_4': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_4_label': 3},\n",
       " {'label': 1,\n",
       "  'time_series': [0.9, 1.8, 2.7, 3.6, 4.5],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_3_label': 4,\n",
       "  'exemplar_4': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_4_label': 3},\n",
       " {'label': 1,\n",
       "  'time_series': [1.5, 2.1, 2.7, 3.3, 3.9],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_3_label': 4,\n",
       "  'exemplar_4': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_4_label': 3},\n",
       " {'label': 1,\n",
       "  'time_series': [0.8, 1.7, 2.5, 3.2, 4.0],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_3_label': 4,\n",
       "  'exemplar_4': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_4_label': 3},\n",
       " {'label': 2,\n",
       "  'time_series': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_3_label': 4,\n",
       "  'exemplar_4': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_4_label': 3},\n",
       " {'label': 2,\n",
       "  'time_series': [3.0, 3.8, 4.6, 5.4, 6.2],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_3_label': 4,\n",
       "  'exemplar_4': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_4_label': 3},\n",
       " {'label': 4,\n",
       "  'time_series': [6.5, 7.0, 7.5, 8.0, 8.5],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_3_label': 4,\n",
       "  'exemplar_4': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_4_label': 3},\n",
       " {'label': 1,\n",
       "  'time_series': [0.5, 1.0, 1.5, 2.0, 2.5],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_3_label': 4,\n",
       "  'exemplar_4': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_4_label': 3},\n",
       " {'label': 2,\n",
       "  'time_series': [0.6, 1.4, 1.3, 2.1, 2.5],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_3_label': 4,\n",
       "  'exemplar_4': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_4_label': 3},\n",
       " {'label': 4,\n",
       "  'time_series': [0.2, 1.9, 1.6, 2.3, 2.7],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_3_label': 4,\n",
       "  'exemplar_4': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_4_label': 3}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_exemplars(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    if not partition_data:\n",
    "        return iter([])\n",
    "    \n",
    "    # Group data by class\n",
    "    grouped_data_by_class = {}\n",
    "    for row in partition_data:\n",
    "        label = row.get('label')\n",
    "        if label is not None:\n",
    "            grouped_data_by_class.setdefault(label, []).append(row)\n",
    "    \n",
    "    # Select one exemplar per class and track exemplar-to-label mapping\n",
    "    chosen_exemplars = []\n",
    "    exemplar_labels = {}  # Map to track exemplar labels\n",
    "    \n",
    "    for label, instances in grouped_data_by_class.items():\n",
    "        if instances:  # Ensure there are instances for the class\n",
    "            exemplar = sample(instances, 1)[0]\n",
    "            chosen_exemplars.append((exemplar['time_series'], label))  # Store tuple of (time_series, label)\n",
    "    \n",
    "    # Remove chosen exemplars from the working data\n",
    "    exemplar_time_series = [ex[0] for ex in chosen_exemplars]\n",
    "    filtered_partition = [\n",
    "        row for row in partition_data\n",
    "        if row['time_series'] not in exemplar_time_series\n",
    "    ]\n",
    "    \n",
    "    # Return rows with individual exemplar columns and their labels\n",
    "    result = []\n",
    "    for row in filtered_partition:\n",
    "        new_row = {**row}\n",
    "        # Add each exemplar and its label as columns\n",
    "        for i, (exemplar, label) in enumerate(chosen_exemplars, 1):\n",
    "            new_row[f\"exemplar_{i}\"] = exemplar\n",
    "            new_row[f\"exemplar_{i}_label\"] = label\n",
    "        result.append(new_row)\n",
    "    \n",
    "    return iter(result)\n",
    "\n",
    "# example usage\n",
    "rdd_with_exemplars = rdd.mapPartitions(choose_exemplars)\n",
    "rdd_with_exemplars.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a538e",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "da68b739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 3,\n",
       "  'time_series': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2,\n",
       "  'dtw_distance_exemplar_1': 1.7320508075688772,\n",
       "  'dtw_distance_exemplar_2': 11.20267825120404,\n",
       "  'dtw_distance_exemplar_3': 0.7348469228349536,\n",
       "  'dtw_distance_exemplar_4': 1.6703293088490065},\n",
       " {'label': 4,\n",
       "  'time_series': [5.5, 6.6, 7.7, 8.8, 9.9],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2,\n",
       "  'dtw_distance_exemplar_1': 10.78424777163433,\n",
       "  'dtw_distance_exemplar_2': 0.7416198487095661,\n",
       "  'dtw_distance_exemplar_3': 11.244998888394788,\n",
       "  'dtw_distance_exemplar_4': 7.784600182411426},\n",
       " {'label': 4,\n",
       "  'time_series': [6.1, 6.2, 6.3, 6.4, 6.5],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'exemplar_1_label': 3,\n",
       "  'exemplar_2': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_2_label': 4,\n",
       "  'exemplar_3': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_3_label': 1,\n",
       "  'exemplar_4': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_4_label': 2,\n",
       "  'dtw_distance_exemplar_1': 8.96102672688794,\n",
       "  'dtw_distance_exemplar_2': 4.6097722286464435,\n",
       "  'dtw_distance_exemplar_3': 8.423182296495783,\n",
       "  'dtw_distance_exemplar_4': 6.268971207462991}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row.get('time_series', [])\n",
    "        \n",
    "        # Check for individual exemplar columns (exemplar_1, exemplar_2, etc.)\n",
    "        exemplar_columns = {k: v for k, v in row.items() if k.startswith('exemplar_') and isinstance(v, list)}\n",
    "        \n",
    "        if not exemplar_columns:\n",
    "            # Try to get exemplars from the 'exemplars' list if individual columns aren't found\n",
    "            exemplars = row.get('exemplars', [])\n",
    "            if not exemplars:\n",
    "                continue  # Skip if no exemplars found\n",
    "            \n",
    "            # Calculate DTW distances for each exemplar in the list\n",
    "            updated_row = {**row}\n",
    "            for i, exemplar in enumerate(exemplars):\n",
    "                dtw_distance = dtw.distance(time_series, exemplar)\n",
    "                updated_row[f\"dtw_distance_exemplar_{i+1}\"] = dtw_distance\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "        else:\n",
    "            # Calculate DTW distances for each exemplar column\n",
    "            updated_row = {**row}\n",
    "            for col_name, exemplar in exemplar_columns.items():\n",
    "                # Extract index from column name (e.g., \"exemplar_1\" -> \"1\")\n",
    "                idx = col_name.split('_')[1]\n",
    "                dtw_distance = dtw.distance(time_series, exemplar)\n",
    "                updated_row[f\"dtw_distance_exemplar_{idx}\"] = dtw_distance\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "# example usage\n",
    "rdd_with_dtw = rdd_with_exemplars.mapPartitions(calc_dtw_distance)\n",
    "rdd_with_dtw.collect()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6deac",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "04397cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 3,\n",
       "  'time_series': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 4,\n",
       "  'time_series': [5.5, 6.6, 7.7, 8.8, 9.9],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 4,\n",
       "  'time_series': [6.1, 6.2, 6.3, 6.4, 6.5],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [0.7, 1.3, 1.9, 2.5, 3.1],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 1,\n",
       "  'time_series': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 1,\n",
       "  'time_series': [0.6, 1.2, 1.8, 2.4, 3.0],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 2,\n",
       "  'time_series': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 3,\n",
       "  'time_series': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 1,\n",
       "  'time_series': [1.3, 2.3, 3.3, 4.3, 5.3],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 1,\n",
       "  'time_series': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 2,\n",
       "  'time_series': [2.2, 3.1, 4.0, 4.9, 5.8],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 2,\n",
       "  'time_series': [2.6, 3.2, 3.8, 4.4, 5.0],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 3,\n",
       "  'time_series': [1.2, 2.0, 2.8, 3.6, 4.4],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 3,\n",
       "  'time_series': [0.6, 1.3, 2.0, 2.7, 3.4],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 4,\n",
       "  'time_series': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 4,\n",
       "  'time_series': [7.0, 7.8, 8.6, 9.4, 10.2],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [0.9, 1.8, 2.7, 3.6, 4.5],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 1,\n",
       "  'time_series': [1.5, 2.1, 2.7, 3.3, 3.9],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [0.8, 1.7, 2.5, 3.2, 4.0],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 2,\n",
       "  'time_series': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 2,\n",
       "  'time_series': [3.0, 3.8, 4.6, 5.4, 6.2],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 4,\n",
       "  'time_series': [6.5, 7.0, 7.5, 8.0, 8.5],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 1,\n",
       "  'time_series': [0.5, 1.0, 1.5, 2.0, 2.5],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 2,\n",
       "  'time_series': [0.6, 1.4, 1.3, 2.1, 2.5],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'closest_exemplar_original_label': 3},\n",
       " {'label': 4,\n",
       "  'time_series': [0.2, 1.9, 1.6, 2.3, 2.7],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'closest_exemplar_original_label': 3}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_closest_exemplar(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        # Get all DTW distances\n",
    "        dtw_distances = {k: v for k, v in row.items() if k.startswith('dtw_distance_exemplar_')}\n",
    "        \n",
    "        if not dtw_distances:\n",
    "            # Create a simplified row without exemplar columns\n",
    "            simplified_row = {k: v for k, v in row.items() \n",
    "                             if not k.startswith('exemplar_')}\n",
    "            updated_rows.append(simplified_row)\n",
    "            continue\n",
    "        \n",
    "        # Find the closest exemplar based on the minimum DTW distance\n",
    "        closest_exemplar_key = min(dtw_distances, key=dtw_distances.get)\n",
    "        min_distance = dtw_distances[closest_exemplar_key]\n",
    "        \n",
    "        # Extract exemplar number from the key (e.g., \"dtw_distance_exemplar_1\" -> \"1\")\n",
    "        exemplar_num = closest_exemplar_key.split('_')[-1]\n",
    "        \n",
    "        # Get the corresponding exemplar time series data and its original label\n",
    "        exemplar_key = f'exemplar_{exemplar_num}'\n",
    "        exemplar_label_key = f'exemplar_{exemplar_num}_label'\n",
    "        exemplar_time_series = row.get(exemplar_key, None)\n",
    "        exemplar_original_label = row.get(exemplar_label_key, None)\n",
    "        \n",
    "        # Create a new row without the DTW distance columns and exemplar columns\n",
    "        updated_row = {k: v for k, v in row.items() \n",
    "                      if not k.startswith('dtw_distance_exemplar_') and not k.startswith('exemplar_')}\n",
    "        \n",
    "        # Add information about the closest exemplar\n",
    "        updated_row['closest_exemplar_id'] = closest_exemplar_key\n",
    "        updated_row['closest_exemplar_data'] = exemplar_time_series\n",
    "        updated_row['closest_exemplar_original_label'] = exemplar_original_label\n",
    "        \n",
    "        updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_closest_exemplar = rdd_with_dtw.mapPartitions(assign_closest_exemplar)\n",
    "rdd_with_closest_exemplar.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471e7d5",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f9daa07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'partition_id': 0,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'exemplar_data': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'exemplar_label': 2,\n",
       "  'gini_reduction': 0.1376420454545454},\n",
       " {'partition_id': 0,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'exemplar_data': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'exemplar_label': 1,\n",
       "  'gini_reduction': 0.16679067460317454},\n",
       " {'partition_id': 0,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'exemplar_data': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'exemplar_label': 4,\n",
       "  'gini_reduction': 0.25260416666666663},\n",
       " {'partition_id': 1,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'exemplar_data': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'exemplar_label': 3,\n",
       "  'gini_reduction': 0.09499999999999997},\n",
       " {'partition_id': 1,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'exemplar_data': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'exemplar_label': 1,\n",
       "  'gini_reduction': 0.04222222222222227},\n",
       " {'partition_id': 1,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'exemplar_data': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'exemplar_label': 4,\n",
       "  'gini_reduction': 0.2533333333333333},\n",
       " {'partition_id': 1,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'exemplar_data': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_label': 2,\n",
       "  'gini_reduction': 0.1723809523809524}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gini(labels):\n",
    "    if not labels:\n",
    "        return 0\n",
    "    label_counts = {}\n",
    "    for label in labels:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    total = sum(label_counts.values())\n",
    "    gini = 1 - sum((count / total) ** 2 for count in label_counts.values()) if total > 0 else 0\n",
    "    return gini\n",
    "\n",
    "def evaluate_splits_within_partition(index, iterator):\n",
    "    partition_data = list(iterator)\n",
    "    results = []\n",
    "    \n",
    "    # Calculate Gini impurity before splitting\n",
    "    labels = [row.get('label') for row in partition_data if row.get('label') is not None]\n",
    "    before_split_gini = calculate_gini(labels)\n",
    "    \n",
    "    # Get all unique exemplars in the partition\n",
    "    unique_exemplars = set(\n",
    "        row['closest_exemplar_id'] for row in partition_data\n",
    "        if row.get('closest_exemplar_id') is not None\n",
    "    )\n",
    "    \n",
    "    # Create a mapping of exemplar_id to exemplar_data and exemplar_label\n",
    "    exemplar_data_map = {}\n",
    "    exemplar_label_map = {}\n",
    "    \n",
    "    for row in partition_data:\n",
    "        exemplar_id = row.get('closest_exemplar_id')\n",
    "        if exemplar_id and exemplar_id not in exemplar_data_map:\n",
    "            exemplar_data_map[exemplar_id] = row.get('closest_exemplar_data')\n",
    "            # Use the original exemplar label instead of the row's label\n",
    "            exemplar_label_map[exemplar_id] = row.get('closest_exemplar_original_label')\n",
    "    \n",
    "    # Evaluate all possible splits\n",
    "    for exemplar_id in unique_exemplars:\n",
    "        # Split the data based on the current exemplar\n",
    "        yes_split = [r for r in partition_data if r.get('closest_exemplar_id') == exemplar_id]\n",
    "        no_split = [r for r in partition_data if r.get('closest_exemplar_id') != exemplar_id]\n",
    "        \n",
    "        # Calculate Gini for each daughter node\n",
    "        yes_labels = [r.get('label') for r in yes_split if r.get('label') is not None]\n",
    "        no_labels = [r.get('label') for r in no_split if r.get('label') is not None]\n",
    "        \n",
    "        yes_gini = calculate_gini(yes_labels)\n",
    "        no_gini = calculate_gini(no_labels)\n",
    "        \n",
    "        # Calculate weighted Gini after split\n",
    "        total_size = len(yes_split) + len(no_split)\n",
    "        weighted_gini = (yes_gini * len(yes_split) / total_size + no_gini * len(no_split) / total_size) if total_size > 0 else float('inf')\n",
    "        \n",
    "        # Calculate Gini reduction\n",
    "        gini_reduction = before_split_gini - weighted_gini if total_size > 0 else float('-inf')\n",
    "        \n",
    "        # Get the exemplar data and label for this exemplar_id\n",
    "        exemplar_data = exemplar_data_map.get(exemplar_id)\n",
    "        exemplar_label = exemplar_label_map.get(exemplar_id)\n",
    "        \n",
    "        # Add this split evaluation to results\n",
    "        results.append({\n",
    "            \"partition_id\": index,\n",
    "            \"exemplar_id\": exemplar_id,\n",
    "            \"exemplar_data\": exemplar_data,\n",
    "            \"exemplar_label\": exemplar_label,\n",
    "            \"gini_reduction\": gini_reduction   \n",
    "        })\n",
    "    \n",
    "    return iter(results)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_splits = rdd_with_closest_exemplar.mapPartitionsWithIndex(evaluate_splits_within_partition)\n",
    "rdd_with_splits.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff060386",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "87b9275b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree for Partition 0:\n",
      "Is closest to exemplar dtw_distance_exemplar_2 (class 4)?\n",
      "Data: [6.0, 7.0, 8.0, 9.0, 10.0], Gini reduction: 0.2526\n",
      "Yes ->\n",
      "  Leaf: Predict class 4\n",
      "No ->\n",
      "  Is closest to exemplar dtw_distance_exemplar_3 (class 1)?\n",
      "  Data: [1.4, 2.0, 2.6, 3.2, 3.8], Gini reduction: 0.1671\n",
      "  Yes ->\n",
      "    Leaf: Predict class 3\n",
      "  No ->\n",
      "    Leaf: Predict class 2\n",
      "\n",
      "Decision Tree for Partition 1:\n",
      "Is closest to exemplar dtw_distance_exemplar_3 (class 4)?\n",
      "Data: [0.9, 1.7, 1.2, 2.4, 2.8], Gini reduction: 0.2533\n",
      "Yes ->\n",
      "  Leaf: Predict class 1\n",
      "No ->\n",
      "  Is closest to exemplar dtw_distance_exemplar_1 (class 1)?\n",
      "  Data: [1.2, 2.4, 3.6, 4.8, 6.0], Gini reduction: 0.2111\n",
      "  Yes ->\n",
      "    Leaf: Predict class 1\n",
      "  No ->\n",
      "    Is closest to exemplar dtw_distance_exemplar_4 (class 3)?\n",
      "    Data: [0.3, 1.7, 1.6, 2.2, 2.6], Gini reduction: 0.0133\n",
      "    Yes ->\n",
      "      Leaf: Predict class 2\n",
      "    No ->\n",
      "      Leaf: Predict class 2\n",
      "\n",
      "Ensemble Accuracy: 0.5000 (13/26)\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "import collections\n",
    "\n",
    "# Create initial test data\n",
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]},\n",
    "    {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
    "    {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
    "    {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
    "    {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
    "    {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
    "    {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
    "    {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
    "    {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
    "    {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.4, 1.9, 2.4, 2.9]},\n",
    "    {'label': 1, 'time_series': [1.4, 2.0, 2.6, 3.2, 3.8]},\n",
    "    {'label': 2, 'time_series': [2.2, 3.1, 4.0, 4.9, 5.8]},\n",
    "    {'label': 2, 'time_series': [2.6, 3.2, 3.8, 4.4, 5.0]},\n",
    "    {'label': 3, 'time_series': [1.2, 2.0, 2.8, 3.6, 4.4]},\n",
    "    {'label': 3, 'time_series': [0.6, 1.3, 2.0, 2.7, 3.4]},\n",
    "    {'label': 4, 'time_series': [6.3, 6.5, 6.7, 6.9, 7.1]},\n",
    "    {'label': 4, 'time_series': [7.0, 7.8, 8.6, 9.4, 10.2]},\n",
    "    {'label': 4, 'time_series': [6.5, 7.0, 7.5, 8.0, 8.5]},\n",
    "    {'label': 1, 'time_series': [0.5, 1.0, 1.5, 2.0, 2.5]},\n",
    "    {'label': 2, 'time_series': [0.6, 1.4, 1.3, 2.1, 2.5]},\n",
    "    {'label': 3, 'time_series': [0.3, 1.7, 1.6, 2.2, 2.6]},\n",
    "    {'label': 4, 'time_series': [0.2, 1.9, 1.6, 2.3, 2.7]},\n",
    "    {'label': 4, 'time_series': [0.9, 1.7, 1.2, 2.4, 2.8]}\n",
    "]\n",
    "\n",
    "def repartition_sparkdf(df, num_partitions):\n",
    "    rdd = df.rdd\n",
    "    rdd = rdd.repartition(num_partitions)\n",
    "    rdd = rdd.mapPartitionsWithIndex(\n",
    "            lambda idx, iter: [{**row.asDict(), \"partition_id\": idx} for row in iter]\n",
    "        )\n",
    "    return rdd\n",
    "\n",
    "def choose_exemplars(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    if not partition_data:\n",
    "        return iter([])\n",
    "    \n",
    "    # Group data by class\n",
    "    grouped_data_by_class = {}\n",
    "    for row in partition_data:\n",
    "        label = row.get('label')\n",
    "        if label is not None:\n",
    "            grouped_data_by_class.setdefault(label, []).append(row)\n",
    "    \n",
    "    # Select one exemplar per class and track exemplar-to-label mapping\n",
    "    chosen_exemplars = []\n",
    "    exemplar_labels = {}  # Map to track exemplar labels\n",
    "    \n",
    "    for label, instances in grouped_data_by_class.items():\n",
    "        if instances:  # Ensure there are instances for the class\n",
    "            exemplar = sample(instances, 1)[0]\n",
    "            chosen_exemplars.append((exemplar['time_series'], label))  # Store tuple of (time_series, label)\n",
    "    \n",
    "    # Remove chosen exemplars from the working data\n",
    "    exemplar_time_series = [ex[0] for ex in chosen_exemplars]\n",
    "    filtered_partition = [\n",
    "        row for row in partition_data\n",
    "        if row['time_series'] not in exemplar_time_series\n",
    "    ]\n",
    "    \n",
    "    # Return rows with individual exemplar columns and their labels\n",
    "    result = []\n",
    "    for row in filtered_partition:\n",
    "        new_row = {**row}\n",
    "        # Add each exemplar and its label as columns\n",
    "        for i, (exemplar, label) in enumerate(chosen_exemplars, 1):\n",
    "            new_row[f\"exemplar_{i}\"] = exemplar\n",
    "            new_row[f\"exemplar_{i}_label\"] = label\n",
    "        result.append(new_row)\n",
    "    \n",
    "    return iter(result)\n",
    "\n",
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row.get('time_series', [])\n",
    "        \n",
    "        # Check for individual exemplar columns (exemplar_1, exemplar_2, etc.)\n",
    "        exemplar_columns = {k: v for k, v in row.items() \n",
    "                           if k.startswith('exemplar_') and not k.endswith('_label') and isinstance(v, list)}\n",
    "        \n",
    "        if not exemplar_columns:\n",
    "            # Try to get exemplars from the 'exemplars' list if individual columns aren't found\n",
    "            exemplars = row.get('exemplars', [])\n",
    "            if not exemplars:\n",
    "                continue  # Skip if no exemplars found\n",
    "            \n",
    "            # Calculate DTW distances for each exemplar in the list\n",
    "            updated_row = {**row}\n",
    "            for i, exemplar in enumerate(exemplars):\n",
    "                dtw_distance = dtw.distance(time_series, exemplar)\n",
    "                updated_row[f\"dtw_distance_exemplar_{i+1}\"] = dtw_distance\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "        else:\n",
    "            # Calculate DTW distances for each exemplar column\n",
    "            updated_row = {**row}\n",
    "            for col_name, exemplar in exemplar_columns.items():\n",
    "                # Extract index from column name (e.g., \"exemplar_1\" -> \"1\")\n",
    "                idx = col_name.split('_')[1]\n",
    "                dtw_distance = dtw.distance(time_series, exemplar)\n",
    "                updated_row[f\"dtw_distance_exemplar_{idx}\"] = dtw_distance\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "def assign_closest_exemplar(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        # Get all DTW distances\n",
    "        dtw_distances = {k: v for k, v in row.items() if k.startswith('dtw_distance_exemplar_')}\n",
    "        \n",
    "        if not dtw_distances:\n",
    "            # Create a simplified row without exemplar columns\n",
    "            simplified_row = {k: v for k, v in row.items() \n",
    "                             if not k.startswith('exemplar_')}\n",
    "            updated_rows.append(simplified_row)\n",
    "            continue\n",
    "        \n",
    "        # Find the closest exemplar based on the minimum DTW distance\n",
    "        closest_exemplar_key = min(dtw_distances, key=dtw_distances.get)\n",
    "        min_distance = dtw_distances[closest_exemplar_key]\n",
    "        \n",
    "        # Extract exemplar number from the key (e.g., \"dtw_distance_exemplar_1\" -> \"1\")\n",
    "        exemplar_num = closest_exemplar_key.split('_')[-1]\n",
    "        \n",
    "        # Get the corresponding exemplar time series data and its original label\n",
    "        exemplar_key = f'exemplar_{exemplar_num}'\n",
    "        exemplar_label_key = f'exemplar_{exemplar_num}_label'\n",
    "        exemplar_time_series = row.get(exemplar_key, None)\n",
    "        exemplar_original_label = row.get(exemplar_label_key, None)\n",
    "        \n",
    "        # Create a new row without the DTW distance columns and exemplar columns\n",
    "        updated_row = {k: v for k, v in row.items() \n",
    "                      if not k.startswith('dtw_distance_exemplar_') and not k.startswith('exemplar_')}\n",
    "        \n",
    "        # Add information about the closest exemplar\n",
    "        updated_row['closest_exemplar_id'] = closest_exemplar_key\n",
    "        updated_row['closest_exemplar_data'] = exemplar_time_series\n",
    "        updated_row['closest_exemplar_original_label'] = exemplar_original_label\n",
    "        \n",
    "        updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "def calculate_gini(labels):\n",
    "    if not labels:\n",
    "        return 0\n",
    "    label_counts = {}\n",
    "    for label in labels:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    total = sum(label_counts.values())\n",
    "    gini = 1 - sum((count / total) ** 2 for count in label_counts.values()) if total > 0 else 0\n",
    "    return gini\n",
    "\n",
    "# Define tree node structure\n",
    "class TreeNode:\n",
    "    def __init__(self, node_id=None, split_exemplar_id=None, split_exemplar_data=None, \n",
    "                 split_exemplar_label=None, is_leaf=False, predicted_label=None, gini_reduction=None,\n",
    "                 partition_id=None):\n",
    "        self.node_id = node_id\n",
    "        self.split_exemplar_id = split_exemplar_id\n",
    "        self.split_exemplar_data = split_exemplar_data\n",
    "        self.split_exemplar_label = split_exemplar_label\n",
    "        self.gini_reduction = gini_reduction\n",
    "        self.is_leaf = is_leaf\n",
    "        self.predicted_label = predicted_label\n",
    "        self.yes_child = None  # Instances closest to this exemplar\n",
    "        self.no_child = None   # Instances closest to other exemplars\n",
    "        self.partition_id = partition_id  # Track which partition built this tree\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert tree to dictionary for serialization\"\"\"\n",
    "        result = {\n",
    "            'node_id': self.node_id,\n",
    "            'split_exemplar_id': self.split_exemplar_id,\n",
    "            'split_exemplar_data': self.split_exemplar_data,\n",
    "            'split_exemplar_label': self.split_exemplar_label,\n",
    "            'gini_reduction': self.gini_reduction,\n",
    "            'is_leaf': self.is_leaf,\n",
    "            'predicted_label': self.predicted_label,\n",
    "            'partition_id': self.partition_id,\n",
    "        }\n",
    "        \n",
    "        if self.yes_child:\n",
    "            result['yes_child'] = self.yes_child.to_dict()\n",
    "        if self.no_child:\n",
    "            result['no_child'] = self.no_child.to_dict()\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Find the best split from all evaluated splits (within a single partition)\n",
    "def find_best_split(all_splits):\n",
    "    if not all_splits:\n",
    "        return None\n",
    "    return max(all_splits, key=lambda x: x['gini_reduction'])\n",
    "\n",
    "# Build a tree for a single partition (locally, not distributed)\n",
    "def build_partition_tree(partition_data, partition_id, max_depth=3, min_samples=2, node_id=1):\n",
    "    # If too few samples or max depth reached, create a leaf node\n",
    "    if len(partition_data) < min_samples or max_depth <= 0:\n",
    "        # Determine the majority class for prediction\n",
    "        label_counts = collections.Counter([row.get('label') for row in partition_data if row.get('label') is not None])\n",
    "        if not label_counts:\n",
    "            return TreeNode(node_id=node_id, is_leaf=True, predicted_label=None, partition_id=partition_id)\n",
    "        \n",
    "        majority_class = label_counts.most_common(1)[0][0]\n",
    "        return TreeNode(node_id=node_id, is_leaf=True, predicted_label=majority_class, partition_id=partition_id)\n",
    "    \n",
    "    # Evaluate all possible splits (reuse your existing function logic)\n",
    "    labels = [row.get('label') for row in partition_data if row.get('label') is not None]\n",
    "    before_split_gini = calculate_gini(labels)\n",
    "    \n",
    "    # Get all unique exemplars in the partition\n",
    "    unique_exemplars = set(\n",
    "        row['closest_exemplar_id'] for row in partition_data\n",
    "        if row.get('closest_exemplar_id') is not None\n",
    "    )\n",
    "    \n",
    "    # Create a mapping of exemplar_id to exemplar_data and exemplar_label\n",
    "    exemplar_data_map = {}\n",
    "    exemplar_label_map = {}\n",
    "    \n",
    "    for row in partition_data:\n",
    "        exemplar_id = row.get('closest_exemplar_id')\n",
    "        if exemplar_id and exemplar_id not in exemplar_data_map:\n",
    "            exemplar_data_map[exemplar_id] = row.get('closest_exemplar_data')\n",
    "            exemplar_label_map[exemplar_id] = row.get('closest_exemplar_original_label')\n",
    "    \n",
    "    # Evaluate all possible splits\n",
    "    splits = []\n",
    "    for exemplar_id in unique_exemplars:\n",
    "        # Split the data based on the current exemplar\n",
    "        yes_split = [r for r in partition_data if r.get('closest_exemplar_id') == exemplar_id]\n",
    "        no_split = [r for r in partition_data if r.get('closest_exemplar_id') != exemplar_id]\n",
    "        \n",
    "        # Calculate Gini for each daughter node\n",
    "        yes_labels = [r.get('label') for r in yes_split if r.get('label') is not None]\n",
    "        no_labels = [r.get('label') for r in no_split if r.get('label') is not None]\n",
    "        \n",
    "        yes_gini = calculate_gini(yes_labels)\n",
    "        no_gini = calculate_gini(no_labels)\n",
    "        \n",
    "        # Calculate weighted Gini after split\n",
    "        total_size = len(yes_split) + len(no_split)\n",
    "        weighted_gini = (yes_gini * len(yes_split) / total_size + no_gini * len(no_split) / total_size) if total_size > 0 else float('inf')\n",
    "        \n",
    "        # Calculate Gini reduction\n",
    "        gini_reduction = before_split_gini - weighted_gini if total_size > 0 else float('-inf')\n",
    "        \n",
    "        # Get the exemplar data and label for this exemplar_id\n",
    "        exemplar_data = exemplar_data_map.get(exemplar_id)\n",
    "        exemplar_label = exemplar_label_map.get(exemplar_id)\n",
    "        \n",
    "        # Add this split evaluation to results\n",
    "        splits.append({\n",
    "            \"partition_id\": partition_id,\n",
    "            \"exemplar_id\": exemplar_id,\n",
    "            \"exemplar_data\": exemplar_data,\n",
    "            \"exemplar_label\": exemplar_label,\n",
    "            \"gini_reduction\": gini_reduction   \n",
    "        })\n",
    "    \n",
    "    # Find best split\n",
    "    best_split = find_best_split(splits)\n",
    "    \n",
    "    # If no good split found, create a leaf node\n",
    "    if not best_split or best_split['gini_reduction'] <= 0:\n",
    "        label_counts = collections.Counter([row.get('label') for row in partition_data if row.get('label') is not None])\n",
    "        if not label_counts:\n",
    "            return TreeNode(node_id=node_id, is_leaf=True, predicted_label=None, partition_id=partition_id)\n",
    "        \n",
    "        majority_class = label_counts.most_common(1)[0][0]\n",
    "        return TreeNode(node_id=node_id, is_leaf=True, predicted_label=majority_class, partition_id=partition_id)\n",
    "    \n",
    "    # Create a decision node\n",
    "    node = TreeNode(\n",
    "        node_id=node_id,\n",
    "        split_exemplar_id=best_split['exemplar_id'],\n",
    "        split_exemplar_data=best_split['exemplar_data'],\n",
    "        split_exemplar_label=best_split['exemplar_label'],\n",
    "        gini_reduction=best_split['gini_reduction'],\n",
    "        partition_id=partition_id\n",
    "    )\n",
    "    \n",
    "    # Split the data based on the best exemplar\n",
    "    yes_data = [r for r in partition_data if r.get('closest_exemplar_id') == best_split['exemplar_id']]\n",
    "    no_data = [r for r in partition_data if r.get('closest_exemplar_id') != best_split['exemplar_id']]\n",
    "    \n",
    "    # Recursively build the subtrees\n",
    "    node.yes_child = build_partition_tree(yes_data, partition_id, max_depth-1, min_samples, node_id=node_id*2)\n",
    "    node.no_child = build_partition_tree(no_data, partition_id, max_depth-1, min_samples, node_id=node_id*2+1)\n",
    "    \n",
    "    return node\n",
    "\n",
    "# Function to visualize a single tree\n",
    "def visualize_tree(node, indent=\"\"):\n",
    "    if node is None:\n",
    "        return\n",
    "    \n",
    "    if node.is_leaf:\n",
    "        print(f\"{indent}Leaf: Predict class {node.predicted_label}\")\n",
    "    else:\n",
    "        # Format the exemplar data nicely for display\n",
    "        exemplar_data_str = \"[\" + \", \".join(f\"{val:.1f}\" for val in node.split_exemplar_data) + \"]\"\n",
    "        \n",
    "        print(f\"{indent}Is closest to exemplar {node.split_exemplar_id} (class {node.split_exemplar_label})?\")\n",
    "        print(f\"{indent}Data: {exemplar_data_str}, Gini reduction: {node.gini_reduction:.4f}\")\n",
    "        \n",
    "        print(f\"{indent}Yes ->\")\n",
    "        visualize_tree(node.yes_child, indent + \"  \")\n",
    "        \n",
    "        print(f\"{indent}No ->\")\n",
    "        visualize_tree(node.no_child, indent + \"  \")\n",
    "\n",
    "# Function to predict with a single tree\n",
    "def predict_with_tree(row, tree):\n",
    "    current_node = tree\n",
    "    \n",
    "    while not current_node.is_leaf:\n",
    "        closest_exemplar = row.get('closest_exemplar_id')\n",
    "        \n",
    "        if closest_exemplar == current_node.split_exemplar_id:\n",
    "            current_node = current_node.yes_child\n",
    "        else:\n",
    "            current_node = current_node.no_child\n",
    "    \n",
    "    return current_node.predicted_label\n",
    "\n",
    "# Main function to run the full pipeline\n",
    "def main():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"ExemplarTreeEnsemble\").getOrCreate()\n",
    "    \n",
    "    # Create DataFrame from the test data\n",
    "    df = spark.createDataFrame(tsdata)\n",
    "    \n",
    "    # Repartition data\n",
    "    rdd = repartition_sparkdf(df, 2)\n",
    "    \n",
    "    # Choose exemplars\n",
    "    rdd_with_exemplars = rdd.mapPartitions(choose_exemplars)\n",
    "    \n",
    "    # Calculate DTW distances\n",
    "    \n",
    "    rdd_with_dtw = rdd_with_exemplars.mapPartitions(calc_dtw_distance)\n",
    "    \n",
    "    # Assign closest exemplars\n",
    "    rdd_with_closest_exemplar = rdd_with_dtw.mapPartitions(assign_closest_exemplar)\n",
    "    \n",
    "    # Collect all data (for our small test dataset, this is fine)\n",
    "    all_data = rdd_with_closest_exemplar.collect()\n",
    "    \n",
    "    # Group by partition\n",
    "    partitions = {}\n",
    "    for row in all_data:\n",
    "        pid = row['partition_id']\n",
    "        if pid not in partitions:\n",
    "            partitions[pid] = []\n",
    "        partitions[pid].append(row)\n",
    "    \n",
    "    # Build trees for each partition\n",
    "    trees = []\n",
    "    for pid, data in partitions.items():\n",
    "        if data:  # Make sure there's data in this partition\n",
    "            tree = build_partition_tree(data, pid, max_depth=3, min_samples=2)\n",
    "            trees.append(tree)\n",
    "    \n",
    "    # Visualize each tree\n",
    "    for i, tree in enumerate(trees):\n",
    "        print(f\"\\nDecision Tree for Partition {tree.partition_id}:\")\n",
    "        visualize_tree(tree)\n",
    "    \n",
    "    # Make predictions and evaluate accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for row in all_data:\n",
    "        # Get predictions from all trees\n",
    "        predictions = [predict_with_tree(row, tree) for tree in trees]\n",
    "        # Take majority vote\n",
    "        if predictions:\n",
    "            counter = collections.Counter(predictions)\n",
    "            ensemble_prediction = counter.most_common(1)[0][0]\n",
    "            \n",
    "            # Update accuracy counts\n",
    "            total += 1\n",
    "            if ensemble_prediction == row.get('label'):\n",
    "                correct += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\nEnsemble Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "    \n",
    "    # Save the ensemble for future use (optional)\n",
    "    with open(\"exemplar_tree_ensemble.json\", \"w\") as f:\n",
    "        json.dump([tree.to_dict() for tree in trees], f, indent=2)\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0bbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
