{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc7c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from data_ingestion import DataIngestion\n",
    "from preprocessing import Preprocessor\n",
    "from prediction_manager import PredictionManager\n",
    "from local_model_manager import LocalModelManager\n",
    "from evaluation import Evaluator\n",
    "from utilities import show_compact\n",
    "import time\n",
    "import json\n",
    "from random import sample\n",
    "from dtaidistance import dtw\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"GenericRDD\").getOrCreate()\n",
    "\n",
    "# Access the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f30b2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]},\n",
    "    {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
    "    {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
    "    {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
    "    {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
    "    {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
    "    {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
    "    {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
    "    {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
    "    {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.4, 1.9, 2.4, 2.9]},\n",
    "    {'label': 1, 'time_series': [1.4, 2.0, 2.6, 3.2, 3.8]},\n",
    "    {'label': 2, 'time_series': [2.2, 3.1, 4.0, 4.9, 5.8]},\n",
    "    {'label': 2, 'time_series': [2.6, 3.2, 3.8, 4.4, 5.0]},\n",
    "    {'label': 3, 'time_series': [1.2, 2.0, 2.8, 3.6, 4.4]},\n",
    "    {'label': 3, 'time_series': [0.6, 1.3, 2.0, 2.7, 3.4]},\n",
    "    {'label': 4, 'time_series': [6.3, 6.5, 6.7, 6.9, 7.1]},\n",
    "    {'label': 4, 'time_series': [7.0, 7.8, 8.6, 9.4, 10.2]},\n",
    "    {'label': 4, 'time_series': [6.5, 7.0, 7.5, 8.0, 8.5]},\n",
    "    {'label': 1, 'time_series': [0.5, 1.0, 1.5, 2.0, 2.5]}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(tsdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669a74f",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee4551f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0], 'partition_id': 0}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repartition_sparkdf(df, num_partitions):\n",
    "    rdd = df.rdd\n",
    "    rdd = rdd.repartition(num_partitions)\n",
    "    rdd = rdd.mapPartitionsWithIndex(\n",
    "            lambda idx, iter: [{**row.asDict(), \"partition_id\": idx} for row in iter]\n",
    "        )\n",
    "    return rdd\n",
    "\n",
    "# example usage\n",
    "rdd = repartition_sparkdf(df, 1)\n",
    "print(rdd.getNumPartitions())  # should print 1\n",
    "rdd.collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be92f01",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee6bc902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 1,\n",
       "  'time_series': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_4': [7.0, 7.8, 8.6, 9.4, 10.2]},\n",
       " {'label': 1,\n",
       "  'time_series': [0.9, 1.8, 2.7, 3.6, 4.5],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_4': [7.0, 7.8, 8.6, 9.4, 10.2]},\n",
       " {'label': 1,\n",
       "  'time_series': [1.5, 2.1, 2.7, 3.3, 3.9],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_4': [7.0, 7.8, 8.6, 9.4, 10.2]}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_exemplars(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    if not partition_data:\n",
    "        return iter([])\n",
    "    \n",
    "    # Group data by class\n",
    "    grouped_data_by_class = {}\n",
    "    for row in partition_data:\n",
    "        label = row.get('label')\n",
    "        if label is not None:\n",
    "            grouped_data_by_class.setdefault(label, []).append(row)\n",
    "    \n",
    "    # Select one exemplar per class\n",
    "    chosen_exemplars = []\n",
    "    for label, instances in grouped_data_by_class.items():\n",
    "        if instances:  # Ensure there are instances for the class\n",
    "            exemplar = sample(instances, 1)[0]\n",
    "            chosen_exemplars.append(exemplar['time_series'])\n",
    "    \n",
    "    # Remove chosen exemplars from the working data\n",
    "    filtered_partition = [\n",
    "        row for row in partition_data\n",
    "        if row['time_series'] not in chosen_exemplars\n",
    "    ]\n",
    "    \n",
    "    # Return rows with individual exemplar columns\n",
    "    result = []\n",
    "    for row in filtered_partition:\n",
    "        new_row = {**row}\n",
    "        # Add each exemplar as its own column\n",
    "        for i, exemplar in enumerate(chosen_exemplars):\n",
    "            new_row[f\"exemplar_{i+1}\"] = exemplar\n",
    "        result.append(new_row)\n",
    "    \n",
    "    return iter(result)\n",
    "\n",
    "# example usage\n",
    "rdd_with_exemplars = rdd.mapPartitions(choose_exemplars)\n",
    "rdd_with_exemplars.collect()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a538e",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da68b739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 1,\n",
       "  'time_series': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_4': [7.0, 7.8, 8.6, 9.4, 10.2],\n",
       "  'dtw_distance_exemplar_1': 2.019900987672415,\n",
       "  'dtw_distance_exemplar_2': 2.4124676163629633,\n",
       "  'dtw_distance_exemplar_3': 2.3790754506740637,\n",
       "  'dtw_distance_exemplar_4': 10.507140429250956},\n",
       " {'label': 1,\n",
       "  'time_series': [0.9, 1.8, 2.7, 3.6, 4.5],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_4': [7.0, 7.8, 8.6, 9.4, 10.2],\n",
       "  'dtw_distance_exemplar_1': 0.38729833462074165,\n",
       "  'dtw_distance_exemplar_2': 3.7709415269929605,\n",
       "  'dtw_distance_exemplar_3': 1.2806248474865698,\n",
       "  'dtw_distance_exemplar_4': 13.105723940324701},\n",
       " {'label': 1,\n",
       "  'time_series': [1.5, 2.1, 2.7, 3.3, 3.9],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'exemplar_2': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_4': [7.0, 7.8, 8.6, 9.4, 10.2],\n",
       "  'dtw_distance_exemplar_1': 0.670820393249937,\n",
       "  'dtw_distance_exemplar_2': 4.009987531152684,\n",
       "  'dtw_distance_exemplar_3': 0.66332495807108,\n",
       "  'dtw_distance_exemplar_4': 13.207952150125317}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row.get('time_series', [])\n",
    "        \n",
    "        # Check for individual exemplar columns (exemplar_1, exemplar_2, etc.)\n",
    "        exemplar_columns = {k: v for k, v in row.items() if k.startswith('exemplar_') and isinstance(v, list)}\n",
    "        \n",
    "        if not exemplar_columns:\n",
    "            # Try to get exemplars from the 'exemplars' list if individual columns aren't found\n",
    "            exemplars = row.get('exemplars', [])\n",
    "            if not exemplars:\n",
    "                continue  # Skip if no exemplars found\n",
    "            \n",
    "            # Calculate DTW distances for each exemplar in the list\n",
    "            updated_row = {**row}\n",
    "            for i, exemplar in enumerate(exemplars):\n",
    "                dtw_distance = dtw.distance(time_series, exemplar)\n",
    "                updated_row[f\"dtw_distance_exemplar_{i+1}\"] = dtw_distance\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "        else:\n",
    "            # Calculate DTW distances for each exemplar column\n",
    "            updated_row = {**row}\n",
    "            for col_name, exemplar in exemplar_columns.items():\n",
    "                # Extract index from column name (e.g., \"exemplar_1\" -> \"1\")\n",
    "                idx = col_name.split('_')[1]\n",
    "                dtw_distance = dtw.distance(time_series, exemplar)\n",
    "                updated_row[f\"dtw_distance_exemplar_{idx}\"] = dtw_distance\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "# example usage\n",
    "rdd_with_dtw = rdd_with_exemplars.mapPartitions(calc_dtw_distance)\n",
    "rdd_with_dtw.collect()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6deac",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04397cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 1,\n",
       "  'time_series': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
       " {'label': 1,\n",
       "  'time_series': [0.9, 1.8, 2.7, 3.6, 4.5],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
       " {'label': 1,\n",
       "  'time_series': [1.5, 2.1, 2.7, 3.3, 3.9],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0]}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_closest_exemplar(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        # Get all DTW distances\n",
    "        dtw_distances = {k: v for k, v in row.items() if k.startswith('dtw_distance_exemplar_')}\n",
    "        \n",
    "        if not dtw_distances:\n",
    "            # Create a simplified row without exemplar columns\n",
    "            simplified_row = {k: v for k, v in row.items() \n",
    "                             if not k.startswith('exemplar_')}\n",
    "            updated_rows.append(simplified_row)\n",
    "            continue\n",
    "        \n",
    "        # Find the closest exemplar based on the minimum DTW distance\n",
    "        closest_exemplar_key = min(dtw_distances, key=dtw_distances.get)\n",
    "        min_distance = dtw_distances[closest_exemplar_key]\n",
    "        \n",
    "        # Extract exemplar number from the key (e.g., \"dtw_distance_exemplar_1\" -> \"1\")\n",
    "        exemplar_num = closest_exemplar_key.split('_')[-1]\n",
    "        \n",
    "        # Get the corresponding exemplar time series data\n",
    "        exemplar_key = f'exemplar_{exemplar_num}'\n",
    "        exemplar_time_series = row.get(exemplar_key, None)\n",
    "        \n",
    "        # Create a new row without the DTW distance columns and exemplar columns\n",
    "        updated_row = {k: v for k, v in row.items() \n",
    "                      if not k.startswith('dtw_distance_exemplar_') and not k.startswith('exemplar_')}\n",
    "        \n",
    "        # Add information about the closest exemplar\n",
    "        updated_row['closest_exemplar_id'] = closest_exemplar_key\n",
    "        # updated_row['closest_exemplar_distance'] = min_distance\n",
    "        updated_row['closest_exemplar_data'] = exemplar_time_series\n",
    "        \n",
    "        updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_closest_exemplar = rdd_with_dtw.mapPartitions(assign_closest_exemplar)\n",
    "rdd_with_closest_exemplar.collect()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9daa07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'exemplar_data': [7.0, 7.8, 8.6, 9.4, 10.2],\n",
       "  'gini_reduction': 0.1234909090909091,\n",
       "  'partition_id': 0},\n",
       " {'exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'exemplar_data': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'gini_reduction': 0.23645128205128213,\n",
       "  'partition_id': 0},\n",
       " {'exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'gini_reduction': 0.033793939393939376,\n",
       "  'partition_id': 0},\n",
       " {'exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'exemplar_data': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'gini_reduction': 0.12878596491228078,\n",
       "  'partition_id': 0}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gini(labels):\n",
    "    if not labels:\n",
    "        return 0\n",
    "    label_counts = {}\n",
    "    for label in labels:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    total = sum(label_counts.values())\n",
    "    gini = 1 - sum((count / total) ** 2 for count in label_counts.values()) if total > 0 else 0\n",
    "    return gini\n",
    "\n",
    "def evaluate_splits_within_partition(index, iterator):\n",
    "    partition_data = list(iterator)\n",
    "    results = []\n",
    "    \n",
    "    # Calculate Gini impurity before splitting\n",
    "    labels = [row.get('label') for row in partition_data if row.get('label') is not None]\n",
    "    before_split_gini = calculate_gini(labels)\n",
    "    \n",
    "    # Get all unique exemplars in the partition\n",
    "    unique_exemplars = set(\n",
    "        row['closest_exemplar_id'] for row in partition_data\n",
    "        if row.get('closest_exemplar_id') is not None\n",
    "    )\n",
    "    \n",
    "    # Create a mapping of exemplar_id to exemplar_data\n",
    "    exemplar_data_map = {}\n",
    "    for row in partition_data:\n",
    "        exemplar_id = row.get('closest_exemplar_id')\n",
    "        if exemplar_id and exemplar_id not in exemplar_data_map:\n",
    "            exemplar_data_map[exemplar_id] = row.get('closest_exemplar_data')\n",
    "    \n",
    "    # Evaluate all possible splits\n",
    "    for exemplar_id in unique_exemplars:\n",
    "        # Split the data based on the current exemplar\n",
    "        yes_split = [r for r in partition_data if r.get('closest_exemplar_id') == exemplar_id]\n",
    "        no_split = [r for r in partition_data if r.get('closest_exemplar_id') != exemplar_id]\n",
    "        \n",
    "        # Calculate Gini for each daughter node\n",
    "        yes_labels = [r.get('label') for r in yes_split if r.get('label') is not None]\n",
    "        no_labels = [r.get('label') for r in no_split if r.get('label') is not None]\n",
    "        \n",
    "        yes_gini = calculate_gini(yes_labels)\n",
    "        no_gini = calculate_gini(no_labels)\n",
    "        \n",
    "        # Calculate weighted Gini after split\n",
    "        total_size = len(yes_split) + len(no_split)\n",
    "        weighted_gini = (yes_gini * len(yes_split) / total_size + no_gini * len(no_split) / total_size) if total_size > 0 else float('inf')\n",
    "        \n",
    "        # Calculate Gini reduction\n",
    "        gini_reduction = before_split_gini - weighted_gini if total_size > 0 else float('-inf')\n",
    "        \n",
    "        # Get the exemplar data for this exemplar_id\n",
    "        exemplar_data = exemplar_data_map.get(exemplar_id)\n",
    "        \n",
    "        # Add this split evaluation to results\n",
    "        results.append({\n",
    "            \"exemplar_id\": exemplar_id,\n",
    "            \"exemplar_data\": exemplar_data,\n",
    "            \"gini_reduction\": gini_reduction,\n",
    "            \"partition_id\": index\n",
    "        })\n",
    "    \n",
    "    return iter(results)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_splits = rdd_with_closest_exemplar.mapPartitionsWithIndex(evaluate_splits_within_partition)\n",
    "rdd_with_splits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67b878db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decision_tree_root(split_evaluations):\n",
    "    # Find the exemplar with the highest Gini reduction\n",
    "    best_split = max(split_evaluations, key=lambda x: x['gini_reduction'])\n",
    "    \n",
    "    # Create the root node of the decision tree\n",
    "    root_node = {\n",
    "        'split_feature': best_split['exemplar_id'],\n",
    "        'split_value': best_split['exemplar_data'],\n",
    "        'gini_reduction': best_split['gini_reduction'],\n",
    "        'is_leaf': False,\n",
    "        'left_child': None,  # Will be populated later (YES branch)\n",
    "        'right_child': None  # Will be populated later (NO branch)\n",
    "    }\n",
    "    \n",
    "    return root_node\n",
    "\n",
    "# Use the function with your collected results\n",
    "collected_results = [\n",
    "    {'exemplar_id': 'dtw_distance_exemplar_4', 'exemplar_data': [7.0, 7.8, 8.6, 9.4, 10.2], 'gini_reduction': 0.1234909090909091, 'partition_id': 0}, \n",
    "    {'exemplar_id': 'dtw_distance_exemplar_1', 'exemplar_data': [1.0, 1.8, 2.6, 3.4, 4.2], 'gini_reduction': 0.23645128205128213, 'partition_id': 0}, \n",
    "    {'exemplar_id': 'dtw_distance_exemplar_3', 'exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0], 'gini_reduction': 0.033793939393939376, 'partition_id': 0}, \n",
    "    {'exemplar_id': 'dtw_distance_exemplar_2', 'exemplar_data': [3.3, 4.1, 4.9, 5.7, 6.5], 'gini_reduction': 0.12878596491228078, 'partition_id': 0}\n",
    "]\n",
    "\n",
    "root_node = build_decision_tree_root(collected_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4104c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
