{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9063d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from data_ingestion import DataIngestion\n",
    "from preprocessing import Preprocessor\n",
    "from prediction_manager import PredictionManager\n",
    "from local_model_manager import LocalModelManager\n",
    "from evaluation import Evaluator\n",
    "from utilities import show_compact\n",
    "import time\n",
    "import json\n",
    "from random import sample\n",
    "from dtaidistance import dtw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01e9ea",
   "metadata": {},
   "source": [
    "## ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84413ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"GenericRDD\").getOrCreate()\n",
    "\n",
    "# Access the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a0e00",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0715b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"label\": 1, \"time_series\": [1.0, 2.1, 3.2, 4.3, 5.4]},\n",
    "    {\"label\": 2, \"time_series\": [2.0, 3.1, 4.2, 5.3, 6.4]},\n",
    "    {\"label\": 3, \"time_series\": [3.0, 4.1, 5.2, 6.3, 7.4]},\n",
    "    {\"label\": 4, \"time_series\": [4.0, 5.1, 6.2, 7.3, 8.4]},\n",
    "    {\"label\": 1, \"time_series\": [1.5, 2.6, 3.7, 4.8, 5.9]},\n",
    "    {\"label\": 2, \"time_series\": [2.5, 3.6, 4.7, 5.8, 6.9]},\n",
    "    {\"label\": 3, \"time_series\": [3.5, 4.6, 5.7, 6.8, 7.9]},\n",
    "    {\"label\": 4, \"time_series\": [4.5, 5.6, 6.7, 7.8, 8.9]}\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae169e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.repartition(2)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "414dc4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: {'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4]}\n",
      "Partition 0: {'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "Partition 0: {'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9]}\n",
      "Partition 0: {'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9]}\n",
      "Partition 0: {'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9]}\n",
      "Partition 1: {'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4]}\n",
      "Partition 1: {'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4]}\n",
      "Partition 1: {'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9]}\n"
     ]
    }
   ],
   "source": [
    "def print_partition_rows(index, iterator):\n",
    "    # Add partition index to each row\n",
    "    return [(index, row) for row in iterator]\n",
    "\n",
    "# Use mapPartitionsWithIndex to include partition index\n",
    "partitioned_rdd = rdd.mapPartitionsWithIndex(print_partition_rows)\n",
    "\n",
    "# Collect and print the rows along with their partition index\n",
    "for partition_index, row in partitioned_rdd.collect():\n",
    "    print(f\"Partition {partition_index}: {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bde2d9",
   "metadata": {},
   "source": [
    "# adding exemplar column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7f8c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4]}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4]}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4]}\n"
     ]
    }
   ],
   "source": [
    "def sample_and_add_column(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    sampled_element = sample(partition_data, 1)[0]['time_series']\n",
    "    return iter([{**row, \"exemplar\": sampled_element} for row in partition_data])\n",
    "\n",
    "rdd_with_sampled_column = rdd.mapPartitions(sample_and_add_column)\n",
    "\n",
    "# Collect and print the updated RDD\n",
    "for row in rdd_with_sampled_column.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d7f27e",
   "metadata": {},
   "source": [
    "# calculating DTW distance using time series and exemplar columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd9ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4], 'dtw_distance': 1.42828568570857}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4], 'dtw_distance': 0.0}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4], 'dtw_distance': 4.085339643163099}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4], 'dtw_distance': 2.2671568097509267}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4], 'dtw_distance': 1.118033988749895}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4], 'dtw_distance': 0.0}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4], 'dtw_distance': 1.42828568570857}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4], 'dtw_distance': 6.283311228962003}\n"
     ]
    }
   ],
   "source": [
    "# def calc_dtw_distance(iterator):\n",
    "#     partition_data = list(iterator)\n",
    "#     time_series = partition_data['time_series']\n",
    "#     exemplar = partition_data['exemplar']\n",
    "#     dtw_distance = dtw.distance(time_series, exemplar)\n",
    "#     return iter([{**row, \"dtw_distance\": dtw_distance} for row in partition_data])\n",
    "\n",
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row['time_series']\n",
    "        exemplar = row['exemplar']\n",
    "        \n",
    "        dtw_distance = dtw.distance(time_series, exemplar)\n",
    "        \n",
    "        updated_row = {**row, \"dtw_distance\": dtw_distance}\n",
    "        updated_rows.append(updated_row)\n",
    "    return iter(updated_rows)\n",
    "\n",
    "rdd_with_dtw = rdd_with_sampled_column.mapPartitions(calc_dtw_distance)\n",
    "for row in rdd_with_dtw.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59dd988",
   "metadata": {},
   "source": [
    "# WORKS FOR ANY NUM OF PARTITIONS AND EXEMPLARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05bf335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]]}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]]}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]]}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]]}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]]}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]]}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]]}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]]}\n",
      "\n",
      "rdd num partitions: 2\n"
     ]
    }
   ],
   "source": [
    "def create_sample_and_add_column_function(num_exemplars):\n",
    "    def sample_and_add_column(iterator):\n",
    "        partition_data = list(iterator)\n",
    "        exemplars = []\n",
    "        for row in sample(partition_data, min(num_exemplars, len(partition_data))):\n",
    "            exemplars.append(row['time_series'])\n",
    "        return iter([{**row, \"exemplars\": exemplars} for row in partition_data])\n",
    "    return sample_and_add_column\n",
    "\n",
    "# Example usage\n",
    "num_exemplars = 2\n",
    "sample_and_add_column = create_sample_and_add_column_function(num_exemplars)\n",
    "rdd_with_exemplar_column = rdd.mapPartitions(sample_and_add_column)\n",
    "for row in rdd_with_exemplar_column.collect():\n",
    "    print(row)\n",
    "\n",
    "print(f'\\nrdd num partitions: {rdd_with_exemplar_column.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7bed113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 1.42828568570857, 'dtw_distance_exemplar_2': 2.2671568097509267}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 4.085339643163099}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 4.085339643163099, 'dtw_distance_exemplar_2': 0.0}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 2.2671568097509267, 'dtw_distance_exemplar_2': 1.42828568570857}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 1.118033988749895, 'dtw_distance_exemplar_2': 3.1208973068654466}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 1.42828568570857}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 1.42828568570857, 'dtw_distance_exemplar_2': 0.0}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 6.283311228962003, 'dtw_distance_exemplar_2': 4.085339643163099}\n",
      "\n",
      "rdd num partitions: 2\n"
     ]
    }
   ],
   "source": [
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row['time_series']\n",
    "        exemplars = row['exemplars']\n",
    "        \n",
    "        # Calculate DTW distances for each exemplar\n",
    "        dtw_distances = [dtw.distance(time_series, exemplar) for exemplar in exemplars]\n",
    "        \n",
    "        # Add each DTW distance as a separate column\n",
    "        updated_row = {**row}\n",
    "        for i, dtw_distance in enumerate(dtw_distances):\n",
    "            updated_row[f\"dtw_distance_exemplar_{i+1}\"] = dtw_distance\n",
    "        \n",
    "        updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_dtw = rdd_with_exemplar_column.mapPartitions(calc_dtw_distance)\n",
    "for row in rdd_with_dtw.collect():\n",
    "    print(row)\n",
    "\n",
    "print(f'\\nrdd num partitions: {rdd_with_dtw.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bd906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 1.42828568570857, 'dtw_distance_exemplar_2': 2.2671568097509267, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 4.085339643163099, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 4.085339643163099, 'dtw_distance_exemplar_2': 0.0, 'closest exemplar': 'dtw_distance_exemplar_2'}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 2.2671568097509267, 'dtw_distance_exemplar_2': 1.42828568570857, 'closest exemplar': 'dtw_distance_exemplar_2'}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplars': [[4.0, 5.1, 6.2, 7.3, 8.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 1.118033988749895, 'dtw_distance_exemplar_2': 3.1208973068654466, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 1.42828568570857, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 1.42828568570857, 'dtw_distance_exemplar_2': 0.0, 'closest exemplar': 'dtw_distance_exemplar_2'}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 6.283311228962003, 'dtw_distance_exemplar_2': 4.085339643163099, 'closest exemplar': 'dtw_distance_exemplar_2'}\n"
     ]
    }
   ],
   "source": [
    "# not sure if this is needed\n",
    "\n",
    "def assign_closest_exemplar(iterator):\n",
    "    partition_data = list(iterator)\n",
    "\n",
    "    for row in partition_data:\n",
    "        # Check if there are DTW distances for exemplars\n",
    "        exemplar_distances = {key: value for key, value in row.items() if key.startswith(\"dtw_distance_exemplar_\")}\n",
    "        \n",
    "        if exemplar_distances:\n",
    "            # Find the exemplar with the smallest DTW distance\n",
    "            closest_exemplar = min(exemplar_distances, key=exemplar_distances.get)\n",
    "            \n",
    "            # Assign the closest exemplar to the row\n",
    "            row[\"closest exemplar\"] = closest_exemplar\n",
    "\n",
    "    return iter(partition_data)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_classification = rdd_with_dtw.mapPartitions(assign_closest_exemplar)\n",
    "for row in rdd_with_classification.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1539859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_partition_gini(iterator):\n",
    "    labels = [row['label'] for row in iterator]\n",
    "\n",
    "    label_counts_dict = {}\n",
    "    for label in labels:\n",
    "        if label in label_counts_dict:\n",
    "            label_counts_dict[label] += 1\n",
    "        else:\n",
    "            label_counts_dict[label] = 1\n",
    "    \n",
    "    total = sum(label_counts_dict.values())\n",
    "    proportion_sqrd_values = [(count / total) ** 2 for count in label_counts_dict.values()]\n",
    "    gini_impurity = 1 - sum(proportion_sqrd_values)\n",
    "    \n",
    "    return iter([gini_impurity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini impurity of partition 1: 0.72\n",
      "gini impurity of partition 2: 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "gini_rdd = rdd_with_classification.mapPartitions(calculate_partition_gini)\n",
    "\n",
    "# Collect and print the Gini impurity for each partition\n",
    "i=0\n",
    "for gini in gini_rdd.collect():\n",
    "    print(f'gini impurity of partition {i+1}: {gini}')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321d01d",
   "metadata": {},
   "source": [
    "### trying splitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "56633663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_rdd num partitions: 1\n",
      "gini impurity of partition 1: 0.74\n"
     ]
    }
   ],
   "source": [
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0], 'closest_exemplar': 'exemplar_1'},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9], 'closest_exemplar': 'exemplar_2'},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5], 'closest_exemplar': 'exemplar_1'},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2], 'closest_exemplar': 'exemplar_2'},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2], 'closest_exemplar': 'exemplar_1'},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9], 'closest_exemplar': 'exemplar_2'},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0], 'closest_exemplar': 'exemplar_1'},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5], 'closest_exemplar': 'exemplar_2'},\n",
    "    {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5], 'closest_exemplar': 'exemplar_1'},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5], 'closest_exemplar': 'exemplar_2'}\n",
    "]\n",
    "\n",
    "ts_rdd = sc.parallelize(tsdata)\n",
    "ts_rdd = ts_rdd.repartition(1)\n",
    "\n",
    "print(f'ts_rdd num partitions: {ts_rdd.getNumPartitions()}')\n",
    "\n",
    "ts_rdd_gini = ts_rdd.mapPartitions(calculate_partition_gini)\n",
    "# Collect and print the Gini impurity for each partition\n",
    "i=0\n",
    "for gini in ts_rdd_gini.collect():\n",
    "    print(f'gini impurity of partition {i+1}: {gini}')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2062c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 3, 1]\n",
      "[2, 2, 4, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "closestto1_yes = [row['label'] for row in tsdata if row['closest_exemplar'] == 'exemplar_1']\n",
    "closestto1_no = [row['label'] for row in tsdata if row['closest_exemplar'] != 'exemplar_1']\n",
    "\n",
    "print(closestto1_yes)\n",
    "print(closestto1_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "59e67b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rdd_by_closest_exemplar(rdd, exemplar_name):\n",
    "    yes_rdd = rdd.filter(lambda row: row['closest_exemplar'] == exemplar_name)\n",
    "    no_rdd = rdd.filter(lambda row: row['closest_exemplar'] != exemplar_name)\n",
    "    return yes_rdd, no_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bedb55ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes_rdd:\n",
      "{'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0], 'closest_exemplar': 'exemplar_1'}\n",
      "{'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5], 'closest_exemplar': 'exemplar_1'}\n",
      "{'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2], 'closest_exemplar': 'exemplar_1'}\n",
      "{'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0], 'closest_exemplar': 'exemplar_1'}\n",
      "{'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5], 'closest_exemplar': 'exemplar_1'}\n",
      "\n",
      "no_rdd:\n",
      "{'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9], 'closest_exemplar': 'exemplar_2'}\n",
      "{'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2], 'closest_exemplar': 'exemplar_2'}\n",
      "{'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9], 'closest_exemplar': 'exemplar_2'}\n",
      "{'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5], 'closest_exemplar': 'exemplar_2'}\n",
      "{'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5], 'closest_exemplar': 'exemplar_2'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "yes_rdd, no_rdd = split_rdd_by_closest_exemplar(ts_rdd, 'exemplar_1')\n",
    "\n",
    "# Collect and print the results\n",
    "print(\"yes_rdd:\")\n",
    "for row in yes_rdd.collect():\n",
    "    print(row)\n",
    "print(\"\\nno_rdd:\")\n",
    "for row in no_rdd.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c7396243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes_rdd gini impurity: [0.48]\n",
      "no_rdd gini impurity: [0.48]\n"
     ]
    }
   ],
   "source": [
    "print(f'yes_rdd gini impurity: {yes_rdd.mapPartitions(calculate_partition_gini).collect()}')\n",
    "print(f'no_rdd gini impurity: {no_rdd.mapPartitions(calculate_partition_gini).collect()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDDProcessor:\n",
    "    def print_partition_rows(self, index, iterator):\n",
    "        # Add partition index to each row\n",
    "        return [(index, row) for row in iterator]\n",
    "\n",
    "    def sample_and_add_column(self, iterator):\n",
    "        # Add an exemplar column by sampling one element from the partition\n",
    "        partition_data = list(iterator)\n",
    "        sampled_element = sample(partition_data, 1)[0]['time_series']\n",
    "        return iter([{**row, \"exemplar\": sampled_element} for row in partition_data])\n",
    "\n",
    "    def create_sample_and_add_column_function(self, num_exemplars):\n",
    "        # Create a function to add multiple exemplars\n",
    "        def sample_and_add_column(iterator):\n",
    "            partition_data = list(iterator)\n",
    "            exemplars = []\n",
    "            for row in sample(partition_data, min(num_exemplars, len(partition_data))):\n",
    "                exemplars.append(row['time_series'])\n",
    "            return iter([{**row, \"exemplars\": exemplars} for row in partition_data])\n",
    "        return sample_and_add_column\n",
    "\n",
    "    def calc_dtw_distance(self, iterator, dtw_distance_func):\n",
    "        # Calculate DTW distance for each row\n",
    "        partition_data = list(iterator)\n",
    "        updated_rows = []\n",
    "        for row in partition_data:\n",
    "            time_series = row['time_series']\n",
    "            exemplars = row['exemplars']\n",
    "            dtw_distances = [dtw_distance_func(time_series, exemplar) for exemplar in exemplars]\n",
    "            updated_row = {**row}\n",
    "            for i, dtw_distance in enumerate(dtw_distances):\n",
    "                updated_row[f\"dtw_distance_exemplar_{i+1}\"] = dtw_distance\n",
    "            updated_rows.append(updated_row)\n",
    "        return iter(updated_rows)\n",
    "\n",
    "    def assign_closest_exemplar(self, iterator):\n",
    "        # Assign the closest exemplar based on DTW distance\n",
    "        partition_data = list(iterator)\n",
    "        for row in partition_data:\n",
    "            exemplar_distances = {key: value for key, value in row.items() if key.startswith(\"dtw_distance_exemplar_\")}\n",
    "            if exemplar_distances:\n",
    "                closest_exemplar = min(exemplar_distances, key=exemplar_distances.get)\n",
    "                row[\"closest exemplar\"] = closest_exemplar\n",
    "        return iter(partition_data)\n",
    "\n",
    "    def calculate_partition_gini(self, iterator):\n",
    "        # Calculate Gini impurity for a partition\n",
    "        labels = [row['label'] for row in iterator]\n",
    "        label_counts_dict = {}\n",
    "        for label in labels:\n",
    "            if label in label_counts_dict:\n",
    "                label_counts_dict[label] += 1\n",
    "            else:\n",
    "                label_counts_dict[label] = 1\n",
    "        total = sum(label_counts_dict.values())\n",
    "        proportion_sqrd_values = [(count / total) ** 2 for count in label_counts_dict.values()]\n",
    "        gini_impurity = 1 - sum(proportion_sqrd_values)\n",
    "        return iter([gini_impurity])\n",
    "\n",
    "    def split_rdd_by_closest_exemplar(self, rdd, exemplar_name):\n",
    "        # Split RDD based on the closest exemplar\n",
    "        yes_rdd = rdd.filter(lambda row: row['closest_exemplar'] == exemplar_name)\n",
    "        no_rdd = rdd.filter(lambda row: row['closest_exemplar'] != exemplar_name)\n",
    "        return yes_rdd, no_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "eca7eadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: {'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "Partition 0: {'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9]}\n",
      "Partition 0: {'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4]}\n",
      "Partition 1: {'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4]}\n",
      "Partition 1: {'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9]}\n",
      "Partition 1: {'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9]}\n",
      "Partition 1: {'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4]}\n",
      "Partition 1: {'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9]}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplar': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplar': [1.5, 2.6, 3.7, 4.8, 5.9]}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplar': [1.5, 2.6, 3.7, 4.8, 5.9]}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplar': [1.5, 2.6, 3.7, 4.8, 5.9]}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplar': [1.5, 2.6, 3.7, 4.8, 5.9]}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplar': [1.5, 2.6, 3.7, 4.8, 5.9]}\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 94.0 failed 1 times, most recent failure: Lost task 0.0 in stage 94.0 (TID 155) (BenAtkinson-Dell-Inspiron3505.Home executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"C:\\Users\\benat\\AppData\\Local\\Temp\\ipykernel_20836\\1884868875.py\", line 20, in <lambda>\nTypeError: RDDProcessor.calc_dtw_distance() takes 2 positional arguments but 3 were given\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"C:\\Users\\benat\\AppData\\Local\\Temp\\ipykernel_20836\\1884868875.py\", line 20, in <lambda>\nTypeError: RDDProcessor.calc_dtw_distance() takes 2 positional arguments but 3 were given\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m rdd_with_dtw \u001b[38;5;241m=\u001b[39m rdd_with_sampled_column\u001b[38;5;241m.\u001b[39mmapPartitions(\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m iterator: processor\u001b[38;5;241m.\u001b[39mcalc_dtw_distance(iterator, dtw_distance_broadcast\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Collect and print the results\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rdd_with_dtw\u001b[38;5;241m.\u001b[39mcollect():\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Example: Assign closest exemplar\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 94.0 failed 1 times, most recent failure: Lost task 0.0 in stage 94.0 (TID 155) (BenAtkinson-Dell-Inspiron3505.Home executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"C:\\Users\\benat\\AppData\\Local\\Temp\\ipykernel_20836\\1884868875.py\", line 20, in <lambda>\nTypeError: RDDProcessor.calc_dtw_distance() takes 2 positional arguments but 3 were given\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\benat\\miniconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"C:\\Users\\benat\\AppData\\Local\\Temp\\ipykernel_20836\\1884868875.py\", line 20, in <lambda>\nTypeError: RDDProcessor.calc_dtw_distance() takes 2 positional arguments but 3 were given\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Initialize the processor\n",
    "processor = RDDProcessor()\n",
    "\n",
    "# Example: Repartition and print partition rows\n",
    "partitioned_rdd = rdd.mapPartitionsWithIndex(processor.print_partition_rows)\n",
    "for partition_index, row in partitioned_rdd.collect():\n",
    "    print(f\"Partition {partition_index}: {row}\")\n",
    "\n",
    "# Example: Add exemplar column\n",
    "rdd_with_sampled_column = rdd.mapPartitions(processor.sample_and_add_column)\n",
    "for row in rdd_with_sampled_column.collect():\n",
    "    print(row)\n",
    "\n",
    "# Broadcast the dtw.distance function\n",
    "dtw_distance_broadcast = sc.broadcast(dtw.distance)\n",
    "# Initialize the processor\n",
    "processor = RDDProcessor()\n",
    "# Example: Calculate DTW distances\n",
    "rdd_with_dtw = rdd_with_sampled_column.mapPartitions(\n",
    "    lambda iterator: processor.calc_dtw_distance(iterator, dtw_distance_broadcast.value)\n",
    ")\n",
    "\n",
    "# Collect and print the results\n",
    "for row in rdd_with_dtw.collect():\n",
    "    print(row)\n",
    "# Example: Assign closest exemplar\n",
    "rdd_with_classification = rdd_with_dtw.mapPartitions(processor.assign_closest_exemplar)\n",
    "for row in rdd_with_classification.collect():\n",
    "    print(row)\n",
    "\n",
    "# Example: Calculate Gini impurity\n",
    "gini_rdd = rdd_with_classification.mapPartitions(processor.calculate_partition_gini)\n",
    "for gini in gini_rdd.collect():\n",
    "    print(f\"Gini impurity: {gini}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71441488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
