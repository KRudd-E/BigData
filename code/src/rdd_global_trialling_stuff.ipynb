{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9063d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from data_ingestion import DataIngestion\n",
    "from preprocessing import Preprocessor\n",
    "from prediction_manager import PredictionManager\n",
    "from local_model_manager import LocalModelManager\n",
    "from evaluation import Evaluator\n",
    "from utilities import show_compact\n",
    "import time\n",
    "import json\n",
    "from random import sample\n",
    "from dtaidistance import dtw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01e9ea",
   "metadata": {},
   "source": [
    "## ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84413ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"GenericRDD\").getOrCreate()\n",
    "\n",
    "# Access the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a0e00",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0715b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"label\": 1, \"time_series\": [1.0, 2.1, 3.2, 4.3, 5.4]},\n",
    "    {\"label\": 2, \"time_series\": [2.0, 3.1, 4.2, 5.3, 6.4]},\n",
    "    {\"label\": 3, \"time_series\": [3.0, 4.1, 5.2, 6.3, 7.4]},\n",
    "    {\"label\": 4, \"time_series\": [4.0, 5.1, 6.2, 7.3, 8.4]},\n",
    "    {\"label\": 1, \"time_series\": [1.5, 2.6, 3.7, 4.8, 5.9]},\n",
    "    {\"label\": 2, \"time_series\": [2.5, 3.6, 4.7, 5.8, 6.9]},\n",
    "    {\"label\": 3, \"time_series\": [3.5, 4.6, 5.7, 6.8, 7.9]},\n",
    "    {\"label\": 4, \"time_series\": [4.5, 5.6, 6.7, 7.8, 8.9]}\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cae169e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.repartition(2)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "414dc4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: {'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4]}\n",
      "Partition 0: {'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "Partition 0: {'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9]}\n",
      "Partition 0: {'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9]}\n",
      "Partition 0: {'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9]}\n",
      "Partition 1: {'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4]}\n",
      "Partition 1: {'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4]}\n",
      "Partition 1: {'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9]}\n"
     ]
    }
   ],
   "source": [
    "def print_partition_rows(index, iterator):\n",
    "    # Add partition index to each row\n",
    "    return [(index, row) for row in iterator]\n",
    "\n",
    "# Use mapPartitionsWithIndex to include partition index\n",
    "partitioned_rdd = rdd.mapPartitionsWithIndex(print_partition_rows)\n",
    "\n",
    "# Collect and print the rows along with their partition index\n",
    "for partition_index, row in partitioned_rdd.collect():\n",
    "    print(f\"Partition {partition_index}: {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bde2d9",
   "metadata": {},
   "source": [
    "# adding exemplar column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7f8c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplar': [3.5, 4.6, 5.7, 6.8, 7.9]}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplar': [3.5, 4.6, 5.7, 6.8, 7.9]}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplar': [3.5, 4.6, 5.7, 6.8, 7.9]}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplar': [3.5, 4.6, 5.7, 6.8, 7.9]}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplar': [3.5, 4.6, 5.7, 6.8, 7.9]}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplar': [4.5, 5.6, 6.7, 7.8, 8.9]}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplar': [4.5, 5.6, 6.7, 7.8, 8.9]}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplar': [4.5, 5.6, 6.7, 7.8, 8.9]}\n"
     ]
    }
   ],
   "source": [
    "def sample_and_add_column(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    sampled_element = sample(partition_data, 1)[0]['time_series']\n",
    "    return iter([{**row, \"exemplar\": sampled_element} for row in partition_data])\n",
    "\n",
    "rdd_with_sampled_column = rdd.mapPartitions(sample_and_add_column)\n",
    "\n",
    "# Collect and print the updated RDD\n",
    "for row in rdd_with_sampled_column.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d7f27e",
   "metadata": {},
   "source": [
    "# calculating DTW distance using time series and exemplar columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bd9ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplar': [3.5, 4.6, 5.7, 6.8, 7.9], 'dtw_distance': 1.118033988749895}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplar': [3.5, 4.6, 5.7, 6.8, 7.9], 'dtw_distance': 1.118033988749895}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplar': [3.5, 4.6, 5.7, 6.8, 7.9], 'dtw_distance': 3.1208973068654466}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplar': [3.5, 4.6, 5.7, 6.8, 7.9], 'dtw_distance': 1.42828568570857}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplar': [3.5, 4.6, 5.7, 6.8, 7.9], 'dtw_distance': 0.0}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplar': [4.5, 5.6, 6.7, 7.8, 8.9], 'dtw_distance': 6.283311228962003}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplar': [4.5, 5.6, 6.7, 7.8, 8.9], 'dtw_distance': 4.085339643163099}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplar': [4.5, 5.6, 6.7, 7.8, 8.9], 'dtw_distance': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# def calc_dtw_distance(iterator):\n",
    "#     partition_data = list(iterator)\n",
    "#     time_series = partition_data['time_series']\n",
    "#     exemplar = partition_data['exemplar']\n",
    "#     dtw_distance = dtw.distance(time_series, exemplar)\n",
    "#     return iter([{**row, \"dtw_distance\": dtw_distance} for row in partition_data])\n",
    "\n",
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row['time_series']\n",
    "        exemplar = row['exemplar']\n",
    "        \n",
    "        dtw_distance = dtw.distance(time_series, exemplar)\n",
    "        \n",
    "        updated_row = {**row, \"dtw_distance\": dtw_distance}\n",
    "        updated_rows.append(updated_row)\n",
    "    return iter(updated_rows)\n",
    "\n",
    "rdd_with_dtw = rdd_with_sampled_column.mapPartitions(calc_dtw_distance)\n",
    "for row in rdd_with_dtw.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964959a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59dd988",
   "metadata": {},
   "source": [
    "# WORKS FOR ANY NUM OF PARTITIONS AND EXEMPLARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05bf335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]]}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]]}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]]}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]]}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]]}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplars': [[4.5, 5.6, 6.7, 7.8, 8.9], [2.0, 3.1, 4.2, 5.3, 6.4]]}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplars': [[4.5, 5.6, 6.7, 7.8, 8.9], [2.0, 3.1, 4.2, 5.3, 6.4]]}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplars': [[4.5, 5.6, 6.7, 7.8, 8.9], [2.0, 3.1, 4.2, 5.3, 6.4]]}\n",
      "\n",
      "rdd num partitions: 2\n"
     ]
    }
   ],
   "source": [
    "def create_sample_and_add_column_function(num_exemplars):\n",
    "    def sample_and_add_column(iterator):\n",
    "        partition_data = list(iterator)\n",
    "        exemplars = []\n",
    "        for row in sample(partition_data, min(num_exemplars, len(partition_data))):\n",
    "            exemplars.append(row['time_series'])\n",
    "        return iter([{**row, \"exemplars\": exemplars} for row in partition_data])\n",
    "    return sample_and_add_column\n",
    "\n",
    "# Example usage\n",
    "num_exemplars = 2\n",
    "\n",
    "chosen_exemplars = create_sample_and_add_column_function(num_exemplars)\n",
    "rdd_with_exemplar_column = rdd.mapPartitions(chosen_exemplars)\n",
    "\n",
    "for row in rdd_with_exemplar_column.collect():\n",
    "    print(row)\n",
    "\n",
    "print(f'\\nrdd num partitions: {rdd_with_exemplar_column.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7bed113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]], 'dtw_distance_exemplar_1': 1.118033988749895, 'dtw_distance_exemplar_2': 1.1180339887498947}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]], 'dtw_distance_exemplar_1': 1.118033988749895, 'dtw_distance_exemplar_2': 2.2671568097509267}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]], 'dtw_distance_exemplar_1': 3.1208973068654466, 'dtw_distance_exemplar_2': 1.42828568570857}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]], 'dtw_distance_exemplar_1': 1.42828568570857, 'dtw_distance_exemplar_2': 0.0}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 1.42828568570857}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplars': [[4.5, 5.6, 6.7, 7.8, 8.9], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 6.283311228962003, 'dtw_distance_exemplar_2': 1.42828568570857}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplars': [[4.5, 5.6, 6.7, 7.8, 8.9], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 4.085339643163099, 'dtw_distance_exemplar_2': 0.0}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplars': [[4.5, 5.6, 6.7, 7.8, 8.9], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 4.085339643163099}\n",
      "\n",
      "rdd num partitions: 2\n"
     ]
    }
   ],
   "source": [
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row['time_series']\n",
    "        exemplars = row['exemplars']\n",
    "        \n",
    "        # Calculate DTW distances for each exemplar\n",
    "        dtw_distances = [dtw.distance(time_series, exemplar) for exemplar in exemplars]\n",
    "        \n",
    "        # Add each DTW distance as a separate column\n",
    "        updated_row = {**row}\n",
    "        for i, dtw_distance in enumerate(dtw_distances):\n",
    "            updated_row[f\"dtw_distance_exemplar_{i+1}\"] = dtw_distance\n",
    "        \n",
    "        updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_dtw = rdd_with_exemplar_column.mapPartitions(calc_dtw_distance)\n",
    "for row in rdd_with_dtw.collect():\n",
    "    print(row)\n",
    "\n",
    "print(f'\\nrdd num partitions: {rdd_with_dtw.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "667bd906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]], 'dtw_distance_exemplar_1': 1.118033988749895, 'dtw_distance_exemplar_2': 1.1180339887498947, 'closest exemplar': 'dtw_distance_exemplar_2'}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]], 'dtw_distance_exemplar_1': 1.118033988749895, 'dtw_distance_exemplar_2': 2.2671568097509267, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]], 'dtw_distance_exemplar_1': 3.1208973068654466, 'dtw_distance_exemplar_2': 1.42828568570857, 'closest exemplar': 'dtw_distance_exemplar_2'}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]], 'dtw_distance_exemplar_1': 1.42828568570857, 'dtw_distance_exemplar_2': 0.0, 'closest exemplar': 'dtw_distance_exemplar_2'}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplars': [[3.5, 4.6, 5.7, 6.8, 7.9], [2.5, 3.6, 4.7, 5.8, 6.9]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 1.42828568570857, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplars': [[4.5, 5.6, 6.7, 7.8, 8.9], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 6.283311228962003, 'dtw_distance_exemplar_2': 1.42828568570857, 'closest exemplar': 'dtw_distance_exemplar_2'}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplars': [[4.5, 5.6, 6.7, 7.8, 8.9], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 4.085339643163099, 'dtw_distance_exemplar_2': 0.0, 'closest exemplar': 'dtw_distance_exemplar_2'}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplars': [[4.5, 5.6, 6.7, 7.8, 8.9], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 4.085339643163099, 'closest exemplar': 'dtw_distance_exemplar_1'}\n"
     ]
    }
   ],
   "source": [
    "# not sure if this is needed\n",
    "\n",
    "def assign_closest_exemplar(iterator):\n",
    "    partition_data = list(iterator)\n",
    "\n",
    "    for row in partition_data:\n",
    "        # Check if there are DTW distances for exemplars\n",
    "        exemplar_distances = {key: value for key, value in row.items() if key.startswith(\"dtw_distance_exemplar_\")}\n",
    "        \n",
    "        if exemplar_distances:\n",
    "            # Find the exemplar with the smallest DTW distance\n",
    "            closest_exemplar = min(exemplar_distances, key=exemplar_distances.get)\n",
    "            \n",
    "            # Assign the closest exemplar to the row\n",
    "            row[\"closest exemplar\"] = closest_exemplar\n",
    "\n",
    "    return iter(partition_data)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_classification = rdd_with_dtw.mapPartitions(assign_closest_exemplar)\n",
    "for row in rdd_with_classification.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1539859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_partition_gini(iterator):\n",
    "    labels = [row['label'] for row in iterator]\n",
    "\n",
    "    label_counts_dict = {}\n",
    "    for label in labels:\n",
    "        if label in label_counts_dict:\n",
    "            label_counts_dict[label] += 1\n",
    "        else:\n",
    "            label_counts_dict[label] = 1\n",
    "    \n",
    "    total = sum(label_counts_dict.values())\n",
    "    proportion_sqrd_values = [(count / total) ** 2 for count in label_counts_dict.values()]\n",
    "    gini_impurity = 1 - sum(proportion_sqrd_values)\n",
    "    \n",
    "    return iter([gini_impurity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34b0d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini impurity of partition 1: 0.72\n",
      "gini impurity of partition 2: 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "gini_rdd = rdd_with_classification.mapPartitions(calculate_partition_gini)\n",
    "\n",
    "# Collect and print the Gini impurity for each partition\n",
    "i=0\n",
    "for gini in gini_rdd.collect():\n",
    "    print(f'gini impurity of partition {i+1}: {gini}')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321d01d",
   "metadata": {},
   "source": [
    "### trying splitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56633663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsdata = [\n",
    "#     {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0], 'closest_exemplar': 'exemplar_1'},\n",
    "#     {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9], 'closest_exemplar': 'exemplar_2'},\n",
    "#     {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5], 'closest_exemplar': 'exemplar_1'},\n",
    "#     {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2], 'closest_exemplar': 'exemplar_2'},\n",
    "#     {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2], 'closest_exemplar': 'exemplar_1'},\n",
    "#     {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9], 'closest_exemplar': 'exemplar_2'},\n",
    "#     {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0], 'closest_exemplar': 'exemplar_1'},\n",
    "#     {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5], 'closest_exemplar': 'exemplar_2'},\n",
    "#     {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5], 'closest_exemplar': 'exemplar_1'},\n",
    "#     {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5], 'closest_exemplar': 'exemplar_2'}\n",
    "# ]\n",
    "\n",
    "# ts_rdd = sc.parallelize(tsdata)\n",
    "# ts_rdd = ts_rdd.repartition(2)\n",
    "\n",
    "# print(f'ts_rdd num partitions: {ts_rdd.getNumPartitions()}')\n",
    "\n",
    "# ts_rdd_gini = ts_rdd.mapPartitions(calculate_partition_gini)\n",
    "# # Collect and print the Gini impurity for each partition\n",
    "# i=0\n",
    "# for gini in ts_rdd_gini.collect():\n",
    "#     print(f'gini impurity of partition {i+1}: {gini}')\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2062c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closestto1_yes = [row['label'] for row in tsdata if row['closest_exemplar'] == 'exemplar_1']\n",
    "# closestto1_no = [row['label'] for row in tsdata if row['closest_exemplar'] != 'exemplar_1']\n",
    "\n",
    "# print(closestto1_yes)\n",
    "# print(closestto1_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f56951b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(labels):\n",
    "    label_counts = {}\n",
    "    for label in labels:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    total = sum(label_counts.values())\n",
    "    gini = 1 - sum((count / total) ** 2 for count in label_counts.values())\n",
    "    return gini\n",
    "\n",
    "def evaluate_splits_within_partition(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    \n",
    "    # If the partition is empty, return an empty iterator\n",
    "    if not partition_data:\n",
    "        return iter([])\n",
    "    \n",
    "    # Get all unique exemplar names in the partition\n",
    "    unique_exemplars = set(row['closest exemplar'] for row in partition_data)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Loop through each exemplar to evaluate splits\n",
    "    for exemplar_name in unique_exemplars:\n",
    "        # Split the data based on the current exemplar\n",
    "        yes_split = [row for row in partition_data if row['closest exemplar'] == exemplar_name]\n",
    "        no_split = [row for row in partition_data if row['closest exemplar'] != exemplar_name]\n",
    "        \n",
    "        # Calculate metrics for the split (e.g., Gini impurity)\n",
    "        yes_labels = [row['label'] for row in yes_split]\n",
    "        no_labels = [row['label'] for row in no_split]\n",
    "        \n",
    "        yes_gini = calculate_gini(yes_labels)\n",
    "        no_gini = calculate_gini(no_labels)\n",
    "        \n",
    "        # Store the results for this split\n",
    "        results.append({\n",
    "            'exemplar': exemplar_name,\n",
    "            'yes_gini': yes_gini,\n",
    "            'no_gini': no_gini,\n",
    "            'yes_split_size': len(yes_split),\n",
    "            'no_split_size': len(no_split)\n",
    "        })\n",
    "    \n",
    "    # Return the results as an iterator\n",
    "    return iter(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "864a41e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exemplar': 'dtw_distance_exemplar_1', 'yes_gini': 0.5, 'no_gini': 0.6666666666666667, 'yes_split_size': 2, 'no_split_size': 3}\n",
      "{'exemplar': 'dtw_distance_exemplar_2', 'yes_gini': 0.6666666666666667, 'no_gini': 0.5, 'yes_split_size': 3, 'no_split_size': 2}\n",
      "{'exemplar': 'dtw_distance_exemplar_1', 'yes_gini': 0.0, 'no_gini': 0.5, 'yes_split_size': 1, 'no_split_size': 2}\n",
      "{'exemplar': 'dtw_distance_exemplar_2', 'yes_gini': 0.5, 'no_gini': 0.0, 'yes_split_size': 2, 'no_split_size': 1}\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "ts_rdd_split_results = rdd_with_classification.mapPartitions(evaluate_splits_within_partition)\n",
    "# Collect and print the results\n",
    "for result in ts_rdd_split_results.collect():\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59e67b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_within_partition(iterator):\n",
    "#     partition_data = list(iterator)\n",
    "#     exemplar_name = partition_data[0]['closest_exemplar'] # randomly chosen exemplar name\n",
    "    \n",
    "#     # Split the data within the partition\n",
    "#     yes_split = [row for row in partition_data if row['closest_exemplar'] == exemplar_name]\n",
    "#     no_split = [row for row in partition_data if row['closest_exemplar'] != exemplar_name]\n",
    "    \n",
    "#     # Add a flag to indicate which split the row belongs to\n",
    "#     yes_split = [{**row, 'split': 'yes'} for row in yes_split]\n",
    "#     no_split = [{**row, 'split': 'no'} for row in no_split]\n",
    "    \n",
    "#     # Combine the splits and return as an iterator\n",
    "#     return iter(yes_split + no_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e50d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example usage\n",
    "# split_rdd = ts_rdd.mapPartitions(split_within_partition)\n",
    "\n",
    "# # Collect and print the results\n",
    "# for row in split_rdd.collect():\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71441488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "633719b1",
   "metadata": {},
   "source": [
    "OG returned best exemplar -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa99d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from random import sample\n",
    "from dtaidistance import dtw\n",
    "\n",
    "class GlobalModelManager:\n",
    "    def __init__(self):\n",
    "        self.num_exemplars = 2\n",
    "        self.num_partitions = 2\n",
    "\n",
    "    def train(self, df: DataFrame):\n",
    "        rdd = self.partition_data(df)\n",
    "        rdd = rdd.repartition(self.num_partitions)\n",
    "\n",
    "        # choose_exemplars = self.choose_exemplars_function(self.num_exemplars)\n",
    "        rdd_with_exemplar_column = rdd.mapPartitions(choose_exemplars)\n",
    "\n",
    "        rdd_with_dtw = rdd_with_exemplar_column.mapPartitions(self.calc_dtw_distance)\n",
    "\n",
    "        rdd_with_closest_exemplar = rdd_with_dtw.mapPartitions(self.assign_closest_exemplar)\n",
    "\n",
    "        # Evaluate splits and collect minimal results\n",
    "        results = rdd_with_closest_exemplar.mapPartitionsWithIndex(self.evaluate_splits_within_partition).collect()\n",
    "        index = np.argmax([result['best_gini_reduction'] for result in results])\n",
    "        bestsplitter = results[index]['best_split_exemplar']\n",
    "        bestsplitters_gini_reduction = results[index]['best_gini_reduction']\n",
    "        print(f\"best time series to split on is {bestsplitter} with a gini reduction of {bestsplitters_gini_reduction}\")\n",
    "        return bestsplitter\n",
    "\n",
    "    def partition_data(self, df: DataFrame) -> DataFrame:\n",
    "        # Convert DataFrame to RDD and add partition_id\n",
    "        rdd = df.rdd.mapPartitionsWithIndex(\n",
    "            lambda idx, iter: [{**row.asDict(), \"partition_id\": idx} for row in iter]\n",
    "        )\n",
    "        return rdd\n",
    "\n",
    "    def choose_exemplars(self, iterator):\n",
    "        partition_data = list(iterator)\n",
    "        if not partition_data:\n",
    "            return iter([])\n",
    "        \n",
    "        # Group data by class\n",
    "        grouped_data_by_class = {}\n",
    "        for row in partition_data:\n",
    "            label = row.get('label')\n",
    "            if label is not None:\n",
    "                grouped_data_by_class.setdefault(label, []).append(row)\n",
    "        \n",
    "        # Select one exemplar per class\n",
    "        rows_containing_chosen_exemplars = []\n",
    "        for label, instances in grouped_data_by_class.items():\n",
    "            if instances:  # Ensure there are instances for the class\n",
    "                exemplar = sample(instances, 1)[0]\n",
    "                rows_containing_chosen_exemplars.append(exemplar)\n",
    "        \n",
    "        chosen_exemplars_list = [row['time_series'] for row in rows_containing_chosen_exemplars]\n",
    "        \n",
    "        # Remove chosen exemplars from the working data\n",
    "        filtered_partition = [\n",
    "            row for row in partition_data\n",
    "            if row['time_series'] not in chosen_exemplars_list\n",
    "        ]\n",
    "        \n",
    "        # Return rows with exemplars attached\n",
    "        return iter([{**row, \"exemplars\": chosen_exemplars_list} for row in filtered_partition])\n",
    "\n",
    "    def calc_dtw_distance(self, iterator):\n",
    "        partition_data = list(iterator)\n",
    "        updated_rows = []\n",
    "        \n",
    "        for row in partition_data:\n",
    "            time_series = row.get('time_series', [])\n",
    "            exemplars = row.get('exemplars', [])\n",
    "            if not exemplars:\n",
    "                continue  # Skip if no exemplars\n",
    "            \n",
    "            dtw_distances = [dtw.distance(time_series, exemplar) for exemplar in exemplars]\n",
    "            \n",
    "            updated_row = {**row}\n",
    "            updated_row['exemplar_map'] = {}  # Map exemplar IDs to time series\n",
    "\n",
    "            for i, (dtw_distance, exemplar_ts) in enumerate(zip(dtw_distances, exemplars)):\n",
    "                exemplar_id = f\"dtw_distance_exemplar_{i+1}\"\n",
    "                updated_row[exemplar_id] = dtw_distance\n",
    "                updated_row['exemplar_map'][exemplar_id] = exemplar_ts\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "        \n",
    "        return iter(updated_rows)\n",
    "    \n",
    "    def assign_closest_exemplar(self, iterator):\n",
    "        partition_data = list(iterator)\n",
    "        updated_rows = []\n",
    "\n",
    "        for row in partition_data:\n",
    "            # Check if there are DTW distances for exemplars\n",
    "            exemplar_distances = {\n",
    "                key: value for key, value in row.items()\n",
    "                if key.startswith(\"dtw_distance_exemplar_\") and isinstance(value, (int, float))\n",
    "            }\n",
    "            \n",
    "            updated_row = {**row}\n",
    "            if exemplar_distances:\n",
    "                # Find the exemplar with the smallest DTW distance\n",
    "                closest_exemplar_id = min(exemplar_distances, key=exemplar_distances.get)\n",
    "                \n",
    "                # Assign the closest exemplar ID and time series\n",
    "                updated_row[\"closest_exemplar_id\"] = closest_exemplar_id\n",
    "                updated_row[\"closest_exemplar_ts\"] = row.get('exemplar_map', {}).get(closest_exemplar_id, [])\n",
    "            else:\n",
    "                # Handle case with no valid distances\n",
    "                updated_row[\"closest_exemplar_id\"] = None\n",
    "                updated_row[\"closest_exemplar_ts\"] = []\n",
    "\n",
    "            updated_rows.append(updated_row)\n",
    "\n",
    "        return iter(updated_rows)\n",
    "    \n",
    "    def calculate_gini(self, labels):\n",
    "        if not labels:\n",
    "            return 0\n",
    "        label_counts = {}\n",
    "        for label in labels:\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1\n",
    "        total = sum(label_counts.values())\n",
    "        gini = 1 - sum((count / total) ** 2 for count in label_counts.values()) if total > 0 else 0\n",
    "        return gini\n",
    "\n",
    "    def evaluate_splits_within_partition(self, index, iterator):\n",
    "        partition_data = list(iterator)\n",
    "        \n",
    "        # Handle empty partition\n",
    "        if not partition_data:\n",
    "            return iter([{\n",
    "                \"partition_id\": index,\n",
    "                \"best_split_exemplar\": None,\n",
    "                \"best_gini_reduction\": None\n",
    "            }])\n",
    "        \n",
    "        # Calculate Gini impurity before splitting\n",
    "        labels = [row.get('label') for row in partition_data if row.get('label') is not None]\n",
    "        before_split_gini = self.calculate_gini(labels)\n",
    "        \n",
    "        # Get all unique exemplar IDs in the partition\n",
    "        unique_exemplars = set(\n",
    "            row['closest_exemplar_id'] for row in partition_data\n",
    "            if row.get('closest_exemplar_id') is not None\n",
    "        )\n",
    "        \n",
    "        # Handle case with no valid exemplars\n",
    "        if not unique_exemplars:\n",
    "            return iter([{\n",
    "                \"partition_id\": index,\n",
    "                \"best_split_exemplar\": None,\n",
    "                \"best_gini_reduction\": None\n",
    "            }])\n",
    "        \n",
    "        # Evaluate all possible splits\n",
    "        best_split_exemplar = None\n",
    "        best_gini_reduction = float('-inf')\n",
    "        best_weighted_gini = None\n",
    "        \n",
    "        for exemplar_id in unique_exemplars:\n",
    "            # Split the data based on the current exemplar\n",
    "            yes_split = [r for r in partition_data if r.get('closest_exemplar_id') == exemplar_id]\n",
    "            no_split = [r for r in partition_data if r.get('closest_exemplar_id') != exemplar_id]\n",
    "            \n",
    "            # Calculate Gini for each daughter node\n",
    "            yes_labels = [r.get('label') for r in yes_split if r.get('label') is not None]\n",
    "            no_labels = [r.get('label') for r in no_split if r.get('label') is not None]\n",
    "            \n",
    "            yes_gini = self.calculate_gini(yes_labels)\n",
    "            no_gini = self.calculate_gini(no_labels)\n",
    "            \n",
    "            # Calculate weighted Gini after split\n",
    "            total_size = len(yes_split) + len(no_split)\n",
    "            weighted_gini = (yes_gini * len(yes_split) / total_size + no_gini * len(no_split) / total_size) if total_size > 0 else float('inf')\n",
    "            \n",
    "            # Calculate Gini reduction\n",
    "            gini_reduction = before_split_gini - weighted_gini if total_size > 0 else float('-inf')\n",
    "            \n",
    "            # Get the exemplar time series for this exemplar_id\n",
    "            exemplar = yes_split[0].get('exemplar_map', {}).get(exemplar_id, []) if yes_split else []\n",
    "            \n",
    "            # Update best split if this one has a larger Gini reduction\n",
    "            if gini_reduction > best_gini_reduction:\n",
    "                best_gini_reduction = gini_reduction\n",
    "                best_split_exemplar = exemplar\n",
    "                best_weighted_gini = weighted_gini\n",
    "        \n",
    "        # Handle case where no valid split was found\n",
    "        if best_gini_reduction == float('-inf'):\n",
    "            return iter([{\n",
    "                \"partition_id\": index,\n",
    "                \"best_split_exemplar\": None,\n",
    "                \"best_gini_reduction\": None\n",
    "            }])\n",
    "        \n",
    "        return iter([{\n",
    "            \"partition_id\": index,\n",
    "            \"best_split_exemplar\": best_split_exemplar,\n",
    "            \"best_gini_reduction\": best_gini_reduction\n",
    "        }])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59352146",
   "metadata": {},
   "source": [
    "<-- OG returned best exemplar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7c4df",
   "metadata": {},
   "source": [
    "claude code -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6122fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from random import sample, choice\n",
    "from dtaidistance import dtw\n",
    "\n",
    "class GlobalModelManager:\n",
    "    def __init__(self, max_depth=3, min_samples_split=2, n_splitters=5):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_splitters = n_splitters\n",
    "        self.num_partitions = 2\n",
    "        self.distance_measures = ['dtw', 'euclidean', 'manhattan']  # Available distance measures\n",
    "\n",
    "    def train(self, df: DataFrame):\n",
    "        # Convert DataFrame to a format we can work with\n",
    "        data = df.rdd.map(lambda row: row.asDict()).collect()\n",
    "        \n",
    "        # Build the tree recursively\n",
    "        tree = self.build_tree(data)\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def build_tree(self, data, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build a Proximity Tree\n",
    "        \"\"\"\n",
    "        # Base case: stop if max depth reached or not enough samples\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or len(data) < self.min_samples_split:\n",
    "            return self.create_leaf_node(data)\n",
    "        \n",
    "        # Check if node is pure (all samples belong to the same class)\n",
    "        labels = [row.get('label') for row in data]\n",
    "        unique_labels = set(labels)\n",
    "        if len(unique_labels) == 1:\n",
    "            return self.create_leaf_node(data)\n",
    "        \n",
    "        # Find the best split for this node\n",
    "        best_split = self.find_best_split(data)\n",
    "        \n",
    "        # If no good split found, create a leaf node\n",
    "        if best_split is None or best_split['gini_reduction'] <= 0:\n",
    "            return self.create_leaf_node(data)\n",
    "        \n",
    "        # Create an internal node\n",
    "        node = {\n",
    "            'type': 'internal',\n",
    "            'exemplars': best_split['exemplars'],\n",
    "            'distance_measure': best_split['distance_measure'],\n",
    "            'children': {}\n",
    "        }\n",
    "        \n",
    "        # Split the data\n",
    "        split_data = self.split_data(data, best_split)\n",
    "        \n",
    "        # Recursively build subtrees for each split\n",
    "        for class_label, subset in split_data.items():\n",
    "            if subset:  # Only create child if there are samples\n",
    "                node['children'][class_label] = self.build_tree(subset, depth + 1)\n",
    "            else:\n",
    "                # Create a leaf node for empty splits\n",
    "                node['children'][class_label] = self.create_leaf_node(data)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def create_leaf_node(self, data):\n",
    "        \"\"\"\n",
    "        Create a leaf node that predicts the majority class\n",
    "        \"\"\"\n",
    "        labels = [row.get('label') for row in data if row.get('label') is not None]\n",
    "        if not labels:\n",
    "            return {'type': 'leaf', 'prediction': None}\n",
    "        \n",
    "        # Count occurrences of each class\n",
    "        class_counts = {}\n",
    "        for label in labels:\n",
    "            class_counts[label] = class_counts.get(label, 0) + 1\n",
    "        \n",
    "        # Find majority class\n",
    "        majority_class = max(class_counts, key=class_counts.get)\n",
    "        \n",
    "        return {\n",
    "            'type': 'leaf',\n",
    "            'prediction': majority_class,\n",
    "            'class_distribution': class_counts\n",
    "        }\n",
    "    \n",
    "    def find_best_split(self, data):\n",
    "        \"\"\"\n",
    "        Find the best split by evaluating multiple candidate splitters\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            return None\n",
    "        \n",
    "        # Calculate Gini impurity before splitting\n",
    "        labels = [row.get('label') for row in data if row.get('label') is not None]\n",
    "        before_split_gini = self.calculate_gini(labels)\n",
    "        \n",
    "        best_split = None\n",
    "        best_gini_reduction = -float('inf')\n",
    "        \n",
    "        # Try multiple candidate splitters\n",
    "        for _ in range(self.n_splitters):\n",
    "            # Randomly choose a distance measure\n",
    "            distance_measure = choice(self.distance_measures)\n",
    "            \n",
    "            # Select exemplars (one per class)\n",
    "            exemplars = self.select_exemplars_per_class(data)\n",
    "            if not exemplars:\n",
    "                continue\n",
    "                \n",
    "            # Calculate distances and assign each instance to closest exemplar\n",
    "            assignments = self.assign_to_exemplars(data, exemplars, distance_measure)\n",
    "            \n",
    "            # Calculate Gini impurity after split\n",
    "            weighted_gini = 0\n",
    "            total_samples = len(data)\n",
    "            valid_split = False\n",
    "            \n",
    "            for class_label, instances in assignments.items():\n",
    "                if not instances:\n",
    "                    continue\n",
    "                    \n",
    "                class_labels = [row.get('label') for row in instances]\n",
    "                class_gini = self.calculate_gini(class_labels)\n",
    "                weighted_gini += (len(instances) / total_samples) * class_gini\n",
    "                valid_split = True\n",
    "            \n",
    "            # Skip invalid splits\n",
    "            if not valid_split:\n",
    "                continue\n",
    "                \n",
    "            # Calculate Gini reduction\n",
    "            gini_reduction = before_split_gini - weighted_gini\n",
    "            \n",
    "            # Update best split if this one is better\n",
    "            if gini_reduction > best_gini_reduction:\n",
    "                best_gini_reduction = gini_reduction\n",
    "                best_split = {\n",
    "                    'exemplars': exemplars,\n",
    "                    'distance_measure': distance_measure,\n",
    "                    'gini_reduction': gini_reduction\n",
    "                }\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def select_exemplars_per_class(self, data):\n",
    "        \"\"\"\n",
    "        Select one exemplar per class from the data\n",
    "        \"\"\"\n",
    "        # Group data by class\n",
    "        class_groups = {}\n",
    "        for row in data:\n",
    "            label = row.get('label')\n",
    "            if label is not None:\n",
    "                if label not in class_groups:\n",
    "                    class_groups[label] = []\n",
    "                class_groups[label].append(row)\n",
    "        \n",
    "        # Select one exemplar per class\n",
    "        exemplars = {}\n",
    "        for label, instances in class_groups.items():\n",
    "            if instances:\n",
    "                exemplar = choice(instances)\n",
    "                exemplars[label] = exemplar.get('time_series', [])\n",
    "        \n",
    "        return exemplars\n",
    "    \n",
    "    def assign_to_exemplars(self, data, exemplars, distance_measure):\n",
    "        \"\"\"\n",
    "        Assign each instance to its closest exemplar\n",
    "        \"\"\"\n",
    "        assignments = {label: [] for label in exemplars.keys()}\n",
    "        \n",
    "        for row in data:\n",
    "            time_series = row.get('time_series', [])\n",
    "            closest_label = None\n",
    "            min_distance = float('inf')\n",
    "            \n",
    "            # Find closest exemplar\n",
    "            for label, exemplar in exemplars.items():\n",
    "                distance = self.calculate_distance(time_series, exemplar, distance_measure)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    closest_label = label\n",
    "            \n",
    "            # Assign to closest exemplar\n",
    "            if closest_label is not None:\n",
    "                assignments[closest_label].append(row)\n",
    "        \n",
    "        return assignments\n",
    "    \n",
    "    def calculate_distance(self, ts1, ts2, measure):\n",
    "        \"\"\"\n",
    "        Calculate distance between two time series based on the specified measure\n",
    "        \"\"\"\n",
    "        if measure == 'dtw':\n",
    "            return dtw.distance(ts1, ts2)\n",
    "        elif measure == 'euclidean':\n",
    "            return np.linalg.norm(np.array(ts1) - np.array(ts2))\n",
    "        elif measure == 'manhattan':\n",
    "            return np.sum(np.abs(np.array(ts1) - np.array(ts2)))\n",
    "        else:\n",
    "            return dtw.distance(ts1, ts2)  # Default to DTW\n",
    "    \n",
    "    def split_data(self, data, split):\n",
    "        \"\"\"\n",
    "        Split data based on the closest exemplar\n",
    "        \"\"\"\n",
    "        return self.assign_to_exemplars(data, split['exemplars'], split['distance_measure'])\n",
    "    \n",
    "    def calculate_gini(self, labels):\n",
    "        \"\"\"\n",
    "        Calculate Gini impurity for a set of labels\n",
    "        \"\"\"\n",
    "        if not labels:\n",
    "            return 0\n",
    "            \n",
    "        label_counts = {}\n",
    "        for label in labels:\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1\n",
    "            \n",
    "        total = sum(label_counts.values())\n",
    "        gini = 1 - sum((count / total) ** 2 for count in label_counts.values()) if total > 0 else 0\n",
    "        \n",
    "        return gini\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'tree'):\n",
    "            raise ValueError(\"Model has not been trained yet\")\n",
    "            \n",
    "        predictions = []\n",
    "        for instance in X:\n",
    "            predictions.append(self.predict_single(instance, self.tree))\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def predict_single(self, instance, node):\n",
    "        \"\"\"\n",
    "        Predict class label for a single instance\n",
    "        \"\"\"\n",
    "        # If leaf node, return prediction\n",
    "        if node['type'] == 'leaf':\n",
    "            return node['prediction']\n",
    "        \n",
    "        # Find closest exemplar\n",
    "        time_series = instance.get('time_series', [])\n",
    "        closest_label = None\n",
    "        min_distance = float('inf')\n",
    "        \n",
    "        for label, exemplar in node['exemplars'].items():\n",
    "            distance = self.calculate_distance(time_series, exemplar, node['distance_measure'])\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_label = label\n",
    "        \n",
    "        # If no exemplar is close or branch doesn't exist, take first available child\n",
    "        if closest_label is None or closest_label not in node['children']:\n",
    "            closest_label = next(iter(node['children'].keys()))\n",
    "            \n",
    "        # Continue down the tree\n",
    "        return self.predict_single(instance, node['children'][closest_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959ef4f",
   "metadata": {},
   "source": [
    "<-- claude code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "710807b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]},\n",
    "    {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
    "    {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
    "    {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
    "    {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
    "    {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
    "    {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
    "    {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
    "    {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
    "    {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(tsdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aa8a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------+\n",
      "|label|time_series              |\n",
      "+-----+-------------------------+\n",
      "|1    |[1.2, 2.4, 3.6, 4.8, 6.0]|\n",
      "|1    |[1.0, 1.8, 2.6, 3.4, 4.2]|\n",
      "|1    |[0.9, 1.8, 2.7, 3.6, 4.5]|\n",
      "|1    |[1.5, 2.1, 2.7, 3.3, 3.9]|\n",
      "|1    |[0.8, 1.7, 2.5, 3.2, 4.0]|\n",
      "+-----+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea7a4e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best time series to split on is [2.4, 3.5, 4.6, 5.7, 6.8] with a gini reduction of 0.3375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.4, 3.5, 4.6, 5.7, 6.8]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_model = GlobalModelManager()\n",
    "global_model.train(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0e6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_exemplars(iterator):\n",
    "        partition_data = list(iterator)\n",
    "        if not partition_data:\n",
    "            return iter([])\n",
    "        \n",
    "        # Group data by class\n",
    "        grouped_data_by_class = {}\n",
    "        for row in partition_data:\n",
    "            label = row.get('label')\n",
    "            if label is not None:\n",
    "                grouped_data_by_class.setdefault(label, []).append(row)\n",
    "        \n",
    "        # Select one exemplar per class\n",
    "        rows_containing_chosen_exemplars = []\n",
    "        for label, instances in grouped_data_by_class.items():\n",
    "            if instances:  # Ensure there are instances for the class\n",
    "                exemplar = sample(instances, 1)[0]\n",
    "                rows_containing_chosen_exemplars.append(exemplar)\n",
    "        \n",
    "        chosen_exemplars_list = [row['time_series'] for row in rows_containing_chosen_exemplars]\n",
    "        \n",
    "        # Remove chosen exemplars from the working data\n",
    "        filtered_partition = [\n",
    "            row for row in partition_data\n",
    "            if row['time_series'] not in chosen_exemplars_list\n",
    "        ]\n",
    "        \n",
    "        # Return rows with exemplars attached\n",
    "        return iter([{**row, \"exemplars\": chosen_exemplars_list} for row in filtered_partition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1e19ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]},\n",
    "    {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
    "    {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
    "    {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
    "    {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
    "    {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
    "    {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
    "    {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
    "    {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
    "    {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]}\n",
    "]\n",
    "\n",
    "ts_rdd = sc.parallelize(tsdata)\n",
    "ts_rdd = ts_rdd.repartition(2)\n",
    "ts_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2de15b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 2, 3, 3, 4, 4, 1, 1, 1, 2, 1, 1, 1, 1, 2, 3, 4, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def returndata(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    return iter(partition_data)\n",
    "\n",
    "# example usage\n",
    "ts_rdd = ts_rdd.mapPartitions(returndata)\n",
    "# Collect and print the results\n",
    "[row.get('label') for row in ts_rdd.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc77f54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
       " {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
       " {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
       " {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
       " {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
       " {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
       " {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
       " {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
       " {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
       " {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
       " {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
       " {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
       " {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
       " {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
       " {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]},\n",
       " {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
       " {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
       " {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
       " {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
       " {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b986d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]}, {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]}, {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]}, {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]}]\n",
      "[[0.9, 1.8, 2.7, 3.6, 4.5], [2.1, 3.3, 4.5, 5.7, 6.9], [0.5, 1.5, 2.5, 3.5, 4.5], [6.0, 7.0, 8.0, 9.0, 10.0]]\n"
     ]
    }
   ],
   "source": [
    "grouped_data_by_class = {}\n",
    "for row in ts_rdd.collect():\n",
    "    label = row.get('label')\n",
    "    grouped_data_by_class.setdefault(label, []).append(row)\n",
    "\n",
    "# grouped_data_by_class\n",
    "rows_containing_chosen_exemplars = []\n",
    "for label, instances in grouped_data_by_class.items():\n",
    "    if instances:  # Ensure there are instances for the class\n",
    "        exemplar = sample(instances, 1)[0]\n",
    "        rows_containing_chosen_exemplars.append(exemplar)\n",
    "\n",
    "print(rows_containing_chosen_exemplars)\n",
    "\n",
    "chosen_exemplars = [row['time_series'] for row in rows_containing_chosen_exemplars]\n",
    "print(chosen_exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae174170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'slayble': 'hello'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caaac5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0], 'exemplars': [[1.1, 2.1, 3.1, 4.1, 5.1], [2.4, 3.5, 4.6, 5.7, 6.8], [2.0, 2.5, 3.0, 3.5, 4.0], [5.5, 6.6, 7.7, 8.8, 9.9]]}\n",
      "{'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9], 'exemplars': [[1.1, 2.1, 3.1, 4.1, 5.1], [2.4, 3.5, 4.6, 5.7, 6.8], [2.0, 2.5, 3.0, 3.5, 4.0], [5.5, 6.6, 7.7, 8.8, 9.9]]}\n",
      "{'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2], 'exemplars': [[1.1, 2.1, 3.1, 4.1, 5.1], [2.4, 3.5, 4.6, 5.7, 6.8], [2.0, 2.5, 3.0, 3.5, 4.0], [5.5, 6.6, 7.7, 8.8, 9.9]]}\n",
      "{'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5], 'exemplars': [[1.1, 2.1, 3.1, 4.1, 5.1], [2.4, 3.5, 4.6, 5.7, 6.8], [2.0, 2.5, 3.0, 3.5, 4.0], [5.5, 6.6, 7.7, 8.8, 9.9]]}\n",
      "{'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5], 'exemplars': [[1.1, 2.1, 3.1, 4.1, 5.1], [2.4, 3.5, 4.6, 5.7, 6.8], [2.0, 2.5, 3.0, 3.5, 4.0], [5.5, 6.6, 7.7, 8.8, 9.9]]}\n",
      "{'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5], 'exemplars': [[1.1, 2.1, 3.1, 4.1, 5.1], [2.4, 3.5, 4.6, 5.7, 6.8], [2.0, 2.5, 3.0, 3.5, 4.0], [5.5, 6.6, 7.7, 8.8, 9.9]]}\n",
      "{'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1], 'exemplars': [[1.1, 2.1, 3.1, 4.1, 5.1], [2.4, 3.5, 4.6, 5.7, 6.8], [2.0, 2.5, 3.0, 3.5, 4.0], [5.5, 6.6, 7.7, 8.8, 9.9]]}\n",
      "{'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0], 'exemplars': [[1.1, 2.1, 3.1, 4.1, 5.1], [2.4, 3.5, 4.6, 5.7, 6.8], [2.0, 2.5, 3.0, 3.5, 4.0], [5.5, 6.6, 7.7, 8.8, 9.9]]}\n",
      "{'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0], 'exemplars': [[0.9, 1.8, 2.7, 3.6, 4.5], [1.9, 2.8, 3.7, 4.6, 5.5], [1.0, 1.8, 2.6, 3.4, 4.2], [6.0, 7.0, 8.0, 9.0, 10.0]]}\n",
      "{'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9], 'exemplars': [[0.9, 1.8, 2.7, 3.6, 4.5], [1.9, 2.8, 3.7, 4.6, 5.5], [1.0, 1.8, 2.6, 3.4, 4.2], [6.0, 7.0, 8.0, 9.0, 10.0]]}\n",
      "{'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3], 'exemplars': [[0.9, 1.8, 2.7, 3.6, 4.5], [1.9, 2.8, 3.7, 4.6, 5.5], [1.0, 1.8, 2.6, 3.4, 4.2], [6.0, 7.0, 8.0, 9.0, 10.0]]}\n"
     ]
    }
   ],
   "source": [
    "exemplars = ts_rdd.mapPartitions(choose_exemplars).collect()\n",
    "\n",
    "for exemplar in exemplars:\n",
    "    print(exemplar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e3e443f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
       " {'label': 4, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [{'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},{'label': 4, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]}]\n",
    "\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee5684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
