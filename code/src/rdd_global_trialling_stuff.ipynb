{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9063d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from data_ingestion import DataIngestion\n",
    "from preprocessing import Preprocessor\n",
    "from prediction_manager import PredictionManager\n",
    "from local_model_manager import LocalModelManager\n",
    "from evaluation import Evaluator\n",
    "from utilities import show_compact\n",
    "import time\n",
    "import json\n",
    "from random import sample\n",
    "from dtaidistance import dtw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01e9ea",
   "metadata": {},
   "source": [
    "## ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84413ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"GenericRDD\").getOrCreate()\n",
    "\n",
    "# Access the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a0e00",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0715b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"label\": 1, \"time_series\": [1.0, 2.1, 3.2, 4.3, 5.4]},\n",
    "    {\"label\": 2, \"time_series\": [2.0, 3.1, 4.2, 5.3, 6.4]},\n",
    "    {\"label\": 3, \"time_series\": [3.0, 4.1, 5.2, 6.3, 7.4]},\n",
    "    {\"label\": 4, \"time_series\": [4.0, 5.1, 6.2, 7.3, 8.4]},\n",
    "    {\"label\": 1, \"time_series\": [1.5, 2.6, 3.7, 4.8, 5.9]},\n",
    "    {\"label\": 2, \"time_series\": [2.5, 3.6, 4.7, 5.8, 6.9]},\n",
    "    {\"label\": 3, \"time_series\": [3.5, 4.6, 5.7, 6.8, 7.9]},\n",
    "    {\"label\": 4, \"time_series\": [4.5, 5.6, 6.7, 7.8, 8.9]}\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae169e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.repartition(2)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414dc4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: {'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4]}\n",
      "Partition 0: {'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4]}\n",
      "Partition 0: {'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9]}\n",
      "Partition 0: {'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9]}\n",
      "Partition 0: {'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9]}\n",
      "Partition 1: {'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4]}\n",
      "Partition 1: {'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4]}\n",
      "Partition 1: {'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9]}\n"
     ]
    }
   ],
   "source": [
    "def print_partition_rows(index, iterator):\n",
    "    # Add partition index to each row\n",
    "    return [(index, row) for row in iterator]\n",
    "\n",
    "# Use mapPartitionsWithIndex to include partition index\n",
    "partitioned_rdd = rdd.mapPartitionsWithIndex(print_partition_rows)\n",
    "\n",
    "# Collect and print the rows along with their partition index\n",
    "for partition_index, row in partitioned_rdd.collect():\n",
    "    print(f\"Partition {partition_index}: {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bde2d9",
   "metadata": {},
   "source": [
    "# adding exemplar column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc7f8c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplar': [3.0, 4.1, 5.2, 6.3, 7.4]}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplar': [3.0, 4.1, 5.2, 6.3, 7.4]}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplar': [3.0, 4.1, 5.2, 6.3, 7.4]}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplar': [3.0, 4.1, 5.2, 6.3, 7.4]}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplar': [3.0, 4.1, 5.2, 6.3, 7.4]}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4]}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4]}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4]}\n"
     ]
    }
   ],
   "source": [
    "def sample_and_add_column(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    sampled_element = sample(partition_data, 1)[0]['time_series']\n",
    "    return iter([{**row, \"exemplar\": sampled_element} for row in partition_data])\n",
    "\n",
    "rdd_with_sampled_column = rdd.mapPartitions(sample_and_add_column)\n",
    "\n",
    "# Collect and print the updated RDD\n",
    "for row in rdd_with_sampled_column.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d7f27e",
   "metadata": {},
   "source": [
    "# calculating DTW distance using time series and exemplar columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd9ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplar': [3.0, 4.1, 5.2, 6.3, 7.4], 'dtw_distance': 0.0}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplar': [3.0, 4.1, 5.2, 6.3, 7.4], 'dtw_distance': 1.42828568570857}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplar': [3.0, 4.1, 5.2, 6.3, 7.4], 'dtw_distance': 2.2671568097509267}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplar': [3.0, 4.1, 5.2, 6.3, 7.4], 'dtw_distance': 1.1180339887498947}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplar': [3.0, 4.1, 5.2, 6.3, 7.4], 'dtw_distance': 1.118033988749895}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4], 'dtw_distance': 0.0}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4], 'dtw_distance': 1.42828568570857}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplar': [1.0, 2.1, 3.2, 4.3, 5.4], 'dtw_distance': 6.283311228962003}\n"
     ]
    }
   ],
   "source": [
    "# def calc_dtw_distance(iterator):\n",
    "#     partition_data = list(iterator)\n",
    "#     time_series = partition_data['time_series']\n",
    "#     exemplar = partition_data['exemplar']\n",
    "#     dtw_distance = dtw.distance(time_series, exemplar)\n",
    "#     return iter([{**row, \"dtw_distance\": dtw_distance} for row in partition_data])\n",
    "\n",
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row['time_series']\n",
    "        exemplar = row['exemplar']\n",
    "        \n",
    "        dtw_distance = dtw.distance(time_series, exemplar)\n",
    "        \n",
    "        updated_row = {**row, \"dtw_distance\": dtw_distance}\n",
    "        updated_rows.append(updated_row)\n",
    "    return iter(updated_rows)\n",
    "\n",
    "rdd_with_dtw = rdd_with_sampled_column.mapPartitions(calc_dtw_distance)\n",
    "for row in rdd_with_dtw.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964959a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59dd988",
   "metadata": {},
   "source": [
    "# WORKS FOR ANY NUM OF PARTITIONS AND EXEMPLARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05bf335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]]}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]]}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]]}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]]}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]]}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]]}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]]}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]]}\n",
      "\n",
      "rdd num partitions: 2\n"
     ]
    }
   ],
   "source": [
    "def create_sample_and_add_column_function(num_exemplars):\n",
    "    def sample_and_add_column(iterator):\n",
    "        partition_data = list(iterator)\n",
    "        exemplars = []\n",
    "        for row in sample(partition_data, min(num_exemplars, len(partition_data))):\n",
    "            exemplars.append(row['time_series'])\n",
    "        return iter([{**row, \"exemplars\": exemplars} for row in partition_data])\n",
    "    return sample_and_add_column\n",
    "\n",
    "# Example usage\n",
    "num_exemplars = 2\n",
    "\n",
    "chosen_exemplars = create_sample_and_add_column_function(num_exemplars)\n",
    "rdd_with_exemplar_column = rdd.mapPartitions(chosen_exemplars)\n",
    "\n",
    "for row in rdd_with_exemplar_column.collect():\n",
    "    print(row)\n",
    "\n",
    "print(f'\\nrdd num partitions: {rdd_with_exemplar_column.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7bed113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 2.2671568097509267}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 1.42828568570857, 'dtw_distance_exemplar_2': 4.085339643163099}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 2.2671568097509267, 'dtw_distance_exemplar_2': 0.0}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 1.1180339887498947, 'dtw_distance_exemplar_2': 1.42828568570857}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 1.118033988749895, 'dtw_distance_exemplar_2': 3.1208973068654466}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 1.42828568570857}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 1.42828568570857, 'dtw_distance_exemplar_2': 0.0}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 6.283311228962003, 'dtw_distance_exemplar_2': 4.085339643163099}\n",
      "\n",
      "rdd num partitions: 2\n"
     ]
    }
   ],
   "source": [
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row['time_series']\n",
    "        exemplars = row['exemplars']\n",
    "        \n",
    "        # Calculate DTW distances for each exemplar\n",
    "        dtw_distances = [dtw.distance(time_series, exemplar) for exemplar in exemplars]\n",
    "        \n",
    "        # Add each DTW distance as a separate column\n",
    "        updated_row = {**row}\n",
    "        for i, dtw_distance in enumerate(dtw_distances):\n",
    "            updated_row[f\"dtw_distance_exemplar_{i+1}\"] = dtw_distance\n",
    "        \n",
    "        updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_dtw = rdd_with_exemplar_column.mapPartitions(calc_dtw_distance)\n",
    "for row in rdd_with_dtw.collect():\n",
    "    print(row)\n",
    "\n",
    "print(f'\\nrdd num partitions: {rdd_with_dtw.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "667bd906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 3, 'time_series': [3.0, 4.1, 5.2, 6.3, 7.4], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 2.2671568097509267, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 4, 'time_series': [4.0, 5.1, 6.2, 7.3, 8.4], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 1.42828568570857, 'dtw_distance_exemplar_2': 4.085339643163099, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 1, 'time_series': [1.5, 2.6, 3.7, 4.8, 5.9], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 2.2671568097509267, 'dtw_distance_exemplar_2': 0.0, 'closest exemplar': 'dtw_distance_exemplar_2'}\n",
      "{'label': 2, 'time_series': [2.5, 3.6, 4.7, 5.8, 6.9], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 1.1180339887498947, 'dtw_distance_exemplar_2': 1.42828568570857, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 3, 'time_series': [3.5, 4.6, 5.7, 6.8, 7.9], 'exemplars': [[3.0, 4.1, 5.2, 6.3, 7.4], [1.5, 2.6, 3.7, 4.8, 5.9]], 'dtw_distance_exemplar_1': 1.118033988749895, 'dtw_distance_exemplar_2': 3.1208973068654466, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 1, 'time_series': [1.0, 2.1, 3.2, 4.3, 5.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 0.0, 'dtw_distance_exemplar_2': 1.42828568570857, 'closest exemplar': 'dtw_distance_exemplar_1'}\n",
      "{'label': 2, 'time_series': [2.0, 3.1, 4.2, 5.3, 6.4], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 1.42828568570857, 'dtw_distance_exemplar_2': 0.0, 'closest exemplar': 'dtw_distance_exemplar_2'}\n",
      "{'label': 4, 'time_series': [4.5, 5.6, 6.7, 7.8, 8.9], 'exemplars': [[1.0, 2.1, 3.2, 4.3, 5.4], [2.0, 3.1, 4.2, 5.3, 6.4]], 'dtw_distance_exemplar_1': 6.283311228962003, 'dtw_distance_exemplar_2': 4.085339643163099, 'closest exemplar': 'dtw_distance_exemplar_2'}\n"
     ]
    }
   ],
   "source": [
    "# not sure if this is needed\n",
    "\n",
    "def assign_closest_exemplar(iterator):\n",
    "    partition_data = list(iterator)\n",
    "\n",
    "    for row in partition_data:\n",
    "        # Check if there are DTW distances for exemplars\n",
    "        exemplar_distances = {key: value for key, value in row.items() if key.startswith(\"dtw_distance_exemplar_\")}\n",
    "        \n",
    "        if exemplar_distances:\n",
    "            # Find the exemplar with the smallest DTW distance\n",
    "            closest_exemplar = min(exemplar_distances, key=exemplar_distances.get)\n",
    "            \n",
    "            # Assign the closest exemplar to the row\n",
    "            row[\"closest exemplar\"] = closest_exemplar\n",
    "\n",
    "    return iter(partition_data)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_classification = rdd_with_dtw.mapPartitions(assign_closest_exemplar)\n",
    "for row in rdd_with_classification.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1539859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_partition_gini(iterator):\n",
    "    labels = [row['label'] for row in iterator]\n",
    "\n",
    "    label_counts_dict = {}\n",
    "    for label in labels:\n",
    "        if label in label_counts_dict:\n",
    "            label_counts_dict[label] += 1\n",
    "        else:\n",
    "            label_counts_dict[label] = 1\n",
    "    \n",
    "    total = sum(label_counts_dict.values())\n",
    "    proportion_sqrd_values = [(count / total) ** 2 for count in label_counts_dict.values()]\n",
    "    gini_impurity = 1 - sum(proportion_sqrd_values)\n",
    "    \n",
    "    return iter([gini_impurity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b0d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini impurity of partition 1: 0.72\n",
      "gini impurity of partition 2: 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "gini_rdd = rdd_with_classification.mapPartitions(calculate_partition_gini)\n",
    "\n",
    "# Collect and print the Gini impurity for each partition\n",
    "i=0\n",
    "for gini in gini_rdd.collect():\n",
    "    print(f'gini impurity of partition {i+1}: {gini}')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321d01d",
   "metadata": {},
   "source": [
    "### trying splitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56633663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsdata = [\n",
    "#     {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0], 'closest_exemplar': 'exemplar_1'},\n",
    "#     {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9], 'closest_exemplar': 'exemplar_2'},\n",
    "#     {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5], 'closest_exemplar': 'exemplar_1'},\n",
    "#     {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2], 'closest_exemplar': 'exemplar_2'},\n",
    "#     {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2], 'closest_exemplar': 'exemplar_1'},\n",
    "#     {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9], 'closest_exemplar': 'exemplar_2'},\n",
    "#     {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0], 'closest_exemplar': 'exemplar_1'},\n",
    "#     {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5], 'closest_exemplar': 'exemplar_2'},\n",
    "#     {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5], 'closest_exemplar': 'exemplar_1'},\n",
    "#     {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5], 'closest_exemplar': 'exemplar_2'}\n",
    "# ]\n",
    "\n",
    "# ts_rdd = sc.parallelize(tsdata)\n",
    "# ts_rdd = ts_rdd.repartition(2)\n",
    "\n",
    "# print(f'ts_rdd num partitions: {ts_rdd.getNumPartitions()}')\n",
    "\n",
    "# ts_rdd_gini = ts_rdd.mapPartitions(calculate_partition_gini)\n",
    "# # Collect and print the Gini impurity for each partition\n",
    "# i=0\n",
    "# for gini in ts_rdd_gini.collect():\n",
    "#     print(f'gini impurity of partition {i+1}: {gini}')\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2062c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closestto1_yes = [row['label'] for row in tsdata if row['closest_exemplar'] == 'exemplar_1']\n",
    "# closestto1_no = [row['label'] for row in tsdata if row['closest_exemplar'] != 'exemplar_1']\n",
    "\n",
    "# print(closestto1_yes)\n",
    "# print(closestto1_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f56951b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(labels):\n",
    "    label_counts = {}\n",
    "    for label in labels:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    total = sum(label_counts.values())\n",
    "    gini = 1 - sum((count / total) ** 2 for count in label_counts.values())\n",
    "    return gini\n",
    "\n",
    "def evaluate_splits_within_partition(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    \n",
    "    # If the partition is empty, return an empty iterator\n",
    "    if not partition_data:\n",
    "        return iter([])\n",
    "    \n",
    "    # Get all unique exemplar names in the partition\n",
    "    unique_exemplars = set(row['closest exemplar'] for row in partition_data)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Loop through each exemplar to evaluate splits\n",
    "    for exemplar_name in unique_exemplars:\n",
    "        # Split the data based on the current exemplar\n",
    "        yes_split = [row for row in partition_data if row['closest exemplar'] == exemplar_name]\n",
    "        no_split = [row for row in partition_data if row['closest exemplar'] != exemplar_name]\n",
    "        \n",
    "        # Calculate metrics for the split (e.g., Gini impurity)\n",
    "        yes_labels = [row['label'] for row in yes_split]\n",
    "        no_labels = [row['label'] for row in no_split]\n",
    "        \n",
    "        yes_gini = calculate_gini(yes_labels)\n",
    "        no_gini = calculate_gini(no_labels)\n",
    "        \n",
    "        # Store the results for this split\n",
    "        results.append({\n",
    "            'exemplar': exemplar_name,\n",
    "            'yes_gini': yes_gini,\n",
    "            'no_gini': no_gini,\n",
    "            'yes_split_size': len(yes_split),\n",
    "            'no_split_size': len(no_split)\n",
    "        })\n",
    "    \n",
    "    # Return the results as an iterator\n",
    "    return iter(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "864a41e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exemplar': 'dtw_distance_exemplar_1', 'yes_gini': 0.625, 'no_gini': 0.0, 'yes_split_size': 4, 'no_split_size': 1}\n",
      "{'exemplar': 'dtw_distance_exemplar_2', 'yes_gini': 0.0, 'no_gini': 0.625, 'yes_split_size': 1, 'no_split_size': 4}\n",
      "{'exemplar': 'dtw_distance_exemplar_1', 'yes_gini': 0.0, 'no_gini': 0.5, 'yes_split_size': 1, 'no_split_size': 2}\n",
      "{'exemplar': 'dtw_distance_exemplar_2', 'yes_gini': 0.5, 'no_gini': 0.0, 'yes_split_size': 2, 'no_split_size': 1}\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "ts_rdd_split_results = rdd_with_classification.mapPartitions(evaluate_splits_within_partition)\n",
    "# Collect and print the results\n",
    "for result in ts_rdd_split_results.collect():\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59e67b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_within_partition(iterator):\n",
    "#     partition_data = list(iterator)\n",
    "#     exemplar_name = partition_data[0]['closest_exemplar'] # randomly chosen exemplar name\n",
    "    \n",
    "#     # Split the data within the partition\n",
    "#     yes_split = [row for row in partition_data if row['closest_exemplar'] == exemplar_name]\n",
    "#     no_split = [row for row in partition_data if row['closest_exemplar'] != exemplar_name]\n",
    "    \n",
    "#     # Add a flag to indicate which split the row belongs to\n",
    "#     yes_split = [{**row, 'split': 'yes'} for row in yes_split]\n",
    "#     no_split = [{**row, 'split': 'no'} for row in no_split]\n",
    "    \n",
    "#     # Combine the splits and return as an iterator\n",
    "#     return iter(yes_split + no_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e50d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example usage\n",
    "# split_rdd = ts_rdd.mapPartitions(split_within_partition)\n",
    "\n",
    "# # Collect and print the results\n",
    "# for row in split_rdd.collect():\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe273b5",
   "metadata": {},
   "source": [
    "###running petrus stuff real quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71441488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from pyspark.sql import DataFrame\n",
    "# from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "# import logging\n",
    "# from random import sample\n",
    "# from dtaidistance import dtw\n",
    "\n",
    "# class GlobalModelManager:\n",
    "#     def __init__(self):\n",
    "#         self.num_exemplars = 2\n",
    "#         self.num_partitions = 2\n",
    "\n",
    "#     def train(self, df: DataFrame):\n",
    "#         rdd = self.partition_data(df)\n",
    "#         rdd = rdd.repartition(self.num_partitions)\n",
    "\n",
    "#         choose_exemplars = self.choose_exemplars_function(self.num_exemplars)\n",
    "#         rdd_with_exemplar_column = rdd.mapPartitions(choose_exemplars)\n",
    "\n",
    "#         rdd_with_dtw = rdd_with_exemplar_column.mapPartitions(self.calc_dtw_distance)\n",
    "\n",
    "#         rdd_with_closest_exemplar = rdd_with_dtw.mapPartitions(self.assign_closest_exemplar)\n",
    "\n",
    "#         rdd_with_gini = rdd_with_closest_exemplar.mapPartitions(self.calculate_partition_gini) # gini impurity before splitting\n",
    "\n",
    "#         rdd_splits = rdd_with_gini.mapPartitions(self.evaluate_splits_within_partition)\n",
    "\n",
    "#         return rdd_splits.collect()\n",
    "\n",
    "#     def partition_data(self, df: DataFrame) -> DataFrame:\n",
    "#         # Convert DataFrame to RDD\n",
    "#         rdd = df.rdd\n",
    "\n",
    "#         # Repartition the RDD\n",
    "#         repartitioned_rdd = rdd.repartition(self.num_partitions)\n",
    "\n",
    "#         # # prints out which row is in which partition\n",
    "#         # def print_partition_rows(index, iterator):\n",
    "#         #     return iter((index, row) for row in iterator)\n",
    "#         # partitioned_rdd_with_index = repartitioned_rdd.mapPartitionsWithIndex(print_partition_rows)\n",
    "#         # for partition_index, row in partitioned_rdd_with_index.collect():\n",
    "#         #     print(f\"Partition {partition_index}: {row}\")\n",
    "\n",
    "#         return repartitioned_rdd\n",
    "\n",
    "\n",
    "\n",
    "#     def choose_exemplars_function(self, num_exemplars):\n",
    "#         def choose_exemplars(iterator):\n",
    "#             partition_data = list(iterator)\n",
    "#             chosen_exemplars = sample(partition_data, min(num_exemplars, len(partition_data)))\n",
    "#             exemplar_series = [row['time_series'] for row in chosen_exemplars]\n",
    "\n",
    "#             # Remove exemplars from the working data\n",
    "#             filtered_partition = [\n",
    "#                 row for row in partition_data\n",
    "#                 if row['time_series'] not in exemplar_series\n",
    "#             ]\n",
    "\n",
    "#             return iter([{**row.asDict(), \"exemplars\": exemplar_series} for row in filtered_partition])\n",
    "#         return choose_exemplars\n",
    "\n",
    "    \n",
    "#     def calc_dtw_distance(self, iterator):\n",
    "#         partition_data = list(iterator)\n",
    "#         updated_rows = []\n",
    "        \n",
    "#         for row in partition_data:\n",
    "#             time_series = row['time_series']\n",
    "#             exemplars = row['exemplars']\n",
    "            \n",
    "#             dtw_distances = [dtw.distance(time_series, exemplar) for exemplar in exemplars]\n",
    "            \n",
    "#             updated_row = {**row}\n",
    "\n",
    "#             for i, dtw_distance in enumerate(dtw_distances):\n",
    "#                 updated_row[f\"dtw_distance_exemplar_{i+1}\"] = dtw_distance\n",
    "            \n",
    "#             updated_rows.append(updated_row)\n",
    "        \n",
    "#         return iter(updated_rows)\n",
    "    \n",
    "#     def assign_closest_exemplar(self, iterator):\n",
    "#         partition_data = list(iterator)\n",
    "\n",
    "#         for row in partition_data:\n",
    "#             # Check if there are DTW distances for exemplars\n",
    "#             exemplar_distances = {key: value for key, value in row.items() if key.startswith(\"dtw_distance_exemplar_\")}\n",
    "            \n",
    "#             if exemplar_distances:\n",
    "#                 # Find the exemplar with the smallest DTW distance\n",
    "#                 closest_exemplar = min(exemplar_distances, key=exemplar_distances.get)\n",
    "                \n",
    "#                 # Assign the closest exemplar to the row\n",
    "#                 row[\"closest exemplar\"] = closest_exemplar\n",
    "\n",
    "#         return iter(partition_data)\n",
    "    \n",
    "#     def calculate_partition_gini(self, iterator):\n",
    "#         partition_data = list(iterator)\n",
    "#         labels = [row['label'] for row in partition_data]\n",
    "\n",
    "#         # Calculate Gini impurity for the partition\n",
    "#         label_counts_dict = {}\n",
    "#         for label in labels:\n",
    "#             label_counts_dict[label] = label_counts_dict.get(label, 0) + 1\n",
    "\n",
    "#         total = sum(label_counts_dict.values())\n",
    "#         proportion_sqrd_values = [(count / total) ** 2 for count in label_counts_dict.values()]\n",
    "#         gini_impurity = 1 - sum(proportion_sqrd_values)\n",
    "\n",
    "#         # Add Gini impurity to each row in the partition\n",
    "#         updated_rows = []\n",
    "#         for row in partition_data:\n",
    "#             updated_row = {**row, \"partition_gini\": gini_impurity}\n",
    "#             updated_rows.append(updated_row)\n",
    "\n",
    "#         return iter(updated_rows)\n",
    "    \n",
    "#     def calculate_gini(self, labels):\n",
    "#         label_counts = {}\n",
    "#         for label in labels:\n",
    "#             label_counts[label] = label_counts.get(label, 0) + 1\n",
    "#         total = sum(label_counts.values())\n",
    "#         gini = 1 - sum((count / total) ** 2 for count in label_counts.values())\n",
    "#         return gini\n",
    "\n",
    "#     def evaluate_splits_within_partition(self, iterator):\n",
    "#         partition_data = list(iterator)\n",
    "        \n",
    "#         # If the partition is empty, return an empty iterator\n",
    "#         if not partition_data:\n",
    "#             return iter([])\n",
    "        \n",
    "#         partition_gini = self.calculate_gini([row['label'] for row in partition_data])\n",
    "        \n",
    "#         # Get all unique exemplar names in the partition\n",
    "#         unique_exemplars = set(row['closest exemplar'] for row in partition_data)\n",
    "        \n",
    "#         updated_rows = []\n",
    "        \n",
    "#         # For each row, evaluate all potential splits at once\n",
    "#         for row in partition_data:\n",
    "#             split_evaluations = {}\n",
    "            \n",
    "#             for exemplar_name in unique_exemplars:\n",
    "#                 # Split the data based on the current exemplar\n",
    "#                 yes_split = [r for r in partition_data if r['closest exemplar'] == exemplar_name]\n",
    "#                 no_split = [r for r in partition_data if r['closest exemplar'] != exemplar_name]\n",
    "                \n",
    "#                 # Calculate metrics for the split\n",
    "#                 yes_labels = [r['label'] for r in yes_split]\n",
    "#                 no_labels = [r['label'] for r in no_split]\n",
    "                \n",
    "#                 yes_gini = self.calculate_gini(yes_labels)\n",
    "#                 no_gini = self.calculate_gini(no_labels)\n",
    "                \n",
    "#                 # Store the evaluation for this exemplar\n",
    "#                 split_evaluations[exemplar_name] = {\n",
    "#                     \"yes_gini\": yes_gini,\n",
    "#                     \"no_gini\": no_gini,\n",
    "#                     \"yes_split_size\": len(yes_split),\n",
    "#                     \"no_split_size\": len(no_split)\n",
    "#                 }\n",
    "            \n",
    "#             # Find the best split (lowest weighted average Gini)\n",
    "#             best_split = None\n",
    "#             best_gini = float('inf')\n",
    "            \n",
    "#             for exemplar, metrics in split_evaluations.items():\n",
    "#                 total = metrics[\"yes_split_size\"] + metrics[\"no_split_size\"]\n",
    "#                 weighted_gini = (metrics[\"yes_gini\"] * metrics[\"yes_split_size\"] / total + \n",
    "#                                 metrics[\"no_gini\"] * metrics[\"no_split_size\"] / total)\n",
    "                \n",
    "#                 if weighted_gini < best_gini:\n",
    "#                     best_gini = weighted_gini\n",
    "#                     best_split = exemplar\n",
    "            \n",
    "#             # Add all the information to the row\n",
    "#             updated_row = {\n",
    "#                 **row,\n",
    "#                 \"before_split_partition_gini\": partition_gini,\n",
    "#                 \"split_evaluations\": split_evaluations,\n",
    "#                 \"best_split_exemplar\": best_split,\n",
    "#                 \"best_split_gini\": best_gini,\n",
    "#                 \"gini_reduction\": partition_gini - best_gini\n",
    "#             }\n",
    "            \n",
    "#             updated_rows.append(updated_row)\n",
    "\n",
    "#         return iter(updated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fa99d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from random import sample\n",
    "from dtaidistance import dtw\n",
    "\n",
    "class GlobalModelManager:\n",
    "    def __init__(self):\n",
    "        self.num_exemplars = 2\n",
    "        self.num_partitions = 2\n",
    "\n",
    "    def train(self, df: DataFrame):\n",
    "        rdd = self.partition_data(df)\n",
    "        rdd = rdd.repartition(self.num_partitions)\n",
    "\n",
    "        choose_exemplars = self.choose_exemplars_function(self.num_exemplars)\n",
    "        rdd_with_exemplar_column = rdd.mapPartitions(choose_exemplars)\n",
    "\n",
    "        rdd_with_dtw = rdd_with_exemplar_column.mapPartitions(self.calc_dtw_distance)\n",
    "\n",
    "        rdd_with_closest_exemplar = rdd_with_dtw.mapPartitions(self.assign_closest_exemplar)\n",
    "\n",
    "        # Evaluate splits and collect minimal results\n",
    "        results = rdd_with_closest_exemplar.mapPartitionsWithIndex(self.evaluate_splits_within_partition).collect()\n",
    "\n",
    "        # Print partition number, best exemplar time series, and Gini reduction\n",
    "        for result in results:\n",
    "            print(f\"Partition {result['partition_id']}, Best Exemplar: {result['best_split_exemplar_ts']}, Gini Reduction: {result['best_gini_reduction']}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def partition_data(self, df: DataFrame) -> DataFrame:\n",
    "        # Convert DataFrame to RDD and add partition_id\n",
    "        rdd = df.rdd.mapPartitionsWithIndex(\n",
    "            lambda idx, iter: [{**row.asDict(), \"partition_id\": idx} for row in iter]\n",
    "        )\n",
    "        return rdd\n",
    "\n",
    "    def choose_exemplars_function(self, num_exemplars):\n",
    "        def choose_exemplars(iterator):\n",
    "            partition_data = list(iterator)\n",
    "            if not partition_data:\n",
    "                return iter([])\n",
    "            chosen_exemplars = sample(partition_data, min(num_exemplars, len(partition_data)))\n",
    "            exemplar_series = [row['time_series'] for row in chosen_exemplars]\n",
    "\n",
    "            # Remove exemplars from the working data\n",
    "            filtered_partition = [\n",
    "                row for row in partition_data\n",
    "                if row['time_series'] not in exemplar_series\n",
    "            ]\n",
    "\n",
    "            return iter([{**row, \"exemplars\": exemplar_series} for row in filtered_partition])\n",
    "        return choose_exemplars\n",
    "\n",
    "    def calc_dtw_distance(self, iterator):\n",
    "        partition_data = list(iterator)\n",
    "        updated_rows = []\n",
    "        \n",
    "        for row in partition_data:\n",
    "            time_series = row.get('time_series', [])\n",
    "            exemplars = row.get('exemplars', [])\n",
    "            if not exemplars:\n",
    "                continue  # Skip if no exemplars\n",
    "            \n",
    "            dtw_distances = [dtw.distance(time_series, exemplar) for exemplar in exemplars]\n",
    "            \n",
    "            updated_row = {**row}\n",
    "            updated_row['exemplar_map'] = {}  # Map exemplar IDs to time series\n",
    "\n",
    "            for i, (dtw_distance, exemplar_ts) in enumerate(zip(dtw_distances, exemplars)):\n",
    "                exemplar_id = f\"dtw_distance_exemplar_{i+1}\"\n",
    "                updated_row[exemplar_id] = dtw_distance\n",
    "                updated_row['exemplar_map'][exemplar_id] = exemplar_ts\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "        \n",
    "        return iter(updated_rows)\n",
    "    \n",
    "    def assign_closest_exemplar(self, iterator):\n",
    "        partition_data = list(iterator)\n",
    "        updated_rows = []\n",
    "\n",
    "        for row in partition_data:\n",
    "            # Check if there are DTW distances for exemplars\n",
    "            exemplar_distances = {\n",
    "                key: value for key, value in row.items()\n",
    "                if key.startswith(\"dtw_distance_exemplar_\") and isinstance(value, (int, float))\n",
    "            }\n",
    "            \n",
    "            updated_row = {**row}\n",
    "            if exemplar_distances:\n",
    "                # Find the exemplar with the smallest DTW distance\n",
    "                closest_exemplar_id = min(exemplar_distances, key=exemplar_distances.get)\n",
    "                \n",
    "                # Assign the closest exemplar ID and time series\n",
    "                updated_row[\"closest_exemplar_id\"] = closest_exemplar_id\n",
    "                updated_row[\"closest_exemplar_ts\"] = row.get('exemplar_map', {}).get(closest_exemplar_id, [])\n",
    "            else:\n",
    "                # Handle case with no valid distances\n",
    "                updated_row[\"closest_exemplar_id\"] = None\n",
    "                updated_row[\"closest_exemplar_ts\"] = []\n",
    "\n",
    "            updated_rows.append(updated_row)\n",
    "\n",
    "        return iter(updated_rows)\n",
    "    \n",
    "    def calculate_gini(self, labels):\n",
    "        if not labels:\n",
    "            return 0\n",
    "        label_counts = {}\n",
    "        for label in labels:\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1\n",
    "        total = sum(label_counts.values())\n",
    "        gini = 1 - sum((count / total) ** 2 for count in label_counts.values()) if total > 0 else 0\n",
    "        return gini\n",
    "\n",
    "    def evaluate_splits_within_partition(self, index, iterator):\n",
    "        partition_data = list(iterator)\n",
    "        \n",
    "        # Handle empty partition\n",
    "        if not partition_data:\n",
    "            return iter([{\n",
    "                \"partition_id\": index,\n",
    "                \"best_split_exemplar_ts\": None,\n",
    "                \"best_gini_reduction\": None\n",
    "            }])\n",
    "        \n",
    "        # Calculate Gini impurity before splitting\n",
    "        labels = [row.get('label') for row in partition_data if row.get('label') is not None]\n",
    "        before_split_gini = self.calculate_gini(labels)\n",
    "        \n",
    "        # Get all unique exemplar IDs in the partition\n",
    "        unique_exemplars = set(\n",
    "            row['closest_exemplar_id'] for row in partition_data\n",
    "            if row.get('closest_exemplar_id') is not None\n",
    "        )\n",
    "        \n",
    "        # Handle case with no valid exemplars\n",
    "        if not unique_exemplars:\n",
    "            return iter([{\n",
    "                \"partition_id\": index,\n",
    "                \"best_split_exemplar_ts\": None,\n",
    "                \"best_gini_reduction\": None\n",
    "            }])\n",
    "        \n",
    "        # Evaluate all possible splits\n",
    "        best_split_exemplar_ts = None\n",
    "        best_gini_reduction = float('-inf')\n",
    "        best_weighted_gini = None\n",
    "        \n",
    "        for exemplar_id in unique_exemplars:\n",
    "            # Split the data based on the current exemplar\n",
    "            yes_split = [r for r in partition_data if r.get('closest_exemplar_id') == exemplar_id]\n",
    "            no_split = [r for r in partition_data if r.get('closest_exemplar_id') != exemplar_id]\n",
    "            \n",
    "            # Calculate Gini for each daughter node\n",
    "            yes_labels = [r.get('label') for r in yes_split if r.get('label') is not None]\n",
    "            no_labels = [r.get('label') for r in no_split if r.get('label') is not None]\n",
    "            \n",
    "            yes_gini = self.calculate_gini(yes_labels)\n",
    "            no_gini = self.calculate_gini(no_labels)\n",
    "            \n",
    "            # Calculate weighted Gini after split\n",
    "            total_size = len(yes_split) + len(no_split)\n",
    "            weighted_gini = (yes_gini * len(yes_split) / total_size + no_gini * len(no_split) / total_size) if total_size > 0 else float('inf')\n",
    "            \n",
    "            # Calculate Gini reduction\n",
    "            gini_reduction = before_split_gini - weighted_gini if total_size > 0 else float('-inf')\n",
    "            \n",
    "            # Get the exemplar time series for this exemplar_id\n",
    "            exemplar_ts = yes_split[0].get('exemplar_map', {}).get(exemplar_id, []) if yes_split else []\n",
    "            \n",
    "            # Update best split if this one has a larger Gini reduction\n",
    "            if gini_reduction > best_gini_reduction:\n",
    "                best_gini_reduction = gini_reduction\n",
    "                best_split_exemplar_ts = exemplar_ts\n",
    "                best_weighted_gini = weighted_gini\n",
    "        \n",
    "        # Handle case where no valid split was found\n",
    "        if best_gini_reduction == float('-inf'):\n",
    "            return iter([{\n",
    "                \"partition_id\": index,\n",
    "                \"best_split_exemplar_ts\": None,\n",
    "                \"best_gini_reduction\": None\n",
    "            }])\n",
    "        \n",
    "        return iter([{\n",
    "            \"partition_id\": index,\n",
    "            \"best_split_exemplar_ts\": best_split_exemplar_ts,\n",
    "            \"best_gini_reduction\": best_gini_reduction\n",
    "        }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "710807b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]},\n",
    "    {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
    "    {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
    "    {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
    "    {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
    "    {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
    "    {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
    "    {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
    "    {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
    "    {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(tsdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9aa8a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------+\n",
      "|label|time_series              |\n",
      "+-----+-------------------------+\n",
      "|1    |[1.2, 2.4, 3.6, 4.8, 6.0]|\n",
      "|1    |[1.0, 1.8, 2.6, 3.4, 4.2]|\n",
      "|1    |[0.9, 1.8, 2.7, 3.6, 4.5]|\n",
      "|1    |[1.5, 2.1, 2.7, 3.3, 3.9]|\n",
      "|1    |[0.8, 1.7, 2.5, 3.2, 4.0]|\n",
      "+-----+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ea7a4e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0, Best Exemplar: [2.1, 3.3, 4.5, 5.7, 6.9], Gini Reduction: 0.26\n",
      "Partition 1, Best Exemplar: [1.0, 1.8, 2.6, 3.4, 4.2], Gini Reduction: 0.05333333333333318\n",
      "{'partition_id': 0, 'best_split_exemplar_ts': [2.1, 3.3, 4.5, 5.7, 6.9], 'best_gini_reduction': 0.26}\n",
      "{'partition_id': 1, 'best_split_exemplar_ts': [1.0, 1.8, 2.6, 3.4, 4.2], 'best_gini_reduction': 0.05333333333333318}\n"
     ]
    }
   ],
   "source": [
    "global_model = GlobalModelManager()\n",
    "results = global_model.train(df)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8804a018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
