{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122a1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from data_ingestion import DataIngestion\n",
    "from preprocessing import Preprocessor\n",
    "from prediction_manager import PredictionManager\n",
    "from local_model_manager import LocalModelManager\n",
    "from evaluation import Evaluator\n",
    "from utilities import show_compact\n",
    "import time\n",
    "import json\n",
    "from random import sample\n",
    "from dtaidistance import dtw\n",
    "import collections\n",
    "from pprint import pprint\n",
    "import random\n",
    "import collections\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f20c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, ArrayType, MapType\n",
    "import random\n",
    "import collections\n",
    "import math # For Euclidean distance\n",
    "import json # To potentially serialize complex split_on info if needed, though plain dict is better\n",
    "\n",
    "# Define a simple Euclidean distance function for use in UDF\n",
    "# In a real implementation, this would handle multiple distance measures and parameters\n",
    "def euclidean_distance(ts1, ts2):\n",
    "    \"\"\"Calculates Euclidean distance between two time series.\"\"\"\n",
    "    if ts1 is None or ts2 is None or len(ts1) != len(ts2):\n",
    "        return float('inf') # Handle invalid inputs\n",
    "    # Ensure both are lists of numbers\n",
    "    try:\n",
    "        dist = math.sqrt(sum([(a - b) ** 2 for a, b in zip(ts1, ts2)]))\n",
    "        return float(dist) # Return as float\n",
    "    except Exception as e:\n",
    "        # Print error only in debug mode or with proper logging\n",
    "        # print(f\"Error calculating distance: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "# Define a UDF for predicting a single time series instance\n",
    "# This UDF will need access to the tree structure (broadcasted plain dictionary)\n",
    "def predict_udf_func(plain_tree_structure_broadcast):\n",
    "    \"\"\"\n",
    "    Returns a UDF that traverses the tree for a single time series instance.\n",
    "    plain_tree_structure_broadcast: Broadcast variable containing the plain dictionary tree structure.\n",
    "    \"\"\"\n",
    "    tree = plain_tree_structure_broadcast.value\n",
    "\n",
    "    def traverse_tree(time_series):\n",
    "        \"\"\"Traverse the tree for a single time series instance.\"\"\"\n",
    "        if time_series is None:\n",
    "            return None # Or a default prediction\n",
    "\n",
    "        node_id = 0  # Start at root\n",
    "\n",
    "        # Traverse the tree until a leaf node is reached or traversal stops\n",
    "        while node_id in tree:\n",
    "            current_node = tree[node_id]\n",
    "\n",
    "            # If it's a leaf node, return its prediction\n",
    "            if current_node['is_leaf']:\n",
    "                return current_node['prediction']\n",
    "\n",
    "            # If it's an internal node, use the split info to decide which branch to follow\n",
    "            split_info = current_node.get('split_on') # Use .get for safety\n",
    "            children = current_node.get('children')\n",
    "\n",
    "            # Ensure split info and children exist for internal nodes\n",
    "            if split_info and children and len(children) > 0:\n",
    "                measure_type, exemplars = split_info # split_info is (measure_type, {branch_id: exemplar_ts})\n",
    "                \n",
    "                # Calculate distance to ALL exemplars for this node's split\n",
    "                min_dist_all_exemplars = float('inf')\n",
    "                best_branch_id_all_exemplars = None\n",
    "\n",
    "                for branch_id, exemplar_ts in exemplars.items():\n",
    "                    # Calculate distance using the specified measure (placeholder: euclidean)\n",
    "                    # In a real implementation, call a function that dispatches based on measure_type\n",
    "                    d = euclidean_distance(time_series, exemplar_ts) # Use the distance function\n",
    "\n",
    "                    if d < min_dist_all_exemplars:\n",
    "                        min_dist_all_exemplars = d\n",
    "                        best_branch_id_all_exemplars = branch_id\n",
    "\n",
    "                # --- Enhanced Traversal Logic ---\n",
    "                # Check if the child node corresponding to the nearest exemplar exists\n",
    "                if best_branch_id_all_exemplars is not None and best_branch_id_all_exemplars in children:\n",
    "                    # If the child exists, move to that child node\n",
    "                    node_id = children[best_branch_id_all_exemplars]\n",
    "                    # print(f\"DEBUG: Node {current_node['node_id']}, nearest exemplar branch {best_branch_id_all_exemplars} exists, moving to child {node_id}\") # Debug\n",
    "                else:\n",
    "                    # If the child corresponding to the nearest exemplar does NOT exist (pruned branch),\n",
    "                    # find the nearest exemplar among the *existing* child branches and follow that path.\n",
    "                    # print(f\"DEBUG: Node {current_node['node_id']}, nearest exemplar branch {best_branch_id_all_exemplars} does not exist. Finding nearest among existing children.\") # Debug)\n",
    "                    min_dist_existing_children = float('inf')\n",
    "                    next_node_id = None\n",
    "\n",
    "                    # Iterate through the *existing* child branches\n",
    "                    for existing_branch_id, existing_child_id in children.items():\n",
    "                        # Find the exemplar time series for this existing branch from the original exemplars\n",
    "                        # It's crucial that the branch_id used as the key in 'children' corresponds\n",
    "                        # to the branch_id (exemplar label) in the 'exemplars' dictionary.\n",
    "                        if existing_branch_id in exemplars:\n",
    "                             existing_exemplar_ts = exemplars[existing_branch_id]\n",
    "                             # Calculate distance to this existing branch's exemplar\n",
    "                             d = euclidean_distance(time_series, existing_exemplar_ts) # Use the distance function\n",
    "\n",
    "                             if d < min_dist_existing_children:\n",
    "                                 min_dist_existing_children = d\n",
    "                                 next_node_id = existing_child_id\n",
    "\n",
    "                    # If a nearest existing child was found, move to that child node\n",
    "                    if next_node_id is not None:\n",
    "                        node_id = next_node_id\n",
    "                        # print(f\"DEBUG: Node {current_node['node_id']}, routed to nearest existing child {node_id} via branch {next_node_id}.\") # Debug)\n",
    "                    else:\n",
    "                        # If no existing children were found (shouldn't happen if children dict is non-empty),\n",
    "                        # stop traversal and return the current node's prediction.\n",
    "                        # print(f\"DEBUG: Node {current_node['node_id']}, no existing children found. Stopping traversal.\") # Debug)\n",
    "                        return current_node.get('prediction') # Return prediction of current node\n",
    "\n",
    "            else:\n",
    "                 # If the node is internal but has no split info or children (shouldn't happen with correct training)\n",
    "                 # print(f\"DEBUG: Node {current_node['node_id']} is internal but has no split info/children, stopping traversal.\") # Debug)\n",
    "                 # Return the prediction of the current node\n",
    "                 return current_node.get('prediction')\n",
    "\n",
    "\n",
    "        # If the loop finishes without returning (shouldn't happen if root exists),\n",
    "        # or if the final node_id is not in the tree (error case)\n",
    "        # Return a default fallback prediction\n",
    "        # print(f\"DEBUG: Traversal ended unexpectedly at node {node_id}. Using default 1.\") # Debug)\n",
    "        return 1 # Default fallback prediction\n",
    "\n",
    "\n",
    "    return traverse_tree\n",
    "\n",
    "\n",
    "class GlobalProxTree:\n",
    "    def __init__(self, spark, max_depth=5, min_samples=5, num_candidate_splits=5, num_exemplars_per_class=1):\n",
    "        \"\"\"\n",
    "        Initialize the Global Proximity Tree\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        spark : SparkSession\n",
    "            The Spark session to use\n",
    "        max_depth : int\n",
    "            Maximum depth of the tree\n",
    "        min_samples : int\n",
    "            Minimum number of samples required to split a node\n",
    "        num_candidate_splits : int\n",
    "            Number of random candidate splits to evaluate at each node.\n",
    "        num_exemplars_per_class : int\n",
    "            Number of exemplars to sample per class for each open node.\n",
    "            (Used for sampling pool on driver, not per candidate split as in paper)\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.num_candidate_splits = num_candidate_splits\n",
    "        # Note: num_exemplars_per_class here is used to sample a pool of exemplars\n",
    "        # to the driver per node/label, not per candidate split.\n",
    "        # The paper samples 1 exemplar per class *per candidate split*.\n",
    "        self.num_exemplars_per_class = num_exemplars_per_class\n",
    "\n",
    "\n",
    "        # Define the schema for data assigned to nodes\n",
    "        self.assignment_schema = StructType([\n",
    "            StructField(\"row_id\", IntegerType(), False), # Add a unique row ID\n",
    "            StructField(\"node_id\", IntegerType(), False),\n",
    "            StructField(\"time_series\", ArrayType(DoubleType()), False),\n",
    "            StructField(\"true_label\", IntegerType(), False),\n",
    "        ])\n",
    "\n",
    "        # Define the schema for tagged dataframe during splitting\n",
    "        # This schema is for the lighter DataFrame used for Gini calculation\n",
    "        # (No longer used to create a DataFrame, but kept for reference)\n",
    "        # self.tagged_gini_schema = StructType([\n",
    "        #     StructField(\"row_id\", IntegerType(), False),\n",
    "        #     StructField(\"node_id\", IntegerType(), False), # Parent node ID\n",
    "        #     StructField(\"true_label\", IntegerType(), False),\n",
    "        #     StructField(\"assigned_branch_id\", IntegerType(), False), # Branch ID assigned by the split (label of nearest exemplar)\n",
    "        # ])\n",
    "\n",
    "\n",
    "        # Define the TreeNode structure (still used on the driver)\n",
    "        # split_on will now store information about the chosen split:\n",
    "        # (distance_measure_type: str, {branch_id: exemplar_time_series})\n",
    "        # branch_id here is the label of the exemplar\n",
    "        self.TreeNode = collections.namedtuple(\n",
    "            \"TreeNode\",\n",
    "            \"node_id parent_id split_on is_leaf prediction children\".split()\n",
    "        )\n",
    "\n",
    "        # Initialize the tree with a root node\n",
    "        self.tree = {\n",
    "            0: self.TreeNode(\n",
    "                node_id=0,\n",
    "                parent_id=None,\n",
    "                split_on=None, # Will store the chosen split info (measure, exemplars)\n",
    "                is_leaf=False,\n",
    "                prediction=None,\n",
    "                children={}, # {branch_id: child_node_id}\n",
    "            )\n",
    "        }\n",
    "        self._next_node_id = 1 # Counter for assigning new node IDs\n",
    "\n",
    "        # Store the overall majority class for fallback prediction if needed\n",
    "        self._overall_majority_class = None\n",
    "\n",
    "\n",
    "    def _convert_to_time_series_format(self, df):\n",
    "        \"\"\"\n",
    "        Convert wide dataframe (with each feature in its own column) to a dataframe\n",
    "        with a single array column containing all features. Adds a unique row_id.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : Spark DataFrame\n",
    "            Wide DataFrame with feature columns and label column\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Spark DataFrame\n",
    "            DataFrame with 'row_id', 'time_series' and 'label' columns\n",
    "        \"\"\"\n",
    "        print(\"DEBUG: _convert_to_time_series_format started.\")\n",
    "        # Check if 'time_series' column already exists\n",
    "        if 'time_series' in df.columns:\n",
    "            print(\"DEBUG: DataFrame already has 'time_series' column, no conversion needed.\")\n",
    "            # Ensure row_id is present\n",
    "            if 'row_id' not in df.columns:\n",
    "                 print(\"DEBUG: Adding row_id to existing time_series DataFrame.\")\n",
    "                 df = df.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "            return df\n",
    "\n",
    "        # Get all column names except 'label'\n",
    "        feature_cols = [col for col in df.columns if col != 'label']\n",
    "\n",
    "        print(f\"DEBUG: Converting {len(feature_cols)} feature columns to 'time_series' array.\")\n",
    "\n",
    "        # Use array() function to combine columns and add a unique row_id\n",
    "        ts_df = df.select(\n",
    "            F.monotonically_increasing_id().alias(\"row_id\"), # Add unique ID\n",
    "            F.array(*[F.col(c) for c in feature_cols]).alias(\"time_series\"),\n",
    "            df[\"label\"].cast(IntegerType()).alias(\"true_label\")  # Ensure label is an integer and rename\n",
    "        )\n",
    "\n",
    "        # Show sample of converted data\n",
    "        print(\"DEBUG: Sample of converted DataFrame:\")\n",
    "        ts_df.show(2, truncate=False)\n",
    "        print(\"DEBUG: _convert_to_time_series_format finished.\")\n",
    "\n",
    "        return ts_df\n",
    "\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fit the decision tree on the dataframe\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : Spark DataFrame\n",
    "            DataFrame with feature columns and 'label' column\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : GlobalProxTree\n",
    "            The fitted tree\n",
    "        \"\"\"\n",
    "        print(\"DEBUG: fit started.\")\n",
    "        # First, convert to time_series format if needed and add row_id\n",
    "        df = self._convert_to_time_series_format(df)\n",
    "\n",
    "        # Calculate overall majority class for fallback prediction\n",
    "        label_counts = df.groupBy(\"true_label\").count().collect()\n",
    "        if label_counts:\n",
    "            self._overall_majority_class = max(label_counts, key=lambda x: x['count'])['true_label']\n",
    "            print(f\"DEBUG: Overall majority class calculated: {self._overall_majority_class}\")\n",
    "        else:\n",
    "            self._overall_majority_class = None\n",
    "            print(\"DEBUG: No data to calculate overall majority class.\")\n",
    "\n",
    "\n",
    "        # Initialize assignment dataframe with all rows at the root node\n",
    "        # Select only necessary columns to minimize data size\n",
    "        assign_df = (\n",
    "            df\n",
    "            .withColumn(\"node_id\", F.lit(0).cast(IntegerType()))\n",
    "            .select(\"row_id\", \"node_id\", \"time_series\", \"true_label\")\n",
    "            .cache()\n",
    "        )\n",
    "        print(f\"DEBUG: Initial assign_df created with {assign_df.count()} rows at root node 0.\")\n",
    "\n",
    "\n",
    "        open_nodes = {0}\n",
    "\n",
    "        for depth in range(self.max_depth):\n",
    "            print(f\"\\nDEBUG: === Starting tree level {depth} ===\")\n",
    "            # If no nodes to expand, stop\n",
    "            if not open_nodes:\n",
    "                print(f\"DEBUG: No open_nodes at depth {depth}, stopping tree building.\")\n",
    "                break\n",
    "\n",
    "            print(f\"DEBUG: Open nodes at depth {depth}: {open_nodes}\")\n",
    "\n",
    "            # Filter assign_df to only include rows at the current open nodes\n",
    "            current_level_df = assign_df.filter(F.col(\"node_id\").isin(list(open_nodes))).cache()\n",
    "            print(f\"DEBUG: Filtered data for current level. Row count: {current_level_df.count()}\")\n",
    "\n",
    "            # Check if any data exists for the current open nodes\n",
    "            if current_level_df.count() == 0:\n",
    "                 print(f\"DEBUG: No data for open nodes at depth {depth}, stopping.\")\n",
    "                 current_level_df.unpersist()\n",
    "                 break\n",
    "\n",
    "\n",
    "            # --- Corrected Exemplar Sampling Logic (Driver-side) ---\n",
    "            print(\"DEBUG: Sampling exemplars (driver-side).\")\n",
    "            sampled_exemplars = {} # {node_id: {true_label: [exemplar_ts1, exemplar_ts2, ...]}}\n",
    "\n",
    "            # Get distinct (node_id, true_label) pairs present in the current level's data\n",
    "            node_label_pairs = current_level_df.select(\"node_id\", \"true_label\").distinct().collect()\n",
    "            print(f\"DEBUG: Found {len(node_label_pairs)} distinct (node_id, true_label) pairs for sampling.\")\n",
    "\n",
    "            for node_id, true_label in node_label_pairs:\n",
    "                 print(f\"DEBUG: Sampling exemplars for node {node_id}, label {true_label}.\")\n",
    "                 # Filter the current level's data for this specific node and label\n",
    "                 node_label_df = current_level_df.filter((F.col(\"node_id\") == node_id) & (F.col(\"true_label\") == true_label))\n",
    "\n",
    "                 # Take a sample of rows for this node and label\n",
    "                 # Use .limit() and .collect() on a small sample to avoid OOM\n",
    "                 # A more robust way might use RDD.takeSample\n",
    "                 sampled_rows = node_label_df.limit(self.num_exemplars_per_class).collect()\n",
    "                 sampled_time_series = [row.time_series for row in sampled_rows]\n",
    "\n",
    "                 if node_id not in sampled_exemplars:\n",
    "                      sampled_exemplars[node_id] = {}\n",
    "                 sampled_exemplars[node_id][true_label] = sampled_time_series\n",
    "                 print(f\"DEBUG: Sampled {len(sampled_time_series)} exemplars for node {node_id}, label {true_label}.\")\n",
    "\n",
    "            print(f\"DEBUG: Finished sampling exemplars. Total sampled exemplars structure: {sampled_exemplars}\")\n",
    "            # --- End Corrected Exemplar Sampling ---\n",
    "\n",
    "\n",
    "            # 2. Generate and evaluate candidate splits for each open node\n",
    "            # This logic runs on the driver, but uses distributed operations for evaluation\n",
    "            best_splits = {} # {node_id: (best_gini_gain, best_distance_measure, {branch_id: exemplar_ts})}\n",
    "            # --- FIX: Initialize nodes_to_make_leaves_this_iter here ---\n",
    "            nodes_to_make_leaves_this_iter = set() # Nodes that should become leaves in *this* iteration\n",
    "\n",
    "            for node_id in open_nodes:\n",
    "                print(f\"DEBUG: Evaluating splits for node {node_id}.\")\n",
    "                if node_id not in sampled_exemplars or not sampled_exemplars[node_id]:\n",
    "                    print(f\"DEBUG: No exemplars found for node {node_id}, making it a leaf.\")\n",
    "                    nodes_to_make_leaves_this_iter.add(node_id)\n",
    "                    continue # Cannot split without exemplars\n",
    "\n",
    "                node_data_df = current_level_df.filter(F.col(\"node_id\") == node_id).cache()\n",
    "                node_total_samples = node_data_df.count()\n",
    "\n",
    "                if node_total_samples < self.min_samples:\n",
    "                    print(f\"DEBUG: Node {node_id} has {node_total_samples} samples, below min_samples {self.min_samples}, making it a leaf.\")\n",
    "                    nodes_to_make_leaves_this_iter.add(node_id)\n",
    "                    node_data_df.unpersist()\n",
    "                    continue\n",
    "\n",
    "                # Calculate parent Gini impurity\n",
    "                parent_label_counts = node_data_df.groupBy(\"true_label\").count().collect()\n",
    "                parent_gini = self._calculate_gini_impurity(parent_label_counts, node_total_samples)\n",
    "                print(f\"DEBUG: Node {node_id} parent Gini: {parent_gini}\")\n",
    "\n",
    "                best_gini_gain = -1.0\n",
    "                best_split_info = None # (distance_measure, {branch_id: exemplar_ts})\n",
    "\n",
    "                # Generate and evaluate candidate splits\n",
    "                for i in range(self.num_candidate_splits):\n",
    "                    print(f\"DEBUG: Evaluating candidate split {i+1} for node {node_id}.\")\n",
    "                    # Sample a distance measure and parameters (simplified: using Euclidean)\n",
    "                    # In a full implementation, sample from the 11 measures and their params\n",
    "                    distance_measure_type = \"euclidean\" # Placeholder\n",
    "                    # Sample exemplars for this candidate split (one per class present in node_data_df)\n",
    "                    # Need to get unique labels in node_data_df first\n",
    "                    unique_labels_in_node = [row['true_label'] for row in node_data_df.select(\"true_label\").distinct().collect()]\n",
    "                    candidate_exemplars = {}\n",
    "                    for label in unique_labels_in_node:\n",
    "                        if label in sampled_exemplars[node_id] and sampled_exemplars[node_id][label]:\n",
    "                            # Pick one random exemplar for this label from the sampled pool for this node\n",
    "                            candidate_exemplars[label] = random.choice(sampled_exemplars[node_id][label])\n",
    "                        else:\n",
    "                            # Should not happen if sampling was done correctly and node_data_df has this label\n",
    "                            print(f\"WARNING: No sampled exemplar in pool for label {label} in node {node_id}. Skipping candidate split.\")\n",
    "                            candidate_exemplars = None # Invalidate this candidate\n",
    "                            break\n",
    "\n",
    "                    if candidate_exemplars is None or len(candidate_exemplars) < 2:\n",
    "                         print(f\"DEBUG: Candidate split {i+1} for node {node_id} has less than 2 exemplars, skipping.\")\n",
    "                         continue # Need at least two branches\n",
    "\n",
    "                    print(f\"DEBUG: Candidate split {i+1} exemplars (labels): {list(candidate_exemplars.keys())}\")\n",
    "\n",
    "                    # --- Modified Gini Calculation: Use RDD transformations to get counts directly ---\n",
    "                    bc_candidate_exemplars = self.spark.sparkContext.broadcast(candidate_exemplars)\n",
    "\n",
    "                    def map_to_branch_label_pair(row):\n",
    "                        exemplars = bc_candidate_exemplars.value\n",
    "                        min_dist = float('inf')\n",
    "                        assigned_branch_id = None # The label of the nearest exemplar\n",
    "\n",
    "                        for ex_lbl, ex_ts in exemplars.items():\n",
    "                            # Use the chosen distance measure (placeholder: euclidean)\n",
    "                            d = euclidean_distance(row.time_series, ex_ts) # Use the distance function\n",
    "                            if d < min_dist:\n",
    "                                min_dist = d\n",
    "                                assigned_branch_id = ex_lbl\n",
    "\n",
    "                        # Return a tuple of (assigned_branch_id, true_label) for counting\n",
    "                        return (assigned_branch_id, row.true_label)\n",
    "\n",
    "                    # Apply the map and countByValue to get the counts per (branch, label) pair\n",
    "                    print(f\"DEBUG: Calculating branch-label counts for candidate split {i+1} using RDD.\")\n",
    "                    # countByValue returns a dictionary {(branch_id, true_label): count}\n",
    "                    branch_label_counts_dict = node_data_df.rdd.map(map_to_branch_label_pair).countByValue()\n",
    "                    print(f\"DEBUG: Branch label counts dictionary collected for candidate split {i+1}: {branch_label_counts_dict}\")\n",
    "\n",
    "                    # Convert the dictionary to the list format expected by _calculate_gini_impurity\n",
    "                    # branch_label_counts = [{\"assigned_branch_id\": k[0], \"true_label\": k[1], \"count\": v} for k, v in branch_label_counts_dict.items()] # Not needed in this format anymore\n",
    "\n",
    "\n",
    "                    # Calculate weighted impurity for this split\n",
    "                    weighted_impurity = 0.0\n",
    "                    total_samples_in_split = node_total_samples # Total samples in the node\n",
    "\n",
    "                    # Group counts by branch_id to calculate branch impurity directly from the dictionary\n",
    "                    branch_counts = {}\n",
    "                    for (branch_id, true_label), count in branch_label_counts_dict.items():\n",
    "                        if branch_id not in branch_counts:\n",
    "                            branch_counts[branch_id] = []\n",
    "                        branch_counts[branch_id].append((true_label, count))\n",
    "\n",
    "                    print(f\"DEBUG: Branch counts grouped for candidate split {i+1}: {branch_counts}\")\n",
    "\n",
    "                    for branch_id, label_counts_list in branch_counts.items():\n",
    "                        branch_total = sum(count for label, count in label_counts_list)\n",
    "                        if branch_total > 0:\n",
    "                             branch_impurity = self._calculate_gini_impurity(label_counts_list, branch_total)\n",
    "                             weighted_impurity += (branch_total / total_samples_in_split) * branch_impurity\n",
    "                             print(f\"DEBUG: Branch {branch_id} impurity: {branch_impurity}, weighted: {(branch_total / total_samples_in_split) * branch_impurity}\")\n",
    "\n",
    "\n",
    "                    gini_gain = parent_gini - weighted_impurity\n",
    "                    print(f\"DEBUG: Candidate split {i+1} Gini gain: {gini_gain}\")\n",
    "\n",
    "                    # Unpersist the broadcast variable\n",
    "                    bc_candidate_exemplars.unpersist()\n",
    "                    # --- End Modified Gini Calculation ---\n",
    "\n",
    "\n",
    "                    # Check if this is the best split so far\n",
    "                    if gini_gain > best_gini_gain:\n",
    "                        best_gini_gain = gini_gain\n",
    "                        best_split_info = (distance_measure_type, candidate_exemplars)\n",
    "                        print(f\"DEBUG: Candidate split {i+1} is the best so far for node {node_id} with gain {best_gini_gain}.\")\n",
    "\n",
    "\n",
    "                node_data_df.unpersist() # Unpersist node data\n",
    "\n",
    "                # Decide if the node should split\n",
    "                # A split occurs if best_gini_gain is positive and results in valid children (handled in _split_node_gini)\n",
    "                if best_gini_gain > 0:\n",
    "                    print(f\"DEBUG: Node {node_id} has a positive Gini gain ({best_gini_gain}), attempting to split.\")\n",
    "                    best_splits[node_id] = (best_gini_gain, best_split_info[0], best_split_info[1])\n",
    "                else:\n",
    "                    print(f\"DEBUG: Node {node_id} has non-positive Gini gain ({best_gini_gain}), making it a leaf.\")\n",
    "                    nodes_to_make_leaves_this_iter.add(node_id)\n",
    "\n",
    "\n",
    "            # --- FIX: Finalize nodes marked as leaves *in this iteration* ---\n",
    "            # This loop should be here, inside the depth loop, after evaluating all open_nodes\n",
    "            for node_id in nodes_to_make_leaves_this_iter:\n",
    "                 if node_id in self.tree and not self.tree[node_id].is_leaf:\n",
    "                     print(f\"DEBUG: Finalizing node {node_id} as a leaf.\")\n",
    "                     # Need to calculate the prediction for this leaf node\n",
    "                     # Collect label counts for this node from assign_df\n",
    "                     leaf_data_df = assign_df.filter(F.col(\"node_id\") == node_id).cache()\n",
    "                     leaf_label_counts = leaf_data_df.groupBy(\"true_label\").count().collect()\n",
    "                     leaf_data_df.unpersist()\n",
    "\n",
    "                     leaf_prediction = None\n",
    "                     if leaf_label_counts:\n",
    "                         leaf_prediction = max(leaf_label_counts, key=lambda x: x['count'])['true_label']\n",
    "                     elif self._overall_majority_class is not None:\n",
    "                         # Fallback to overall majority if no data at this node (shouldn't happen with correct logic)\n",
    "                         leaf_prediction = self._overall_majority_class\n",
    "                         print(f\"DEBUG: Node {node_id} had no data, using overall majority prediction: {leaf_prediction}\")\n",
    "                     else:\n",
    "                          # Final fallback if no data and no overall majority\n",
    "                          leaf_prediction = 1 # Defaulting to 1\n",
    "\n",
    "                     self.tree[node_id] = self.tree[node_id]._replace(is_leaf=True, prediction=leaf_prediction)\n",
    "                     print(f\"DEBUG: Node {node_id} marked as leaf with prediction {leaf_prediction}.\")\n",
    "            # --- End FIX ---\n",
    "\n",
    "\n",
    "            # 3. Perform the best splits and update the tree structure (on driver)\n",
    "            # and push rows down to the new child nodes (distributed)\n",
    "            next_open = set() # Nodes that successfully split and will be processed in the next iteration\n",
    "            if best_splits:\n",
    "                print(\"DEBUG: Performing best splits and pushing rows down.\")\n",
    "                # Create a mapping from (parent_node_id, assigned_branch_id) to new_child_node_id\n",
    "                split_mapping = {} # {(parent_id, assigned_branch_id): child_node_id}\n",
    "\n",
    "                for parent_id, (gain, measure, exemplars) in best_splits.items():\n",
    "                    print(f\"DEBUG: Processing best split for parent node {parent_id}.\")\n",
    "                    # Update the tree structure on the driver with the chosen split info\n",
    "                    self.tree[parent_id] = self.tree[parent_id]._replace(split_on=(measure, exemplars))\n",
    "                    print(f\"DEBUG: Node {parent_id} split_on updated: measure={measure}, exemplars={list(exemplars.keys())}.\")\n",
    "\n",
    "                    # --- FIX: Mark the parent node as INTERNAL ---\n",
    "                    # Only mark as INTERNAL if it successfully splits and creates children\n",
    "                    # This is determined by checking branch counts against min_samples below.\n",
    "                    # We'll set is_leaf=False and prediction=None here tentatively,\n",
    "                    # and confirm it becomes internal if children are created.\n",
    "                    self.tree[parent_id] = self.tree[parent_id]._replace(is_leaf=False, prediction=None)\n",
    "                    print(f\"DEBUG: Node {parent_id} tentatively marked as INTERNAL.\")\n",
    "\n",
    "\n",
    "                    # Recalculate branch counts for the best split to check min_samples\n",
    "                    node_data_for_split_df = assign_df.filter(F.col(\"node_id\") == parent_id).cache()\n",
    "                    bc_chosen_exemplars_for_counts = self.spark.sparkContext.broadcast(exemplars)\n",
    "\n",
    "                    def map_to_branch_for_counts(row):\n",
    "                        exemplars = bc_chosen_exemplars_for_counts.value\n",
    "                        min_dist = float('inf')\n",
    "                        assigned_branch_id = None\n",
    "\n",
    "                        for ex_lbl, ex_ts in exemplars.items():\n",
    "                            d = euclidean_distance(row.time_series, ex_ts)\n",
    "                            if d < min_dist:\n",
    "                                min_dist = d\n",
    "                                assigned_branch_id = ex_lbl\n",
    "                        return assigned_branch_id # Return only the assigned branch ID\n",
    "\n",
    "                    # Count samples per assigned branch for the best split\n",
    "                    branch_counts_for_children = node_data_for_split_df.rdd.map(map_to_branch_for_counts).countByValue()\n",
    "                    print(f\"DEBUG: Branch counts for creating children for node {parent_id}: {branch_counts_for_children}\")\n",
    "\n",
    "                    node_data_for_split_df.unpersist()\n",
    "                    bc_chosen_exemplars_for_counts.unpersist()\n",
    "\n",
    "                    children_created_for_node = False\n",
    "                    for branch_id, count in branch_counts_for_children.items():\n",
    "                         # Only create a child node if the branch has enough samples\n",
    "                         if count >= self.min_samples:\n",
    "                             child_id = self._next_node_id\n",
    "                             self._next_node_id += 1\n",
    "                             print(f\"DEBUG: Creating child node {child_id} for branch {branch_id} of parent {parent_id}.\")\n",
    "                             self.tree[child_id] = self.TreeNode(\n",
    "                                 node_id=child_id,\n",
    "                                 parent_id=parent_id,\n",
    "                                 split_on=None, # Split info will be determined in a future iteration if not a leaf\n",
    "                                 is_leaf=False, # Initially internal, will be finalized later\n",
    "                                 prediction=None,\n",
    "                                 children={},\n",
    "                             )\n",
    "                             # Update the parent node's children dictionary on the driver\n",
    "                             self.tree[parent_id].children[branch_id] = child_id\n",
    "                             # Add to the mapping used for pushing rows\n",
    "                             split_mapping[(parent_id, branch_id)] = child_id\n",
    "                             # Add the new child node to the set of nodes to process in the next iteration\n",
    "                             next_open.add(child_id)\n",
    "                             children_created_for_node = True\n",
    "                             print(f\"DEBUG: Added child {child_id} to parent {parent_id} children for branch {branch_id}.\")\n",
    "                         else:\n",
    "                             print(f\"DEBUG: Branch {branch_id} for node {parent_id} has {count} samples, below min_samples. Not creating child node.\")\n",
    "                             # Data points assigned to branches that don't create a child node\n",
    "                             # will remain at the parent_id in the next assignment_df.\n",
    "                             # This is a simplification; ideally, they might be handled differently\n",
    "                             # (e.g., contribute to the parent's prediction if it becomes a leaf).\n",
    "\n",
    "                    # If no children were created for this node despite a positive Gini gain\n",
    "                    # (e.g., all branches had < min_samples), mark it as a leaf.\n",
    "                    if not children_created_for_node:\n",
    "                         print(f\"DEBUG: Node {parent_id} had positive Gini gain but no branches met min_samples. Finalizing as a leaf.\")\n",
    "                         # Recalculate prediction based on data at this node\n",
    "                         leaf_data_df = assign_df.filter(F.col(\"node_id\") == parent_id).cache()\n",
    "                         leaf_label_counts = leaf_data_df.groupBy(\"true_label\").count().collect()\n",
    "                         leaf_data_df.unpersist()\n",
    "\n",
    "                         leaf_prediction = None\n",
    "                         if leaf_label_counts:\n",
    "                             leaf_prediction = max(leaf_label_counts, key=lambda x: x['count'])['true_label']\n",
    "                         elif self._overall_majority_class is not None:\n",
    "                             leaf_prediction = self._overall_majority_class\n",
    "                         else:\n",
    "                             leaf_prediction = 1 # Default\n",
    "\n",
    "                         self.tree[parent_id] = self.tree[parent_id]._replace(is_leaf=True, prediction=leaf_prediction, children={}) # Clear children if no split occurred\n",
    "                         print(f\"DEBUG: Node {parent_id} marked as leaf with prediction {leaf_prediction}.\")\n",
    "\n",
    "\n",
    "                # --- Modified Pushing Rows Down: Use a single UDF ---\n",
    "                # Only apply the push down UDF if any children were actually created in this iteration\n",
    "                if split_mapping: # split_mapping will be non-empty if any children were created\n",
    "                    print(\"DEBUG: Applying single UDF to push rows down.\")\n",
    "                    bc_split_mapping = self.spark.sparkContext.broadcast(split_mapping)\n",
    "                    bc_best_splits_info = self.spark.sparkContext.broadcast({nid: (split_info[0], split_info[2]) for nid, split_info in best_splits.items()}) # Broadcast (measure, exemplars) for splitting nodes\n",
    "\n",
    "                    def push_row_udf_func(split_mapping_broadcast, best_splits_info_broadcast):\n",
    "                        mapping = split_mapping_broadcast.value\n",
    "                        splits_info = best_splits_info_broadcast.value\n",
    "\n",
    "                        def _push_row(row_id, current_node_id, time_series, true_label):\n",
    "                            # If the current node is one of the nodes that split in this iteration\n",
    "                            if current_node_id in splits_info:\n",
    "                                measure_type, exemplars = splits_info[current_node_id]\n",
    "\n",
    "                                # Calculate distance to exemplars for this node's split\n",
    "                                min_dist = float('inf')\n",
    "                                assigned_branch_id = None\n",
    "\n",
    "                                for ex_lbl, ex_ts in exemplars.items():\n",
    "                                    # Use the chosen distance measure (placeholder: euclidean)\n",
    "                                    d = euclidean_distance(time_series, ex_ts)\n",
    "                                    if d < min_dist:\n",
    "                                        min_dist = d\n",
    "                                        assigned_branch_id = ex_lbl\n",
    "\n",
    "                                # Use the split mapping to find the new node ID\n",
    "                                key = (current_node_id, assigned_branch_id)\n",
    "                                # If there's a mapping for this parent/branch, return the child node ID\n",
    "                                # Otherwise, keep the old node ID (this handles branches that didn't create children)\n",
    "                                return mapping.get(key, current_node_id)\n",
    "                            else:\n",
    "                                # If the current node was not one of the nodes that split,\n",
    "                                # the row stays at its current node ID.\n",
    "                                return current_node_id\n",
    "\n",
    "                        # Return the UDF itself\n",
    "                        return F.udf(_push_row, IntegerType())\n",
    "\n",
    "                    # Create an instance of the UDF\n",
    "                    push_row_udf = push_row_udf_func(bc_split_mapping, bc_best_splits_info)\n",
    "\n",
    "                    # Apply the UDF to the entire assign_df to get the new node_id for each row\n",
    "                    old_assign_df = assign_df # Keep reference to unpersist later\n",
    "                    assign_df = assign_df.withColumn(\n",
    "                        \"node_id\", # Overwrite the node_id column\n",
    "                        push_row_udf(F.col(\"row_id\"), F.col(\"node_id\"), F.col(\"time_series\"), F.col(\"true_label\"))\n",
    "                    ).cache() # Cache the updated DataFrame\n",
    "                    print(f\"DEBUG: assign_df updated for depth {depth+1}. Total rows: {assign_df.count()}\")\n",
    "\n",
    "                    # Unpersist intermediate DataFrames and broadcast variables\n",
    "                    old_assign_df.unpersist()\n",
    "                    bc_split_mapping.unpersist()\n",
    "                    bc_best_splits_info.unpersist()\n",
    "                else:\n",
    "                    print(\"DEBUG: No children created in this iteration. No push down needed. assign_df remains unchanged for relevant nodes.\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"DEBUG: No nodes split in this iteration. assign_df remains unchanged.\")\n",
    "\n",
    "\n",
    "            # Unpersist data for the current level\n",
    "            current_level_df.unpersist()\n",
    "\n",
    "            # Update open_nodes for the next iteration\n",
    "            # Only include nodes that were successfully split into internal nodes\n",
    "            open_nodes = next_open\n",
    "            print(f\"DEBUG: open_nodes for next level: {open_nodes}\")\n",
    "\n",
    "\n",
    "        # --- REMOVE: Incorrect Final Leaf Finalization ---\n",
    "        # This loop incorrectly finalizes nodes that were already correctly marked as INTERNAL.\n",
    "        # Removing this loop relies on the logic within the depth loop to correctly finalize leaves.\n",
    "        # print(\"DEBUG: Finalizing any internal nodes that were not explicitly finalized.\")\n",
    "        # all_node_ids = list(self.tree.keys())\n",
    "        # for node_id in all_node_ids:\n",
    "        #      if node_id in self.tree and not self.tree[node_id].is_leaf:\n",
    "        #          print(f\"DEBUG: Finalizing node {node_id} as a leaf.\")\n",
    "        #          # Need to calculate the prediction for this leaf node\n",
    "        #          # Collect label counts for this node from assign_df\n",
    "        #          # Note: Data for these nodes is still in assign_df with their node_id\n",
    "        #          leaf_data_df = assign_df.filter(F.col(\"node_id\") == node_id).cache()\n",
    "        #          leaf_label_counts = leaf_data_df.groupBy(\"true_label\").count().collect()\n",
    "        #          leaf_data_df.unpersist()\n",
    "        #\n",
    "        #          leaf_prediction = None\n",
    "        #          if leaf_label_counts:\n",
    "        #              leaf_prediction = max(leaf_label_counts, key=lambda x: x['count'])['true_label']\n",
    "        #          elif self._overall_majority_class is not None:\n",
    "        #              # Fallback to overall majority if no data at this node (shouldn't happen with correct logic)\n",
    "        #              leaf_prediction = self._overall_majority_class\n",
    "        #              print(f\"DEBUG: Node {node_id} had no data, using overall majority prediction: {leaf_prediction}\")\n",
    "        #          else:\n",
    "        #               # Final fallback if no data and no overall majority\n",
    "        #               leaf_prediction = 1 # Defaulting to 1\n",
    "        #\n",
    "        #          self.tree[node_id] = self.tree[node_id]._replace(is_leaf=True, prediction=leaf_prediction)\n",
    "        #          print(f\"DEBUG: Node {node_id} marked as leaf with prediction {leaf_prediction}.\")\n",
    "        # --- End REMOVE ---\n",
    "\n",
    "\n",
    "        assign_df.unpersist() # Unpersist the final assignment DataFrame\n",
    "        print(\"DEBUG: fit finished.\")\n",
    "        return self\n",
    "\n",
    "    def _calculate_gini_impurity(self, label_counts, total_samples):\n",
    "        \"\"\"\n",
    "        Calculates Gini impurity.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        label_counts : list of (label, count) tuples or dict {label: count}\n",
    "            Counts of each label in the dataset or branch.\n",
    "        total_samples : int\n",
    "            Total number of samples.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float : Gini impurity\n",
    "        \"\"\"\n",
    "        if total_samples == 0:\n",
    "            return 0.0\n",
    "\n",
    "        impurity = 1.0\n",
    "        # Ensure label_counts is treated as a dictionary-like structure\n",
    "        if isinstance(label_counts, list):\n",
    "            counts_dict = dict(label_counts)\n",
    "        else:\n",
    "            counts_dict = label_counts\n",
    "\n",
    "        for label, count in counts_dict.items():\n",
    "            probability_of_label = count / total_samples\n",
    "            impurity -= probability_of_label ** 2\n",
    "\n",
    "        return impurity\n",
    "\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained tree\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : Spark DataFrame\n",
    "            DataFrame with feature columns or 'time_series' column\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Spark DataFrame : DataFrame with predictions\n",
    "        \"\"\"\n",
    "        print(\"DEBUG: predict started.\")\n",
    "        # First, convert to time_series format if needed and add row_id\n",
    "        df = self._convert_to_time_series_format(df)\n",
    "\n",
    "        # --- Convert tree structure to a plain dictionary for broadcasting ---\n",
    "        print(\"DEBUG: Converting tree structure to plain dictionary for broadcasting.\")\n",
    "        plain_tree_structure = {}\n",
    "        for node_id, node in self.tree.items():\n",
    "            plain_tree_structure[node_id] = {\n",
    "                'node_id': node.node_id,\n",
    "                'parent_id': node.parent_id,\n",
    "                # Ensure split_on is also a plain structure (e.g., tuple of string and dict)\n",
    "                'split_on': node.split_on,\n",
    "                'is_leaf': node.is_leaf,\n",
    "                'prediction': node.prediction,\n",
    "                # Children dictionary keys (branch_id) and values (child_node_id) are already plain types\n",
    "                'children': node.children\n",
    "            }\n",
    "\n",
    "        # Broadcast the plain tree structure\n",
    "        print(\"DEBUG: Broadcasting plain tree structure for prediction.\")\n",
    "        plain_tree_structure_broadcast = self.spark.sparkContext.broadcast(plain_tree_structure)\n",
    "\n",
    "        # Create the prediction UDF using the broadcasted plain tree\n",
    "        # Pass the broadcast variable to the function that defines the UDF\n",
    "        prediction_udf = F.udf(predict_udf_func(plain_tree_structure_broadcast), IntegerType())\n",
    "\n",
    "        # Apply the prediction UDF to each row\n",
    "        predictions_df = df.withColumn(\"prediction\", prediction_udf(F.col(\"time_series\")))\n",
    "\n",
    "        # Unpersist the broadcast variable after the prediction is done\n",
    "        # Note: Spark manages broadcast lifecycle, but explicit unpersist is good practice\n",
    "        # in interactive sessions or when memory is tight.\n",
    "        plain_tree_structure_broadcast.unpersist()\n",
    "        print(\"DEBUG: Plain tree structure unbroadcasted.\")\n",
    "\n",
    "        print(\"DEBUG: predict finished.\")\n",
    "        # Select the original columns plus the new prediction column\n",
    "        return predictions_df.select(\"row_id\", \"time_series\", \"true_label\", \"prediction\")\n",
    "\n",
    "\n",
    "    def print_tree(self):\n",
    "        \"\"\"\n",
    "        Print a representation of the tree (driver-side).\n",
    "        Adjusted to show children even if node is marked as leaf,\n",
    "        to better reflect the structure built.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str : String representation of the tree\n",
    "        \"\"\"\n",
    "        print(\"DEBUG: print_tree started.\")\n",
    "        def print_node(node_id, depth=0):\n",
    "            if node_id not in self.tree:\n",
    "                 return f\"{'  ' * depth}Node {node_id}: Does Not Exist\\n\"\n",
    "\n",
    "            node = self.tree[node_id]\n",
    "            indent = \"  \" * depth\n",
    "\n",
    "            # Format split_on info nicely\n",
    "            split_info_str = \"None\"\n",
    "            if node.split_on:\n",
    "                measure_type, exemplars = node.split_on\n",
    "                # Print exemplar time series for small trees\n",
    "                exemplar_details = {lbl: ts for lbl, ts in exemplars.items()}\n",
    "                split_info_str = f\"measure={measure_type}, exemplars={exemplar_details}\"\n",
    "\n",
    "\n",
    "            # Print node info including leaf status, prediction, parent, and depth\n",
    "            result = f\"{indent}Node {node_id} (Depth {depth}, Parent: {node.parent_id}): {'LEAF' if node.is_leaf else 'INTERNAL'}, prediction={node.prediction}, split_on=[{split_info_str}]\\n\"\n",
    "\n",
    "            # Recursively print children if they exist, regardless of is_leaf flag\n",
    "            if node.children:\n",
    "                 result += f\"{indent}  Children:\\n\"\n",
    "                 for branch_id, child_id in sorted(node.children.items()):\n",
    "                     result += f\"{indent}    Branch {branch_id} -> Child {child_id}\\n\"\n",
    "                     # Only recurse if the child node exists in the tree\n",
    "                     if child_id in self.tree:\n",
    "                         result += print_node(child_id, depth + 1) # Increase depth for child nodes\n",
    "                     else:\n",
    "                         result += f\"{indent}      Node {child_id}: Does Not Exist\\n\"\n",
    "\n",
    "\n",
    "            return result\n",
    "\n",
    "        tree_str = print_node(0)  # Start at root at depth 0\n",
    "        print(\"DEBUG: print_tree finished.\")\n",
    "        return tree_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5108ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSplit_stratified_via_sampleBy(df, label_col, weights=[0.8, 0.2], seed=123):\n",
    "    \n",
    "    \"\"\"\n",
    "    Splits a Spark DataFrame into train/test sets based on partition-Preserves per‑class proportions\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    assert abs(sum(weights) - 1.0) < 1e-6 # ensure that our weights must sum to 1.0\n",
    "    train_frac = weights[0]\n",
    "\n",
    "    # figure out all the distinct label values \n",
    "    labels = [row[label_col] for row in df.select(label_col) \n",
    "                                            .distinct()  # build a tiny DataFrame of unique labels\n",
    "                                            .collect() # brings the list to the driver\n",
    "                                            ]\n",
    "\n",
    "    # build a dict: each label -> same fraction\n",
    "    fractions = {dict_lbl: train_frac for dict_lbl in labels}\n",
    "\n",
    "    # sample train set: Use Spark’s native stratified sampler\n",
    "    train_df = df.stat.sampleBy(label_col, fractions, seed) # map‑side sampling per key, jno shuffle\n",
    "    # everything else is test\n",
    "    test_df  = df.join(train_df, on=df.columns, how=\"left_anti\") # one shuffles to get the rest of the data\n",
    "    return train_df, test_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ed50976",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"testingglobal\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0568d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: SparkContext accessed.\n",
      "Dummy Training DataFrame:\n",
      "+--------+--------+-----+\n",
      "|feature1|feature2|label|\n",
      "+--------+--------+-----+\n",
      "|     1.0|     1.1|    1|\n",
      "|     1.2|     1.3|    1|\n",
      "|     0.8|     0.9|    1|\n",
      "|     5.0|     5.1|    2|\n",
      "|     5.2|     5.3|    2|\n",
      "|     4.8|     4.9|    2|\n",
      "|     2.5|     2.6|    1|\n",
      "|     3.5|     3.6|    2|\n",
      "|     4.8|     4.9|    2|\n",
      "|     7.8|     5.9|    3|\n",
      "|     6.8|     5.9|    3|\n",
      "|     7.8|     5.9|    3|\n",
      "|     5.8|     5.9|    3|\n",
      "+--------+--------+-----+\n",
      "\n",
      "DataFrame shape: 3 columns, 13 rows\n",
      "\n",
      "Fitting tree on dummy DataFrame...\n",
      "Error while fitting tree: \n",
      "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\socket.py\", line 721, in readinto\n",
      "    raise\n",
      "TimeoutError: timed out\n",
      "\n",
      "\n",
      "Tree fitting complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Petru\\AppData\\Local\\Temp\\ipykernel_23640\\1699502173.py\", line 75, in <module>\n",
      "    tree.fit(dummy_train_df)\n",
      "  File \"C:\\Users\\Petru\\AppData\\Local\\Temp\\ipykernel_23640\\3617201036.py\", line 297, in fit\n",
      "    if current_level_df.count() == 0:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 1240, in count\n",
      "    return int(self._jdf.count())\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.PythonException: \n",
      "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\socket.py\", line 721, in readinto\n",
      "    raise\n",
      "TimeoutError: timed out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, ArrayType\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import json\n",
    "import traceback # Import traceback to print error details\n",
    "\n",
    "# Assume the GlobalProxTree class definition is available in the environment\n",
    "# (e.g., defined in a previous cell or imported from a file)\n",
    "\n",
    "# Create a SparkSession (replace with your actual SparkSession if running in a cluster)\n",
    "# If running in a notebook like environment, spark might already be defined.\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.appName(\"GlobalProxTreeTest\").getOrCreate()\n",
    "    print(\"DEBUG: SparkSession created.\")\n",
    "\n",
    "# --- FIX: Explicitly access SparkContext to help ensure initialization ---\n",
    "# This line can sometimes help resolve issues with SparkContext not being available\n",
    "spark.sparkContext\n",
    "print(\"DEBUG: SparkContext accessed.\")\n",
    "\n",
    "\n",
    "# --- Create a small, dummy DataFrame for testing ---\n",
    "# This data simulates time series with 2 features and 2 classes (1 and 2)\n",
    "# Designed to potentially create a simple split at the root\n",
    "dummy_data = [\n",
    "    (1.0, 1.1, 1), # Class 1\n",
    "    (1.2, 1.3, 1), # Class 1\n",
    "    (0.8, 0.9, 1), # Class 1\n",
    "    (5.0, 5.1, 2), # Class 2\n",
    "    (5.2, 5.3, 2), # Class 2\n",
    "    (4.8, 4.9, 2), # Class 2\n",
    "    (2.5, 2.6, 1), # Class 1 (closer to Class 1 exemplars)\n",
    "    (3.5, 3.6, 2), # Class 2 (closer to Class 2 exemplars)\n",
    "    (4.8, 4.9, 2), # Class 2\n",
    "    (7.8, 5.9, 3), # Class 3\n",
    "    (6.8, 5.9, 3), # Class 3\n",
    "    (7.8, 5.9, 3), # Class 3\n",
    "    (5.8, 5.9, 3), # Class 3\n",
    "]\n",
    "\n",
    "# Define schema for the dummy data\n",
    "dummy_schema = StructType([\n",
    "    StructField(\"feature1\", DoubleType(), True),\n",
    "    StructField(\"feature2\", DoubleType(), True),\n",
    "    StructField(\"label\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create the dummy DataFrame\n",
    "# This is where the error occurred previously\n",
    "dummy_train_df = spark.createDataFrame(dummy_data, dummy_schema)\n",
    "\n",
    "print(\"Dummy Training DataFrame:\")\n",
    "dummy_train_df.show()\n",
    "print(f\"DataFrame shape: {len(dummy_train_df.columns)} columns, {dummy_train_df.count()} rows\")\n",
    "\n",
    "\n",
    "# --- Test the GlobalProxTree class ---\n",
    "\n",
    "# First, create the tree with the desired parameters\n",
    "# Using small max_depth and min_samples for a shallow tree\n",
    "# num_candidate_splits=3 to see evaluation process\n",
    "# num_exemplars_per_class=1 as in the paper's conceptual split\n",
    "tree = GlobalProxTree(spark, max_depth=3, min_samples=2, num_candidate_splits=3, num_exemplars_per_class=1)\n",
    "\n",
    "\n",
    "# Now we can directly fit the tree on the wide DataFrame\n",
    "# The conversion will happen automatically inside the fit method\n",
    "try:\n",
    "    print(\"\\nFitting tree on dummy DataFrame...\")\n",
    "    tree.fit(dummy_train_df)\n",
    "\n",
    "    print(\"\\nTree structure:\")\n",
    "    # Use the corrected print_tree method to see the full structure\n",
    "    print(tree.print_tree())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while fitting tree: {e}\")\n",
    "    # Print the full traceback for detailed debugging\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nTree fitting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39055d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFitting tree on dummy DataFrame...\")\n",
    "tree.fit(dummy_train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5820de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Prediction...\n",
      "DEBUG: predict started.\n",
      "DEBUG: _convert_to_time_series_format started.\n",
      "DEBUG: Converting 2 feature columns to 'time_series' array.\n",
      "DEBUG: Sample of converted DataFrame:\n",
      "+-----------+-----------+----------+\n",
      "|row_id     |time_series|true_label|\n",
      "+-----------+-----------+----------+\n",
      "|8589934592 |[1.1, 1.2] |1         |\n",
      "|25769803776|[0.1, 1.5] |1         |\n",
      "+-----------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "DEBUG: _convert_to_time_series_format finished.\n",
      "DEBUG: Converting tree structure to plain dictionary for broadcasting.\n",
      "DEBUG: Broadcasting plain tree structure for prediction.\n",
      "DEBUG: Plain tree structure unbroadcasted.\n",
      "DEBUG: predict finished.\n",
      "\n",
      "Sample Predictions vs. True Labels:\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  2.0|       1.0|\n",
      "|  2.0|       1.0|\n",
      "|  2.0|       2.0|\n",
      "|  1.0|       2.0|\n",
      "|  2.0|       2.0|\n",
      "|  3.0|       3.0|\n",
      "|  2.0|       3.0|\n",
      "+-----+----------+\n",
      "\n",
      "Prediction Accuracy = 0.556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Prediction (uncomment and run after successful fitting)\n",
    "dummy_test_data = [\n",
    "    (1.1, 1.2, 1), # Should predict 1\n",
    "    (0.1, 1.5, 1), # Should predict 1\n",
    "    (1.9, 1.5, 2), # Should predict 1\n",
    "    (1.0, 1.6, 2), # Should predict 1\n",
    "    (5.3, 5.4, 2), # Closer to class 2 exemplars\n",
    "    (3.0, 3.1, 1), # Closer to class 2 exemplars\n",
    "    (4.0, 4.1, 2), # Closer to class 2 exemplars\n",
    "    (7.8, 5.9, 3), # shoudl predict class 3\n",
    "    (8.8, 5.9, 2), # shoudl predict class 3\n",
    "]\n",
    "dummy_test_df = spark.createDataFrame(dummy_test_data, dummy_schema)\n",
    "\n",
    "print(\"\\nTesting Prediction...\")\n",
    "try:\n",
    "    pred_df = tree.predict(dummy_test_df)\n",
    "\n",
    "    # Rename true_label to label for evaluator compatibility\n",
    "    pred_df = pred_df.withColumnRenamed(\"true_label\", \"label\")\n",
    "\n",
    "    # Cast both label and prediction to Double for evaluator\n",
    "    pred_df = (\n",
    "        pred_df\n",
    "        .withColumn(\"prediction\", F.col(\"prediction\").cast(DoubleType()))\n",
    "        .withColumn(\"label\",      F.col(\"label\")      .cast(DoubleType()))\n",
    "    )\n",
    "\n",
    "    print(\"\\nSample Predictions vs. True Labels:\")\n",
    "    pred_df.select(\"label\", \"prediction\").show(10)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    acc_evaluator = evaluator.evaluate(pred_df)\n",
    "    print(f\"Prediction Accuracy = {acc_evaluator:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09351f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: If you created the SparkSession here, you might want to stop it\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
