{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be988663",
   "metadata": {},
   "source": [
    "## Import Statements and Dependencies\n",
    "\n",
    "This cell imports all the necessary modules and libraries required for the pipeline.\n",
    "These imports set up the environment for data ingestion, preprocessing, model training, prediction, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bea9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame, Row, SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    ")\n",
    "\n",
    "from aeon.classification.distance_based import (\n",
    "    ProximityForest,\n",
    "    ProximityTree,\n",
    ")\n",
    "\n",
    "from data_ingestion import DataIngestion\n",
    "from evaluation import Evaluator\n",
    "\n",
    "from utilities import (\n",
    "    compute_min_max,\n",
    "    randomSplit_dist,\n",
    "    randomSplit_stratified_via_sampleBy,\n",
    "    show_compact,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e2dc1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The `Preprocessor` class prepares raw ECG time series data for parallel training. It performs three primary tasks:\n",
    "\n",
    "1. **Missing Value Handling**  \n",
    "   Drops rows where all columns are null, ensuring clean data for modelling.\n",
    "\n",
    "2. **Repartitioning**  \n",
    "   - **Balanced (Local Model)**: Uses stratified sampling to maintain class distributions across partitions. Partition IDs are assigned using a randomised window function and used to repartition the dataset.\n",
    "   - **Unbalanced (Global Model)**: Applies random repartitioning for efficiency without enforcing label stratification.\n",
    "\n",
    "3. **Normalization**  \n",
    "   Applies min-max scaling to feature columns using precomputed global min-max values. This scales each feature to the [0,1] range, which is essential for distance-based classifiers like Proximity Trees.\n",
    "\n",
    "The full preprocessing pipeline is executed through the `run_preprocessing()` method, which dynamically selects the appropriate partitioning strategy based on the configuration. It also supports preservation of partition IDs for downstream tasks when required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d4e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    This class cleans up our ECG data.\n",
    "    It handles missing rows, splits the label from the features, and does a simple normalization on the feature columns.\n",
    "    It returns a Spark DataFrame ready for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "\n",
    "    def handle_missing_values(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\" Drops rows where every column is null \"\"\"\n",
    "        return df.dropna(how=\"all\")\n",
    "   \n",
    "    \n",
    "    def normalize(self, df: DataFrame, min_max: dict, preserve_partition_id: bool = False) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Normalizes feature columns using precomputed global min and max values.\n",
    "        \"\"\"\n",
    "        feature_cols = [col for col in df.columns if (col != \"label\" and col != \"_partition_id\")]\n",
    "        \n",
    "        normalized_cols = []\n",
    "        for col in feature_cols:\n",
    "            min_val, max_val = min_max[col]\n",
    "            if max_val != min_val:\n",
    "                expr = (F.col(col) - F.lit(min_val)) / (F.lit(max_val - min_val))\n",
    "            else:\n",
    "                expr = F.lit(0.0)  # if all values identical, set to zero or keep original\n",
    "            normalized_cols.append(expr.alias(col))\n",
    "        \n",
    "        cols_to_select = [\"label\"]\n",
    "        if \"_partition_id\" in df.columns:\n",
    "            cols_to_select.append(\"_partition_id\")\n",
    "\n",
    "        return df.select(*normalized_cols, *cols_to_select)\n",
    "\n",
    "    def _repartition_data_NotBalanced(self, df: DataFrame, preserve_partition_id: bool = False) -> DataFrame:\n",
    "        if \"num_partitions\" in self.config:\n",
    "            new_parts = self.config[\"num_partitions\"]  # \n",
    "            #self.logger.info(f\"Repartitioning data to {new_parts} parts\")\n",
    "            return df.repartition(new_parts)\n",
    "        return df\n",
    "    \n",
    "    def _repartition_data_Balanced(self, df: DataFrame, preserve_partition_id: bool = False) -> DataFrame:\n",
    "        \n",
    "        if (\"num_partitions\" in self.config[\"local_model_config\"] \\\n",
    "            or \"num_partitions\" in self.config[\"global_model_config\"]) \\\n",
    "            and \"label_col\" in self.config:\n",
    "            \n",
    "            if self.config[\"local_model_config\"][\"test_local_model\"] is True:\n",
    "                num_parts = self.config[\"local_model_config\"][\"num_partitions\"]\n",
    "            else:\n",
    "                num_parts = self.config[\"global_model_config\"][\"num_partitions\"]\n",
    "                \n",
    "            label_col = self.config[\"label_col\"]\n",
    "            # self.logger.info(f\"Stratified repartitioning into {num_parts} partitions\")\n",
    "            \n",
    "            # Assign partition IDs (0 to num_parts-1 per class)\n",
    "            # Subtracting 1 so that modulo is computed from 0\n",
    "            window = Window.partitionBy(label_col) \\\n",
    "                            .orderBy(F.rand())          #  one shuffles to group all rows of each label together so we can number them\n",
    "                            \n",
    "            df = df.withColumn(\"_partition_id\", ((F.row_number().over(window) - 1) % num_parts).cast(\"int\"))\n",
    "            \n",
    "            # Force exact number of partitions using partition_id\n",
    "            df = df.repartition(num_parts, F.col(\"_partition_id\"))          # one shuffles to repartition by _partition_id to ensure we have num_parts partitions\n",
    "            print(f'Repartitioning to <<<< {num_parts} >>>> workers - partitions.')\n",
    "            \n",
    "            if not preserve_partition_id:\n",
    "                df = df.drop(\"_partition_id\")\n",
    "            return df\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def run_preprocessing(self, df: DataFrame, min_max) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): Input DataFrame to be preprocessed. pyspark Sql DataFrame.\n",
    "        Returns:\n",
    "            DataFrame: Preprocessed DataFrame ready for training.\n",
    "        \n",
    "            \n",
    "        Run all preprocessing steps in order:\n",
    "         1. Drop rows that are completely null.\n",
    "         2. Repartition the data : shuffle the data to balance the partitions.\n",
    "         3. Normalize the feature columns.\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.handle_missing_values(df)\n",
    "        \n",
    "        if self.config[\"local_model_config\"][\"test_local_model\"] is True:\n",
    "            df = self._repartition_data_Balanced(df, preserve_partition_id = self.config[\"reserve_partition_id\"])\n",
    "        elif self.config[\"global_model_config\"][\"test_global_model\"] is True:\n",
    "            df = self._repartition_data_NotBalanced(df, preserve_partition_id = self.config[\"reserve_partition_id\"])\n",
    "        else:\n",
    "            raise ValueError(\"Preprocessing error.\")\n",
    "        \n",
    "        df = self.normalize(df, min_max, preserve_partition_id = self.config[\"reserve_partition_id\"])\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb49ecb",
   "metadata": {},
   "source": [
    "## Local Model Manager\n",
    "\n",
    "The `LocalModelManager` class is responsible for training local Proximity Tree models and assembling them into a Proximity Forest ensemble.\n",
    "\n",
    "### Overview\n",
    "This file implements the local parallelisation strategy, which applies ensemble learning by partitioning the ECG dataset and independently training multiple `ProximityTree` models. The process is as follows:\n",
    "1. Receives a preprocessed Spark DataFrame.\n",
    "2. Partitions the data (with unbalanced or balanced class distributions).\n",
    "3. Trains one Proximity Tree per partition using the AEON library.\n",
    "4. Serializes trees, returns them to the driver, and combines them into a `ProximityForest`.\n",
    "\n",
    "### Key Features\n",
    "- **Parallel Training**: Spark executors handle partitioned model training concurrently using `mapPartitions`.\n",
    "- **One Tree per Partition**: The number of trained trees equals the number of partitions.\n",
    "- **Optional Weighting**: Tree predictions can be weighted by validation accuracy.\n",
    "- **Manual Assembly**: The ensemble is manually configured with class labels, job parameters, and fit flags for compatibility with AEON.\n",
    "- **Inspection Tools**: Includes utilities to print model structure and details for debugging and analysis.\n",
    "\n",
    "This approach significantly reduces computational overhead via distributed execution but may slightly limit model accuracy due to isolated training on partitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf60034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalModelManager:\n",
    "    \"\"\"\n",
    "    This class handles training local models (Proximity Trees) on chunks of our data and then\n",
    "    puts them together into an  Proximity Forest ensemble.\n",
    "    \n",
    "    The steps are pretty simple:\n",
    "      1. Get a preprocessed Spark DataFrame.\n",
    "      2. Split it into parts.\n",
    "      3. Train a Proximity Tree  model on each part .\n",
    "      4. Then, it gathers all the trees into one Proximity Forest ensemble\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        \"\"\"\n",
    "        Init with our settings.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): Settings like:\n",
    "              - num_partitions: How many parts to split the data into.\n",
    "              - tree_params: Extra parameters for the Proximity Tree  model.\n",
    "        \"\"\"       \n",
    "        # Set default configuration\n",
    "        self.config = config\n",
    "        \n",
    "        # List to store trained trees\n",
    "        self.trees = []\n",
    "        \n",
    "        # Final ensemble model\n",
    "        self.ensemble = None\n",
    "        \n",
    "        # Set up a logger so we can see whats going on\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.addHandler(logging.StreamHandler())\n",
    "        self.logger.setLevel(logging.ERROR)\n",
    "        \n",
    "\n",
    "    def _set_forest_classes(self):\n",
    "        \"\"\"Collect all class labels from individual trees and mark the forest as fitted.\"\"\"\n",
    "        all_classes = []\n",
    "        for tree in self.trees:\n",
    "            if hasattr(tree, \"classes_\"):\n",
    "                all_classes.extend(tree.classes_)\n",
    "\n",
    "        unique_classes = np.unique(all_classes)\n",
    "        self.ensemble.classes_ = unique_classes\n",
    "        self.ensemble.n_classes_ = len(unique_classes)\n",
    "\n",
    "        # AEON’s BaseClassifier typically expects a '_class_dictionary' mapping class->int\n",
    "        self.ensemble._class_dictionary = {\n",
    "            cls: idx for idx, cls in enumerate(unique_classes)\n",
    "        }\n",
    "\n",
    "        # Some older AEON versions store the number of classes in a private attribute\n",
    "        self.ensemble._n_classes = len(unique_classes)\n",
    "\n",
    "        # If n_jobs is used, set it explicitly here\n",
    "        if \"n_jobs\" in self.config[\"forest_params\"]:\n",
    "            self.ensemble._n_jobs = self.config[\"forest_params\"][\"n_jobs\"]\n",
    "\n",
    "        # BaseClassifier sets 'is_fitted = True' at the end of fit().\n",
    "        # So we must set the public property 'is_fitted' (not just 'is_fitted_').\n",
    "        # This ensures ._check_is_fitted() passes in predict().\n",
    "        self.ensemble.is_fitted_ = True\n",
    "        self.ensemble.is_fitted = True\n",
    "\n",
    "        \n",
    "    def get_ensemble(self) -> ProximityForest:\n",
    "        \"\"\"\n",
    "        Return the trained Proximity Forest ensemble.\n",
    "        \"\"\"\n",
    "        return self.ensemble\n",
    "\n",
    "    def print_ensemble_details(self):\n",
    "        \"\"\"\n",
    "        Print the details of the aggregated Proximity Forest ensemble.\n",
    "        \"\"\"\n",
    "        if self.ensemble and hasattr(self.ensemble, 'trees_'):\n",
    "            num_trees = len(self.ensemble.trees_)\n",
    "            print(f\"Aggregated Proximity Forest (contains {num_trees} trees):\")\n",
    "            print(f\"  Number of trees (in trees_ attribute): {num_trees}\")\n",
    "            # You might want to print a summary of the parameters used for the forest here\n",
    "            print(f\"  Forest Parameters: {self.ensemble.get_params()}\")\n",
    "            for i, tree in enumerate(self.ensemble.trees_):\n",
    "                print(f\"  Tree {i+1} Details:\")\n",
    "                self._print_tree_node_info(tree.root, depth=2)\n",
    "            print(\"-\" * 20)\n",
    "        else:\n",
    "            print(\"Proximity Forest ensemble has not been trained yet or the 'trees_' attribute is missing.\")\n",
    "\n",
    "    def _print_tree_node_info(self, node, depth):\n",
    "        indent = \"  \" * depth\n",
    "        print(f\"{indent}Node ID: {node.node_id}, Leaf: {node._is_leaf}\")\n",
    "\n",
    "        if node._is_leaf:\n",
    "            print(f\"{indent}  Label: {node.label}, Class Distribution: {node.class_distribution}\")\n",
    "        else:\n",
    "            splitter = node.splitter\n",
    "            if splitter:\n",
    "                exemplars = splitter[0]\n",
    "                distance_info = splitter[1]\n",
    "                distance_measure = list(distance_info.keys())[0]\n",
    "                distance_params = distance_info[distance_measure]\n",
    "\n",
    "                print(f\"{indent}  Splitter:\")\n",
    "                print(f\"{indent}    Distance Measure: {distance_measure}, Parameters: {distance_params}\")\n",
    "                print(f\"{indent}    Exemplar Classes: {list(exemplars.keys())}\")\n",
    "\n",
    "                print(f\"{indent}  Children:\")\n",
    "                for label, child_node in node.children.items():\n",
    "                    print(f\"{indent}    Branch on exemplar of class '{label}':\")\n",
    "                    self._print_tree_node_info(child_node, depth + 1)\n",
    "\n",
    "   \n",
    "   \n",
    "    def train_ensemble(self, df: DataFrame) -> ProximityForest:\n",
    "        \n",
    "        \"\"\"\n",
    "             Train a forest model iin 3 steps:\n",
    "        1. Prepare data partitions\n",
    "        2. Train trees on each partition\n",
    "        3. Combine trees into a forest\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        tree_params = self.config[\"tree_params\"]      \n",
    "        \n",
    "         # Define how to process each partition - inline function\n",
    "        def process_partition(partition_data):\n",
    "            \"\"\"Process one data partition to train a tree.\"\"\"\n",
    "            try:\n",
    "                # Convert Spark rows to pandas DataFrame\n",
    "                pandas_df = pd.DataFrame([row.asDict() for row in partition_data])\n",
    "                if pandas_df.empty:\n",
    "                    return []\n",
    "                \n",
    "                # Prepare features (3D format for AEON) and labes\n",
    "                X = np.ascontiguousarray(pandas_df.drop(\"label\", axis=1).values)\n",
    "                X_3d = X.reshape((X.shape[0], 1, X.shape[1]))  # (samples, 1, features)\n",
    "                y = pandas_df[\"label\"].values\n",
    "                \n",
    "                # Train one tree\n",
    "                tree = ProximityTree(**tree_params)\n",
    "                tree.fit(X_3d, y)\n",
    "                \n",
    "                # Return serialized tree\n",
    "                return [pickle.dumps(tree)]\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to train tree on partition: {str(e)}\")\n",
    "                return []  # Skip failed partitions\n",
    "            \n",
    "        # Run training on all partitions\n",
    "        trees_rdd = df.rdd.mapPartitions(process_partition)\n",
    "        serialized_trees = trees_rdd.collect()\n",
    "        self.trees = [pickle.loads(b) for b in serialized_trees if b is not None]\n",
    "\n",
    "        # Build the forest\n",
    "        if self.trees:\n",
    "            self.ensemble = ProximityForest(\n",
    "                n_trees=len(self.trees),\n",
    "                **self.config[\"forest_params\"]\n",
    "            )\n",
    "            # Manually set forest properties\n",
    "            self.ensemble.trees_ = self.trees\n",
    "            self._set_forest_classes() \n",
    "            return self.ensemble\n",
    "        else:\n",
    "            print(\"Warning: No trees were trained!\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae24564",
   "metadata": {},
   "source": [
    "## GlobalModelManager: Distributed Proximity Tree \n",
    "\n",
    "The `GlobalModelManager` class builds a single proximity tree across an entire dataset distributed in Apache Spark. It is optimized for global parallelisation and minimizes driver-node bottlenecks via strategic broadcasting and in-place DataFrame updates.\n",
    "\n",
    "### Key Features:\n",
    "- **Tree Construction**: Builds the tree in a depth-wise, breadth-first manner. Each node selects the best exemplar-based split using Gini impurity. Nodes failing split conditions are marked as leaves.\n",
    "- **Distributed Processing**: Uses Spark DataFrames, caching, and broadcast variables to coordinate node splits and data routing across workers without full dataset shuffling.\n",
    "- **Enhanced Prediction**: Converts the tree to a plain dictionary, broadcasts it, and uses a UDF for efficient traversal during prediction.\n",
    "- **Persistence**: Supports saving/loading of the full tree state via pickling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f9b3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure logging for this module\n",
    "# Note: In a distributed environment, the root logger config might be set by Spark.\n",
    "# However, getting a module-specific logger like below is standard practice.\n",
    "# The level set here acts as a minimum threshold for this logger.\n",
    "# The actual output depends on the *handler* configuration (e.g., basicConfig)\n",
    "# and the overall Spark/root logger level.\n",
    "# We keep basicConfig for potential use when running as a standalone script __main__.\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress excessive logging from py4j and pyspark itself\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "\n",
    "    _NP = True\n",
    "except ImportError: # pragma: no cover – NumPy optional for tiny envs\n",
    "    _NP = False\n",
    "\n",
    "# .............................................................................\n",
    "# helpers\n",
    "# .............................................................................\n",
    "\n",
    "TreeNode = collections.namedtuple(\n",
    "    \"TreeNode\", \"node_id parent_id split_on is_leaf prediction children\".split()\n",
    ")\n",
    "\n",
    "# Keep the original efficient euclidean distance function\n",
    "def _euclid(a, b):\n",
    "    \"\"\"Fast Euclidean distance for python *or* NumPy inputs.\"\"\"\n",
    "    # --- Use logger for debugging ---\n",
    "    # Note: Logs from UDFs go to worker logs. In local mode, often appear in console.\n",
    "    logger.debug(f\"UDF: _euclid inputs: a={a[:5] if isinstance(a, (list, np.ndarray)) else a}..., b={b[:5] if isinstance(b, (list, np.ndarray)) else b}...\")\n",
    "    # -------------------------------------\n",
    "    if a is None or b is None or len(a) != len(b):\n",
    "        # --- Use logger ---\n",
    "        logger.debug(f\"UDF: _euclid returning inf due to None/len mismatch: a is None={a is None}, b is None={b is None}, len(a)={len(a) if a is not None else 'N/A'}, len(b)={len(b) if b is not None else 'N/A'}\")\n",
    "        # --------------------\n",
    "        return float(\"inf\")\n",
    "    if _NP:\n",
    "        try:\n",
    "            diff = np.subtract(a, b, dtype=float)\n",
    "            dist = float(np.sqrt(np.dot(diff, diff)))\n",
    "            # --- Use logger ---\n",
    "            # logger.debug(f\"UDF: _euclid (NumPy) returning {dist}\") # Avoid logging too much if successful\n",
    "            # --------------------\n",
    "            return dist\n",
    "        except Exception as e:\n",
    "            # --- Use logger for NumPy errors ---\n",
    "            logger.error(f\"UDF: ERROR in _euclid (NumPy path): {e}. Inputs: a={a[:5] if isinstance(a, (list, np.ndarray)) else a}..., b={b[:5] if isinstance(b, (list, np.ndarray)) else b}...\")\n",
    "            # ---------------------------------------\n",
    "            # Re-raise or return inf, depending on desired behavior on error\n",
    "            return float(\"inf\") # Or raise e\n",
    "    else: # Pure Python path\n",
    "        try:\n",
    "            dist = float(math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b))))\n",
    "            # --- Use logger ---\n",
    "            # logger.debug(f\"UDF: _euclid (Python) returning {dist}\") # Avoid logging too much if successful\n",
    "            # --------------------\n",
    "            return dist\n",
    "        except Exception as e:\n",
    "            # --- Use logger for Python errors ---\n",
    "            logger.error(f\"UDF: ERROR in _euclid (Python path): {e}. Inputs: a={a[:5] if isinstance(a, (list, np.ndarray)) else a}..., b={b[:5] if isinstance(b, (list, np.ndarray)) else b}...\")\n",
    "            # --------------------------------------\n",
    "            # Re-raise or return inf\n",
    "            return float(\"inf\") # Or raise e\n",
    "\n",
    "\n",
    "# .............................................................................\n",
    "# prediction-side helper – pure python, broadcasted once per executor\n",
    "# (MODIFIED to use enhanced traversal logic)\n",
    "# .............................................................................\n",
    "\n",
    "def _enhanced_mk_traverse(bc_plain_tree):\n",
    "    \"\"\"\n",
    "    Return a local function that navigates the broadcast tree using enhanced logic.\n",
    "    Expects a broadcasted plain dictionary structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # The tree is now a plain dictionary\n",
    "    tree: Dict[int, Dict[str, Any]] = bc_plain_tree.value\n",
    "    # Get a logger instance inside the UDF factory function\n",
    "    # This logger will be serialized and sent to workers\n",
    "    udf_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "    def _enhanced_traverse(ts):\n",
    "        \"\"\"Enhanced traversal logic for a single time series.\"\"\"\n",
    "        # Logging inside UDFs can be tricky; messages go to worker logs by default.\n",
    "        # Use sparingly or configure Spark logging to collect worker logs.\n",
    "        # udf_logger.debug(\"UDF: _enhanced_traverse started for time series.\")\n",
    "\n",
    "        if ts is None:\n",
    "            # udf_logger.debug(\"UDF: Input time series is None. Returning None.\")\n",
    "            # Fallback handled by coalesce in predict method\n",
    "            return None\n",
    "\n",
    "        node_id = 0 # Start at root\n",
    "        # udf_logger.debug(f\"UDF: Starting traversal from root node {node_id}.\")\n",
    "\n",
    "        # Traverse the tree until a leaf node is reached or traversal stops\n",
    "        while node_id in tree:\n",
    "            current_node = tree[node_id]\n",
    "            # udf_logger.debug(f\"UDF: Current node_id: {node_id}, is_leaf: {current_node.get('is_leaf', False)}\")\n",
    "\n",
    "            # If it's a leaf node, return its prediction\n",
    "            if current_node.get('is_leaf', False): # Use .get for safety\n",
    "                # udf_logger.debug(f\"UDF: Node {node_id} is leaf. Returning prediction: {current_node.get('prediction')}\")\n",
    "                return current_node.get('prediction') # Prediction is in the plain dict\n",
    "\n",
    "            # If it's an internal node, use the split info to decide which branch to follow\n",
    "            split_info = current_node.get('split_on') # (measure_type, {branch_id: exemplar_ts})\n",
    "            children = current_node.get('children')\n",
    "\n",
    "            # Ensure split info and children exist for internal nodes\n",
    "            if split_info and children and len(children) > 0:\n",
    "                _, exemplars = split_info # We only need exemplars for the split\n",
    "                # udf_logger.debug(f\"UDF: Node {node_id} is internal. Split info: {split_info}, Children: {children}\")\n",
    "\n",
    "                # Calculate distance to ALL exemplars used in THIS node's split\n",
    "                min_dist_all_exemplars = float(\"inf\")\n",
    "                best_branch_id_all_exemplars = None # Label of the nearest exemplar\n",
    "\n",
    "                # Handle case where exemplars might be empty (shouldn't happen in valid tree)\n",
    "                if not exemplars:\n",
    "                    # udf_logger.warning(f\"UDF: Node {node_id} is internal but has no exemplars. Treating as leaf.\")\n",
    "                    # Internal node with no exemplars or children? Treat as leaf with its prediction\n",
    "                    return current_node.get('prediction') # Prediction should be None if not finalized\n",
    "\n",
    "                # udf_logger.debug(f\"UDF: Calculating distances to exemplars for node {node_id}.\")\n",
    "                for branch_id, exemplar_ts in exemplars.items():\n",
    "                    # Use the base _euclid function\n",
    "                    d = _euclid(ts, exemplar_ts)\n",
    "                    # udf_logger.debug(f\"UDF: Distance to exemplar {branch_id}: {d}\")\n",
    "                    if d < min_dist_all_exemplars:\n",
    "                        min_dist_all_exemplars = d\n",
    "                        best_branch_id_all_exemplars = branch_id\n",
    "\n",
    "                # udf_logger.debug(f\"UDF: Nearest exemplar branch_id for node {node_id}: {best_branch_id_all_exemplars}\")\n",
    "\n",
    "                # --- Enhanced Traversal Logic ---\n",
    "                # Check if the child node corresponding to the overall nearest exemplar exists\n",
    "                if best_branch_id_all_exemplars is not None and best_branch_id_all_exemplars in children:\n",
    "                    # If the child exists, move to that child node\n",
    "                    next_node = children[best_branch_id_all_exemplars]\n",
    "                    # udf_logger.debug(f\"UDF: Moving to child node {next_node} via branch {best_branch_id_all_exemplars}.\")\n",
    "                    node_id = next_node\n",
    "                else:\n",
    "                    # If the ideal child does NOT exist (pruned branch),\n",
    "                    # find the nearest exemplar among the *existing* child branches and follow that path.\n",
    "                    # udf_logger.debug(f\"UDF: Ideal child branch {best_branch_id_all_exemplars} not found in children {children}. Finding nearest among existing children.\")\n",
    "                    min_dist_existing_children = float(\"inf\")\n",
    "                    next_node_id = None # The child node ID to move to\n",
    "\n",
    "                    # Iterate through the *existing* child branches listed in the tree structure\n",
    "                    for existing_branch_id, existing_child_id in children.items():\n",
    "                        # Find the exemplar time series for this existing branch from the original exemplars used for the split\n",
    "                        if existing_branch_id in exemplars: # Double check exemplar exists for this branch\n",
    "                            existing_exemplar_ts = exemplars[existing_branch_id]\n",
    "                            # Calculate distance to this existing branch's exemplar\n",
    "                            d = _euclid(ts, existing_exemplar_ts)\n",
    "                            # udf_logger.debug(f\"UDF: Distance to existing child branch {existing_branch_id} exemplar: {d}\")\n",
    "                            if d < min_dist_existing_children:\n",
    "                                min_dist_existing_children = d\n",
    "                                next_node_id = existing_child_id\n",
    "\n",
    "                    # If a nearest existing child was found, move to that child node\n",
    "                    if next_node_id is not None:\n",
    "                        # udf_logger.debug(f\"UDF: Routing to nearest existing child {next_node_id}.\")\n",
    "                        node_id = next_node_id\n",
    "                    else:\n",
    "                        # If no existing children were found or no nearest existing child determined,\n",
    "                        # stop traversal and return the current node's prediction (which should be None for internal nodes)\n",
    "                        # udf_logger.warning(f\"UDF: No nearest existing child found for node {current_node.get('node_id')}. Stopping traversal.\")\n",
    "                        return current_node.get('prediction') # Fallback handled by coalesce later\n",
    "\n",
    "\n",
    "            else:\n",
    "                # If the node is internal but has no split info or children (shouldn't happen in valid tree)\n",
    "                # Stop traversal and return the current node's prediction.\n",
    "                # udf_logger.warning(f\"UDF: Node {current_node.get('node_id')} is internal but missing split info or children. Stopping traversal.\")\n",
    "                return current_node.get('prediction') # Fallback handled by coalesce later\n",
    "\n",
    "\n",
    "        # If the loop finishes without returning (e.g., node_id not found, error)\n",
    "        # This indicates a problem in the tree structure.\n",
    "        # Returning None here, will be caught by coalesce.\n",
    "        # udf_logger.error(f\"UDF: Traversal loop finished unexpectedly at node_id {node_id}.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    return _enhanced_traverse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GlobalModelManager class (Optimised + Enhanced Prediction)\n",
    "# =============================================================================\n",
    "\n",
    "class GlobalModelManager:\n",
    "    \"\"\"Distribution-friendly proximity-tree learner.\"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # init\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def __init__(self, spark: SparkSession, config: Dict[str, Any]):\n",
    "        logger.debug(\"GlobalModelManager __init__ started.\")\n",
    "        p = config[\"tree_params\"]\n",
    "        self.spark = spark\n",
    "        self.max_depth: int | None = p.get(\"max_depth\") # Use .get for safety\n",
    "        self.min_samples: int = p.get(\"min_samples_split\", 2) # Use .get with default\n",
    "        self.k: int = p.get(\"n_splitters\", 5) # Use .get with default\n",
    "        self.tree: Dict[int, TreeNode] = {0: TreeNode(0, None, None, False, None, {})}\n",
    "        self._next_id: int = 1\n",
    "        self._maj: int = 1 # fallback class if everything else fails\n",
    "        logger.debug(f\"Initialized with max_depth={self.max_depth}, min_samples={self.min_samples}, k={self.k}\")\n",
    "        logger.debug(\"GlobalModelManager __init__ finished.\")\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # private helpers\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _to_ts_df(self, df):\n",
    "        \"\"\"Ensure DataFrame has (row_id, time_series[, true_label]).\"\"\"\n",
    "        logger.debug(\"_to_ts_df started.\")\n",
    "\n",
    "        if \"row_id\" not in df.columns:\n",
    "            logger.debug(\"Adding row_id.\")\n",
    "            df = df.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "        else:\n",
    "            logger.debug(\"Casting existing row_id to LongType.\")\n",
    "            df = df.withColumn(\"row_id\", F.col(\"row_id\").cast(LongType()))\n",
    "\n",
    "        if \"time_series\" in df.columns:\n",
    "            logger.debug(\"'time_series' column already exists.\")\n",
    "            if \"label\" in df.columns and \"true_label\" not in df.columns:\n",
    "                logger.debug(\"Renaming 'label' to 'true_label'.\")\n",
    "                df = df.withColumnRenamed(\"label\", \"true_label\")\n",
    "            logger.debug(\"_to_ts_df finished (already formatted).\")\n",
    "            return df\n",
    "\n",
    "        lbl = \"label\" if \"label\" in df.columns else (\n",
    "            \"true_label\" if \"true_label\" in df.columns else None\n",
    "        )\n",
    "        feat_cols = [c for c in df.columns if c not in {lbl, \"row_id\"}]\n",
    "        logger.debug(f\"Found feature columns: {feat_cols}\")\n",
    "        cols = [\n",
    "            \"row_id\",\n",
    "            F.array(*[F.col(c) for c in feat_cols]).alias(\"time_series\"),\n",
    "        ]\n",
    "        if lbl:\n",
    "            logger.debug(f\"Including label column: {lbl}\")\n",
    "            cols.append(F.col(lbl).cast(IntegerType()).alias(\"true_label\"))\n",
    "        else:\n",
    "            logger.debug(\"No label column found.\")\n",
    "\n",
    "        ts_df = df.select(*cols)\n",
    "        logger.debug(\"_to_ts_df finished (conversion done).\")\n",
    "        return ts_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _gini(counts: Dict[int, int]) -> float:\n",
    "        tot = sum(counts.values())\n",
    "        if tot == 0:\n",
    "            return 0.0\n",
    "        return 1.0 - sum((c / tot) ** 2 for c in counts.values())\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # fitting\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def fit(self, df): # noqa: C901 (complexity accepted here)\n",
    "        \"\"\"Train the proximity tree.\"\"\"\n",
    "        logger.debug(\"fit started.\")\n",
    "        df = self._to_ts_df(df).cache()\n",
    "        initial_row_count = df.count()\n",
    "        logger.debug(f\"Initial DataFrame prepared and cached. Row count: {initial_row_count}\")\n",
    "\n",
    "        if initial_row_count == 0:\n",
    "             logger.debug(\"Input DataFrame is empty. Setting root as leaf.\")\n",
    "             self.tree[0] = self.tree[0]._replace(is_leaf=True, prediction=self._maj, children={})\n",
    "             df.unpersist()\n",
    "             logger.debug(\"fit finished (empty DataFrame).\")\n",
    "             return self\n",
    "\n",
    "\n",
    "        maj_row = df.groupBy(\"true_label\").count().orderBy(F.desc(\"count\")).first()\n",
    "        if maj_row:\n",
    "            self._maj = maj_row[0]\n",
    "            logger.debug(f\"Overall majority class calculated: {self._maj}\")\n",
    "        else:\n",
    "            logger.debug(f\"No data to calculate overall majority class. Keeping default: {self._maj}\")\n",
    "\n",
    "\n",
    "        assign = (\n",
    "            df.select(\"row_id\", \"time_series\", \"true_label\")\n",
    "            .withColumn(\"node_id\", F.lit(0))\n",
    "            .cache()\n",
    "        )\n",
    "        assign_initial_count = assign.count()\n",
    "        logger.debug(f\"Initial assignment DataFrame created and cached. Row count: {assign_initial_count} at root node 0.\")\n",
    "        df.unpersist() # Unpersist initial DataFrame\n",
    "\n",
    "\n",
    "        open_nodes, depth = {0}, 0\n",
    "        logger.debug(f\"Starting tree building loop with initial open_nodes: {open_nodes}\")\n",
    "\n",
    "        # --------------- depth-wise growth ----------------------------\n",
    "        while open_nodes and (self.max_depth is None or depth < self.max_depth):\n",
    "            logger.debug(f\"\\n--- Starting tree level {depth} ---\")\n",
    "            logger.debug(f\"Open nodes for this level: {open_nodes}\")\n",
    "\n",
    "            cur = assign.filter(F.col(\"node_id\").isin(list(open_nodes))).cache()\n",
    "            cur_count = cur.count()\n",
    "            logger.debug(f\"Filtered data for current level. Row count: {cur_count}\")\n",
    "\n",
    "            if cur.isEmpty():\n",
    "                logger.debug(f\"No data for open nodes at depth {depth}. Breaking loop.\")\n",
    "                cur.unpersist(); break\n",
    "\n",
    "            # 1) exemplar pool per (node,label) – single pass\n",
    "            logger.debug(\"Starting exemplar pool sampling.\")\n",
    "            all_ts_per_node_label = (\n",
    "                cur.groupBy(\"node_id\", \"true_label\")\n",
    "                .agg(F.collect_list(\"time_series\").alias(\"ts_list\"))\n",
    "                .collect() # Collects list of Rows: Row(node_id=..., true_label=..., ts_list=[...])\n",
    "            )\n",
    "            logger.debug(f\"Collected time series lists for {len(all_ts_per_node_label)} node-label groups.\")\n",
    "\n",
    "            pool: Dict[int, Dict[int, list]] = {}\n",
    "            # Perform random sampling on the driver from the collected lists\n",
    "            for r in all_ts_per_node_label:\n",
    "                node_id = r.node_id\n",
    "                true_label = r.true_label\n",
    "                ts_list = r.ts_list\n",
    "\n",
    "                if not ts_list:\n",
    "                    logger.debug(f\"No time series found for node {node_id}, label {true_label} in collected list.\")\n",
    "                    continue\n",
    "\n",
    "                # Randomly sample self.k exemplars from the list\n",
    "                # Ensure we don't sample more than available\n",
    "                num_to_sample = min(self.k, len(ts_list))\n",
    "                # Use random.sample for actual random selection\n",
    "                sampled_ts = random.sample(ts_list, num_to_sample)\n",
    "\n",
    "                pool.setdefault(node_id, {})[true_label] = sampled_ts\n",
    "                # logger.debug(f\"Sampled {len(sampled_ts)} exemplars for node {node_id}, label {true_label}. Sample: {sampled_ts[:2]}...\") # Avoid printing large lists\n",
    "                logger.debug(f\"Sampled {len(sampled_ts)} exemplars for node {node_id}, label {true_label}.\")\n",
    "\n",
    "\n",
    "            logger.debug(f\"Finished exemplar pool sampling. Pool structure keys: {list(pool.keys())}\")\n",
    "\n",
    "\n",
    "            best: Dict[int, Tuple[str, Dict[int, list]]] = {}\n",
    "            to_leaf: set[int] = set()\n",
    "\n",
    "            # 2) pick best split per node (driver)\n",
    "            logger.debug(\"Starting best split evaluation per node.\")\n",
    "            for nid in list(open_nodes): # Iterate over a copy in case nodes are removed from open_nodes\n",
    "                logger.debug(f\"Evaluating splits for node {nid}.\")\n",
    "                nd_df = cur.filter(F.col(\"node_id\") == nid)\n",
    "                nd_df_count = nd_df.count()\n",
    "                logger.debug(f\"Data count for node {nid}: {nd_df_count}\")\n",
    "\n",
    "                # Calculate local stats for leaf conditions and parent Gini\n",
    "                stats_rows = nd_df.groupBy(\"true_label\").count().collect()\n",
    "                # --- FIXED: Access count using r['count'] ---\n",
    "                stats = {r.true_label: r['count'] for r in stats_rows}\n",
    "                tot = sum(stats.values())\n",
    "                logger.debug(f\"Node {nid} stats: {stats}, total samples: {tot}\")\n",
    "\n",
    "\n",
    "                # Leaf Condition 1: Insufficient samples, purity, or no exemplars\n",
    "                if tot < self.min_samples:\n",
    "                    logger.debug(f\"Node {nid} has {tot} samples, below min_samples {self.min_samples}. Marking as leaf.\")\n",
    "                    to_leaf.add(nid); continue\n",
    "                if len(stats) <= 1:\n",
    "                     logger.debug(f\"Node {nid} is pure ({len(stats)} labels). Marking as leaf.\")\n",
    "                     to_leaf.add(nid); continue\n",
    "                if nid not in pool or not pool[nid]:\n",
    "                    logger.debug(f\"No exemplars found in pool for node {nid}. Marking as leaf.\")\n",
    "                    to_leaf.add(nid); continue\n",
    "\n",
    "                parent_g = self._gini(stats)\n",
    "                logger.debug(f\"Node {nid} parent Gini: {parent_g}\")\n",
    "\n",
    "                labels = list(pool[nid].keys())\n",
    "\n",
    "                # Leaf Condition 2: Insufficient exemplar labels for split\n",
    "                if len(labels) < 2:\n",
    "                    logger.debug(f\"Node {nid} has {len(labels)} exemplar labels in pool, need >= 2 for split. Marking as leaf.\")\n",
    "                    to_leaf.add(nid); continue\n",
    "\n",
    "                best_gain, best_exp = -1.0, None\n",
    "                logger.debug(f\"Evaluating {self.k} candidate splits for node {nid}.\")\n",
    "                for i in range(self.k): # Evaluate k candidate splits\n",
    "                    # Sample exemplars for THIS candidate split from the pool\n",
    "                    # Ensure pool[nid][lbl] is not empty before random.choice\n",
    "                    candidate_ex = {}\n",
    "                    for lbl in labels:\n",
    "                        if lbl in pool[nid] and pool[nid][lbl]:\n",
    "                            candidate_ex[lbl] = random.choice(pool[nid][lbl])\n",
    "                        else:\n",
    "                            logger.debug(f\"Warning: No exemplars in pool for label {lbl} in node {nid} for candidate {i+1}. Skipping this candidate split.\")\n",
    "                            candidate_ex = None # Invalidate this candidate\n",
    "                            break # Stop evaluating this candidate\n",
    "\n",
    "                    if candidate_ex is None or len(candidate_ex) < 2:\n",
    "                         logger.debug(f\"Candidate split {i+1} for node {nid} has less than 2 exemplars ({len(candidate_ex) if candidate_ex is not None else 'None'}). Skipping.\")\n",
    "                         continue # Need at least two branches for a valid split\n",
    "\n",
    "                    # logger.debug(f\"Evaluating candidate split {i+1} for node {nid} with exemplars for labels: {list(candidate_ex.keys())}. Exemplars: {candidate_ex}\")\n",
    "                    logger.debug(f\"Evaluating candidate split {i+1} for node {nid} with exemplars for labels: {list(candidate_ex.keys())}.\")\n",
    "                    bc_ex = self.spark.sparkContext.broadcast(candidate_ex)\n",
    "\n",
    "                    @F.udf(IntegerType())\n",
    "                    def nearest_lbl_udf(ts):\n",
    "                        # Use logger here instead of print\n",
    "                        udf_logger_local = logging.getLogger(__name__) # Get logger instance in worker\n",
    "                        udf_logger_local.debug(f\"UDF: nearest_lbl_udf processing TS: {ts[:5] if isinstance(ts, (list, np.ndarray)) else ts}...\")\n",
    "\n",
    "                        best_d, best_l = float(\"inf\"), None\n",
    "                        # Use the original _euclid function\n",
    "                        exemplars_val = bc_ex.value\n",
    "                        if not exemplars_val:\n",
    "                            udf_logger_local.debug(\"UDF: Exemplars is empty, returning None.\")\n",
    "                            return None # Safety check\n",
    "\n",
    "                        for l, ex_ts in exemplars_val.items():\n",
    "                            # Use logger inside _euclid as well\n",
    "                            d = _euclid(ts, ex_ts) # _euclid now uses logger internally\n",
    "                            if d < best_d:\n",
    "                                best_d, best_l = d, l\n",
    "\n",
    "                        udf_logger_local.debug(f\"UDF: Finished calculating distances. Best label: {best_l}\")\n",
    "                        return best_l\n",
    "\n",
    "                    # This is the DataFrame-based Gini calculation (RETAINED for speed)\n",
    "                    ass = (\n",
    "                        nd_df.withColumn(\"branch\", nearest_lbl_udf(\"time_series\"))\n",
    "                        .groupBy(\"branch\", \"true_label\")\n",
    "                        .count()\n",
    "                    )\n",
    "                    branch_cnt = ass.groupBy(\"branch\").agg(F.sum(\"count\").alias(\"tot\"))\n",
    "                    joined = ass.join(branch_cnt, \"branch\")\n",
    "\n",
    "                    # Check if joined DataFrame is empty before calculating impurity\n",
    "                    if joined.isEmpty():\n",
    "                         logger.debug(f\"Joined DataFrame is empty for candidate {i+1} on node {nid}. Cannot calculate impurity.\")\n",
    "                         bc_ex.unpersist(False)\n",
    "                         continue\n",
    "\n",
    "                    imp_row = (\n",
    "                         joined.withColumn(\"prob_sq\", (F.col(\"count\") / F.col(\"tot\")) ** 2)\n",
    "                         .groupBy(\"branch\", \"tot\")\n",
    "                         .agg(F.sum(\"prob_sq\").alias(\"s\"))\n",
    "                         .withColumn(\"g\", 1.0 - F.col(\"s\"))\n",
    "                         .withColumn(\"w\", (F.col(\"tot\") / tot) * F.col(\"g\"))\n",
    "                         .agg(F.sum(\"w\").alias(\"imp\"))\n",
    "                         .first() # Collect the single result to the driver\n",
    "                    )\n",
    "\n",
    "                    if imp_row is None:\n",
    "                         logger.debug(f\"Impurity calculation returned None for candidate {i+1} on node {nid}. Skipping.\")\n",
    "                         bc_ex.unpersist(False)\n",
    "                         continue\n",
    "\n",
    "                    imp = imp_row[0]\n",
    "                    gain = parent_g - imp\n",
    "                    logger.debug(f\"Candidate split {i+1} for node {nid}: Impurity={imp:.4f}, Gain={gain:.4f}\")\n",
    "\n",
    "                    bc_ex.unpersist(False) # Unpersist broadcasted exemplars for this candidate\n",
    "\n",
    "                    # Leaf Condition 3: Update best split if gain is improved\n",
    "                    if gain > best_gain:\n",
    "                        best_gain, best_exp = gain, candidate_ex\n",
    "                        logger.debug(f\"Candidate split {i+1} is the best so far for node {nid} with gain {best_gain:.4f}.\")\n",
    "\n",
    "                # Leaf Condition 4: If best gain is not significantly positive after all candidates\n",
    "                if best_gain > 1e-9: # Use tolerance for splitting decision\n",
    "                    logger.debug(f\"Node {nid} found a good split with gain {best_gain:.4f}.\")\n",
    "                    best[nid] = (\"euclidean\", best_exp)\n",
    "                else:\n",
    "                    logger.debug(f\"Node {nid} did not find a good split (best gain {best_gain:.4f}). Marking as leaf.\")\n",
    "                    to_leaf.add(nid) # Node becomes a leaf\n",
    "\n",
    "            logger.debug(\"Finished best split evaluation per node.\")\n",
    "\n",
    "\n",
    "            # 2b) mark leaves right away **with LOCAL majority** (REFINED CALCULATION)\n",
    "            logger.debug(\"Finalizing nodes marked as leaves in this iteration.\")\n",
    "            for nid in list(to_leaf): # Iterate over a copy as we remove from open_nodes\n",
    "                if nid not in self.tree:\n",
    "                     logger.debug(f\"Node {nid} already removed from tree? Skipping finalization.\")\n",
    "                     open_nodes.discard(nid) # Ensure it's not in open_nodes\n",
    "                     continue\n",
    "\n",
    "                if self.tree[nid].is_leaf:\n",
    "                     logger.debug(f\"Node {nid} already finalized as leaf. Skipping.\")\n",
    "                     open_nodes.discard(nid) # Ensure it's not in open_nodes\n",
    "                     continue\n",
    "\n",
    "                logger.debug(f\"Finalizing node {nid} as a leaf.\")\n",
    "                # Recalculate stats from the data currently assigned to this node\n",
    "                leaf_data_df = cur.filter(F.col(\"node_id\") == nid).cache()\n",
    "                leaf_stats_rows = leaf_data_df.groupBy(\"true_label\").count().collect()\n",
    "                leaf_stats = {r.true_label: r['count'] for r in leaf_stats_rows}\n",
    "                leaf_data_df.unpersist() # Unpersist leaf data\n",
    "\n",
    "                logger.debug(f\"Node {nid} local stats for leaf prediction: {leaf_stats}\")\n",
    "\n",
    "                maj_lbl = self._maj # Default to overall majority\n",
    "\n",
    "                if leaf_stats:\n",
    "                    # Find majority label using count, break ties using smallest label\n",
    "                    maj_lbl = max(leaf_stats.items(), key=lambda kv: (kv[1], -kv[0]))[0]\n",
    "                    logger.debug(f\"Node {nid} local majority prediction: {maj_lbl}\")\n",
    "                else:\n",
    "                    logger.debug(f\"No data found for node {nid} during leaf finalization. Using overall majority fallback: {maj_lbl}\")\n",
    "\n",
    "\n",
    "                self.tree[nid] = self.tree[nid]._replace(is_leaf=True, prediction=maj_lbl, children={})\n",
    "                logger.debug(f\"Node {nid} marked as leaf with prediction {maj_lbl}.\")\n",
    "                open_nodes.discard(nid) # Remove finalized leaves from consideration\n",
    "\n",
    "            logger.debug(\"Finished finalizing leaves for this iteration.\")\n",
    "\n",
    "\n",
    "            # 3) create children + update assignment DF\n",
    "            # This block is largely REETAINED from the original for its DataFrame efficiency\n",
    "            if not best:\n",
    "                logger.debug(\"No nodes found good splits in this iteration. Breaking loop.\")\n",
    "                cur.unpersist(); break # No nodes successfully split in this iteration\n",
    "\n",
    "            logger.debug(\"Creating children and updating assignment DataFrame.\")\n",
    "            split_map, new_open = {}, {}\n",
    "            for pid, (m, ex) in best.items():\n",
    "                logger.debug(f\"Processing best split for parent node {pid}.\")\n",
    "                ch = {}\n",
    "                # Only create children for branches with exemplars in the chosen split\n",
    "                for lbl in ex:\n",
    "                    cid = self._next_id; self._next_id += 1\n",
    "                    # Children are initially non-leaves with no prediction or split info\n",
    "                    self.tree[cid] = TreeNode(cid, pid, None, False, None, {})\n",
    "                    ch[lbl] = cid\n",
    "                    # Map (parent_id, branch_label) to new child_id\n",
    "                    split_map[(pid, lbl)] = cid\n",
    "                    # Add new children to the set for the next iteration\n",
    "                    new_open[cid] = None # Value doesn't matter, just need the set keys\n",
    "                    logger.debug(f\"Created child {cid} for branch {lbl} of parent {pid}.\")\n",
    "\n",
    "\n",
    "                # Update the parent node in the tree structure\n",
    "                self.tree[pid] = self.tree[pid]._replace(split_on=(m, ex), children=ch, is_leaf=False)\n",
    "                logger.debug(f\"Parent node {pid} updated: split_on={self.tree[pid].split_on}, children={self.tree[pid].children}\")\n",
    "\n",
    "\n",
    "            # Prepare for the next iteration\n",
    "            open_nodes = set(new_open.keys())\n",
    "            logger.debug(f\"New open_nodes for next level: {open_nodes}\")\n",
    "\n",
    "            # Broadcast the split mapping and exemplars for the route UDF\n",
    "            bc_split = self.spark.sparkContext.broadcast(split_map)\n",
    "            bc_exs = self.spark.sparkContext.broadcast({pid: ex for pid, (_, ex) in best.items()})\n",
    "            logger.debug(\"Broadcasted split_map and best_exs for route_udf.\")\n",
    "\n",
    "\n",
    "            # Define the UDF for routing rows to children (RETAINED)\n",
    "            @F.udf(IntegerType())\n",
    "            def route_udf(pid, ts):\n",
    "                # If this parent didn't split in this iteration (shouldn't happen if filtering 'cur' correctly)\n",
    "                if pid not in bc_exs.value:\n",
    "                    # This case might happen if a node was in open_nodes but had no data in cur\n",
    "                    # or if there's a logic error. Returning pid keeps the row at the current node.\n",
    "                    return pid\n",
    "\n",
    "                ex = bc_exs.value[pid]\n",
    "                best_d, best_lbl = float(\"inf\"), None\n",
    "                # Find nearest exemplar for the row among the split exemplars\n",
    "                for l, ex_ts in ex.items():\n",
    "                    d = _euclid(ts, ex_ts)\n",
    "                    if d < best_d:\n",
    "                        best_d, best_lbl = d, l\n",
    "\n",
    "                # Get the new child node ID from the split mapping.\n",
    "                # If the (parent_id, branch_label) is NOT in the map (e.g., branch didn't meet min_samples)\n",
    "                # return the parent_id, effectively keeping the row at the parent node.\n",
    "                return bc_split.value.get((pid, best_lbl), pid)\n",
    "\n",
    "            # Apply the route UDF to the entire assignment DataFrame\n",
    "            # This updates the node_id for all rows that were in splitting nodes\n",
    "            old_assign = assign # Keep reference to unpersist\n",
    "            logger.debug(\"Applying route_udf to update assign DataFrame.\")\n",
    "            assign = assign.withColumn(\"node_id\", route_udf(\"node_id\", \"time_series\")).cache()\n",
    "            assign_updated_count = assign.count() # Trigger action and cache\n",
    "            logger.debug(f\"assign DataFrame updated and cached. New total rows: {assign_updated_count}\")\n",
    "\n",
    "\n",
    "            # Unpersist previous assignment DF and broadcasts\n",
    "            old_assign.unpersist()\n",
    "            bc_split.unpersist(False)\n",
    "            bc_exs.unpersist(False)\n",
    "            cur.unpersist() # Unpersist the current level's data\n",
    "            logger.debug(f\"Unpersisted intermediates for depth {depth}.\")\n",
    "\n",
    "            depth += 1 # Increment depth for next iteration\n",
    "\n",
    "        logger.debug(\"\\n--- Main tree building loop finished ---\")\n",
    "        logger.debug(f\"Final open_nodes: {open_nodes}\")\n",
    "\n",
    "\n",
    "        # --- Final Dangling Node Finalization (Using the final 'assign' state) ---\n",
    "        # This block should execute *before* the final assign.unpersist()\n",
    "        logger.debug(\"Performing final dangling node finalization.\")\n",
    "        nodes_to_finalize_at_end = [nid for nid, nd in self.tree.items() if not nd.is_leaf and not nd.children]\n",
    "        if nodes_to_finalize_at_end:\n",
    "             logger.debug(f\"Found {len(nodes_to_finalize_at_end)} dangling nodes to finalize.\")\n",
    "             # Filter the final assignment DF for these dangling nodes\n",
    "             dangling_df = assign.filter(F.col(\"node_id\").isin(nodes_to_finalize_at_end)).cache()\n",
    "             dangling_df_count = dangling_df.count()\n",
    "             logger.debug(f\"Data count for dangling nodes: {dangling_df_count}\")\n",
    "\n",
    "\n",
    "             # Calculate local majority for each dangling node\n",
    "             dangling_stats_rows = dangling_df.groupBy(\"node_id\", \"true_label\").count().collect()\n",
    "             dangling_stats_by_node = collections.defaultdict(dict)\n",
    "             for r in dangling_stats_rows:\n",
    "                 # --- FIXED: Access count using r['count'] ---\n",
    "                 dangling_stats_by_node[r.node_id][r.true_label] = r['count']\n",
    "\n",
    "             dangling_df.unpersist() # Unpersist dangling data\n",
    "             logger.debug(\"Dangling DataFrame unpersisted.\")\n",
    "\n",
    "\n",
    "             for nid in nodes_to_finalize_at_end:\n",
    "                 stats = dangling_stats_by_node.get(nid, {})\n",
    "                 maj_lbl = self._maj # fallback\n",
    "\n",
    "                 if stats:\n",
    "                     maj_lbl = max(stats.items(), key=lambda kv: (kv[1], -kv[0]))[0]\n",
    "                     logger.debug(f\"Dangling node {nid} local majority prediction: {maj_lbl}\")\n",
    "                 else:\n",
    "                     logger.debug(f\"No data found for dangling node {nid}. Using overall majority fallback: {maj_lbl}\")\n",
    "\n",
    "\n",
    "                 self.tree[nid] = self.tree[nid]._replace(is_leaf=True, prediction=maj_lbl, split_on=None)\n",
    "                 logger.debug(f\"Dangling node {nid} finalized as leaf with prediction {maj_lbl}.\")\n",
    "        else:\n",
    "             logger.debug(\"No dangling nodes found to finalize at the end.\")\n",
    "\n",
    "        assign.unpersist()\n",
    "        logger.debug(\"Final assign DataFrame unpersisted.\")\n",
    "\n",
    "\n",
    "        logger.debug(\"fit finished.\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, df):\n",
    "        logger.debug(\"predict started.\")\n",
    "        df = self._to_ts_df(df)\n",
    "        df_count = df.count()\n",
    "        logger.debug(f\"Input DataFrame for prediction prepared. Row count: {df_count}\")\n",
    "\n",
    "\n",
    "        if not self.tree or (0 not in self.tree) or (self.tree[0].prediction is None and not self.tree[0].children):\n",
    "             logger.warning(\"Tree is not fitted or is empty. Returning DataFrame with default prediction.\")\n",
    "             # Return DataFrame with default prediction if tree is not usable\n",
    "             default_pred_col = F.lit(self._maj).cast(IntegerType()).alias(\"prediction\")\n",
    "             select_cols = [\"row_id\", \"time_series\"] + ([\"true_label\"] if \"true_label\" in df.columns else []) + [default_pred_col]\n",
    "             return df.select(*select_cols)\n",
    "\n",
    "\n",
    "        # --- Convert tree structure to a plain dictionary for broadcasting ---\n",
    "        logger.debug(\"Converting tree structure to plain dictionary for broadcasting.\")\n",
    "        plain_tree_structure = {}\n",
    "        for node_id, node in self.tree.items():\n",
    "            plain_tree_structure[node_id] = {\n",
    "                'node_id': node.node_id,\n",
    "                'parent_id': node.parent_id,\n",
    "                # Ensure split_on is a plain structure (tuple of string and dict)\n",
    "                'split_on': node.split_on,\n",
    "                'is_leaf': node.is_leaf,\n",
    "                # Ensure prediction is serializable (should be int or None)\n",
    "                'prediction': node.prediction, # Should be int or None already\n",
    "                # Children dictionary keys (branch_id) and values (child_node_id) are already plain types\n",
    "                'children': node.children\n",
    "            }\n",
    "        logger.debug(f\"Converted tree structure to plain dictionary with {len(plain_tree_structure)} nodes.\")\n",
    "\n",
    "        # Broadcast the plain tree structure\n",
    "        logger.debug(\"Broadcasting plain tree structure.\")\n",
    "        bc_tree = self.spark.sparkContext.broadcast(plain_tree_structure)\n",
    "        logger.debug(\"Broadcasted plain tree structure.\")\n",
    "        # Use the enhanced traversal function\n",
    "        udf_pred = F.udf(_enhanced_mk_traverse(bc_tree), IntegerType())\n",
    "        logger.debug(\"Created prediction UDF.\")\n",
    "        logger.debug(\"Applying prediction UDF and coalescing results.\")\n",
    "        out = (\n",
    "            df.withColumn(\"pred\", udf_pred(\"time_series\"))\n",
    "            .withColumn(\"prediction\", F.coalesce(\"pred\", F.lit(self._maj)))\n",
    "            .drop(\"pred\")\n",
    "        )\n",
    "        out_count = out.count() # Trigger action\n",
    "        logger.debug(f\"Prediction applied. Output DataFrame row count: {out_count}\")\n",
    "\n",
    "        bc_tree.unpersist(False)\n",
    "        logger.debug(\"Broadcasted tree unpersisted.\")\n",
    "\n",
    "        # Select relevant output columns\n",
    "        sel = [\"row_id\", \"time_series\"] + ([\"true_label\"] if \"true_label\" in out.columns else []) + [\"prediction\"]\n",
    "        logger.debug(f\"Selecting final columns: {sel}\")\n",
    "        final_output_df = out.select(*sel)\n",
    "\n",
    "        logger.debug(\"predict finished.\")\n",
    "        return final_output_df\n",
    "\n",
    "    def print_tree(self) -> str:\n",
    "        \"\"\"Return a human-readable representation (driver-side).\"\"\"\n",
    "        logger.debug(\"print_tree started.\")\n",
    "        lines = []\n",
    "\n",
    "        def rec(nid: int, depth: int):\n",
    "            nd = self.tree.get(nid)\n",
    "            if nd is None:\n",
    "                lines.append(\"  \" * depth + f\"#{nid} MISSING\")\n",
    "                return\n",
    "            ind = \"  \" * depth\n",
    "            if nd.is_leaf:\n",
    "                lines.append(f\"{ind}Leaf {nid} → {nd.prediction}\")\n",
    "            else:\n",
    "                meas, ex = nd.split_on or (None, {})\n",
    "                lines.append(f\"{ind}Node {nid} split={meas} labels={list(ex.keys())}\")\n",
    "                for lbl, cid in sorted(nd.children.items()):\n",
    "                    lines.append(f\"{ind}  ├─ lbl={lbl} → child {cid}\")\n",
    "                    rec(cid, depth + 2)\n",
    "\n",
    "        rec(0, 0)\n",
    "        tree_str = \"\\n\".join(lines)\n",
    "        logger.debug(\"print_tree finished.\")\n",
    "        return tree_str\n",
    "\n",
    "    def save_tree(self, path: str):\n",
    "        \"\"\"Pickle the *entire* manager (tree + params) to a file.\"\"\"\n",
    "        logger.debug(f\"save_tree started. Path: {path}\")\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, \"wb\") as fh:\n",
    "            pickle.dump({\n",
    "                \"max_depth\": self.max_depth,\n",
    "                \"min_samples\": self.min_samples,\n",
    "                \"k\": self.k,\n",
    "                \"tree\": self.tree, # Tree contains TreeNode namedtuples\n",
    "                \"_next_id\": self._next_id,\n",
    "                \"_maj\": self._maj,\n",
    "            }, fh)\n",
    "        logger.debug(f\"Tree saved successfully to {path}.\")\n",
    "        logger.debug(\"save_tree finished.\")\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_tree(cls, spark: SparkSession, path: str) -> \"GlobalModelManager\":\n",
    "        logger.debug(f\"load_tree started. Path: {path}\")\n",
    "        try:\n",
    "            with open(path, \"rb\") as fh:\n",
    "                data: Dict[str, Any] = pickle.load(fh)\n",
    "            logger.debug(\"Data loaded successfully from pickle.\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Model file not found at {path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load tree from {path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        dummy_conf = {\n",
    "            \"tree_params\": {\n",
    "                \"max_depth\": data.get(\"max_depth\"), # Use .get for safety\n",
    "                \"min_samples_split\": data.get(\"min_samples\", 2), # Use .get with default\n",
    "                \"n_splitters\": data.get(\"k\", 5), # Use .get with default\n",
    "            }\n",
    "        }\n",
    "        logger.debug(f\"Loaded hyperparameters: {dummy_conf['tree_params']}\")\n",
    "\n",
    "        inst = cls(spark, dummy_conf)\n",
    "        inst.tree = data.get(\"tree\", {}) # Use .get with default\n",
    "        inst._next_id = data.get(\"_next_id\", 1) # Use .get with default\n",
    "        inst._maj = data.get(\"_maj\", 1) # Use .get with default\n",
    "        logger.debug(f\"Instance created and state restored. Root node exists: {0 in inst.tree}\")\n",
    "        logger.debug(\"load_tree finished.\")\n",
    "        return inst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e6098",
   "metadata": {},
   "source": [
    "## Prediction Logic\n",
    "\n",
    "### Global Model\n",
    "`predict_with_global_prox_tree()` validates and uses a global model to generate predictions on a Spark DataFrame, renaming columns as needed for evaluation.\n",
    "\n",
    "### Local Model\n",
    "`PredictionManager` broadcasts the local ensemble to workers and uses a `pandas_udf` to apply AEON’s `.predict()` on partitioned data, returning predictions in a new column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606f1202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up a logger for this external function if needed\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def predict_with_global_prox_tree(global_tree_model, data_df: DataFrame) -> DataFrame:\n",
    "\n",
    "    logger.info(\"Starting external prediction with GlobalProxTree.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if not (hasattr(global_tree_model, 'predict') and callable(global_tree_model.predict) and hasattr(global_tree_model, 'spark')):\n",
    "        model_type_name = type(global_tree_model).__name__ if global_tree_model is not None else \"None\"\n",
    "        logger.error(f\"Invalid model type provided. Expected GlobalProxTree-like object, got {model_type_name}.\")\n",
    "        raise TypeError(f\"Invalid model type provided to predict_with_global_prox_tree. Expected GlobalProxTree-like object.\")\n",
    "\n",
    "    predictions_df = global_tree_model.predict(data_df)\n",
    "\n",
    "\n",
    "    if \"true_label\" in predictions_df.columns and \"label\" not in predictions_df.columns:\n",
    "        logger.debug(\"Renaming 'true_label' to 'label' in predictions DataFrame for evaluation compatibility.\")\n",
    "        predictions_df = predictions_df.withColumnRenamed(\"true_label\", \"label\")\n",
    "\n",
    "    if \"prediction\" not in predictions_df.columns:\n",
    "         logger.error(\"GlobalProxTree.predict did not return a 'prediction' column.\")    \n",
    "    logger.info(\"Finished external prediction with GlobalProxTree.\")\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "\n",
    "class PredictionManager:\n",
    "    def __init__(self, spark, ensemble: ProximityForest):\n",
    "        \"\"\"\n",
    "        Initialize with a trained ProximityForest model.\n",
    "        Args:\n",
    "            spark: Spark session\n",
    "            ensemble: Trained model from LocalModelManager.train_ensemble()\n",
    "        \n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.ensemble = ensemble\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.addHandler(logging.StreamHandler())\n",
    "        self.logger.setLevel(logging.ERROR)\n",
    "        \n",
    "        # Basic model validation\n",
    "        if not ensemble or not hasattr(ensemble, 'is_fitted') or not ensemble.is_fitted_:\n",
    "            raise ValueError(\"Model is not trained. First call LocalModelManager.train_ensemble()\")\n",
    "\n",
    "\n",
    "    def _create_predict_udf(self):\n",
    "        \"\"\"Create Spark UDF for making predictions.\"\"\"\n",
    "        # Broadcast model to all workers\n",
    "        broadcast_model = self.spark.sparkContext.broadcast(self.ensemble)\n",
    "        \n",
    "        @pandas_udf(DoubleType())\n",
    "        def predict_udf(features: pd.Series) -> pd.Series:\n",
    "            \"\"\"Converts features to AEON format and makes predictions.\"\"\"\n",
    "            def predict_single(feature_array):\n",
    "                try:\n",
    "                    # Reshape to AEON's expected format: (samples, channels, features)\n",
    "                    X = np.ascontiguousarray(feature_array).reshape(1, 1, -1)\n",
    "                    return float(broadcast_model.value.predict(X)[0])\n",
    "                except Exception as e:\n",
    "                    print(f\"Prediction error: {e}\")\n",
    "                    return float(-999)\n",
    "  \n",
    "            return features.apply(predict_single)\n",
    "            \n",
    "        return predict_udf\n",
    "\n",
    "    def generate_predictions_local(self, test_df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        \"\"\"\n",
    "        We take our test data and add a new column to it that will hold the predictions.        \n",
    "\n",
    "        \"\"\"\n",
    "        # First, gotta make sure we actually have some models to use!\n",
    "        if not self.ensemble:\n",
    "            raise ValueError(\"No models available for prediction\")\n",
    "\n",
    "        feature_cols = [col for col in test_df.columns if col != \"label\"]\n",
    "        \n",
    "        test_df = test_df.withColumn(\n",
    "            \"features\", \n",
    "            F.array(*[F.col(c).cast(\"double\") for c in feature_cols])\n",
    "        )\n",
    "        \n",
    "        predict_udf = self._create_predict_udf()\n",
    "      \n",
    "        predictions_df = test_df.withColumn(\n",
    "            \"prediction\", \n",
    "            predict_udf(\"features\")  \n",
    "        ).drop(\"features\")\n",
    "        return predictions_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e6d2d",
   "metadata": {},
   "source": [
    "## PipelineController_Loop: Orchestrating End-to-End Model Training\n",
    "\n",
    "The `PipelineController_Loop` class coordinates the entire model training pipeline for both global and local Proximity Tree models using Apache Spark. It supports dynamic partitioning and looped experimentation based on configuration.\n",
    "\n",
    "### Key Responsibilities:\n",
    "- **Spark Setup**: Dynamically configures a local or Databricks SparkSession. Verifies data paths and manages module dependencies.\n",
    "- **Pipeline Execution**: Iteratively runs ingestion, preprocessing, training, prediction, and evaluation for both model types.\n",
    "- **Model Training**:\n",
    "  - **Global**: Trains a single Proximity Tree using the full dataset with distributed logic.\n",
    "  - **Local**: Trains multiple Proximity Trees on partitioned data in parallel, aggregating them into a Proximity Forest.\n",
    "- **Evaluation & Logging**: Tracks runtime, logs metrics, and saves performance reports.\n",
    "- **Persistence**: Stores models and evaluation logs after each iteration with timestamped filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb83c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineController_Loop:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the controller with the pipeline configuration.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.spark = None # SparkSession managed once for the entire run\n",
    "        self.ingestion_config = {} # Populated during _setup_spark\n",
    "        \n",
    "        # Logger setup\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "            if self.logger.level == logging.NOTSET:\n",
    "                 self.logger.setLevel(logging.INFO) \n",
    "            # Prevent duplicate messages if root logger also has handlers\n",
    "            self.logger.propagate = False \n",
    "\n",
    "    def _setup_spark(self):\n",
    "\n",
    "        # Configure Spark Session only if one doesn't exist \n",
    "        if self.spark is None:\n",
    "            try:\n",
    "                if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "                    # Databricks environment\n",
    "                    self.spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "                    print(\"\\nUsing Databricks Spark session.\")\n",
    "                    self.ingestion_config = {\n",
    "                        \"data_path\": self.config.get(\"databricks_data_path\", \"/mnt/2025-team6/fulldataset_ECG5000.csv\"),\n",
    "                        \"data_percentage\": self.config.get(\"data_percentage\", 0.05) \n",
    "                    }\n",
    "                else:\n",
    "                    # Local environment setup\n",
    "                    # Stop existing local session if present before creating new one\n",
    "                    existing_spark = SparkSession.getActiveSession()\n",
    "                    if existing_spark:\n",
    "                         self.logger.info(\"Stopping existing Spark session before starting new one.\")\n",
    "                         existing_spark.stop()\n",
    "\n",
    "                    self.spark = SparkSession.builder \\\n",
    "                        .appName(f\"LocalPipeline_Run_{time.time()}\") \\\n",
    "                        .master(\"local[6]\") \\\n",
    "                        .config(\"spark.driver.memory\", \"12g\") \\\n",
    "                        .config(\"spark.executor.memory\", \"12g\") \\\n",
    "                        .config(\"spark.driver.maxResultSize\", \"12g\") \\\n",
    "                        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "                        .getOrCreate()\n",
    "                    print(\"\\nUsing local Spark session.\")\n",
    "\n",
    "                    # --- Construct Local Data Path (Robustly) ---\n",
    "                    project_root = None\n",
    "                    current_dir = None\n",
    "                    try:\n",
    "                         # __file__ is the path of the current script (controller_loop.py)\n",
    "                         current_script_path = os.path.abspath(__file__) \n",
    "                         current_dir = pathlib.Path(current_script_path).parent # Directory containing this script (src)\n",
    "                         # Assumes script is in src, parent is code, parent.parent is project root\n",
    "                         project_root = current_dir.parent.parent.resolve() \n",
    "                         self.logger.info(f\"Project root determined using __file__: {project_root}\")\n",
    "                    except NameError:\n",
    "                         # Fallback if __file__ is not defined (e.g., interactive/notebook)\n",
    "                         # *** CORRECTED FALLBACK LOGIC ***\n",
    "                         # Assume CWD is the directory containing the notebook/where python was launched\n",
    "                         cwd = pathlib.Path(os.getcwd()).resolve() \n",
    "                         self.logger.warning(f\"__file__ not defined, using CWD: {cwd}\")\n",
    "                         # Check if CWD looks like the 'src' directory\n",
    "                         if cwd.name == 'src':\n",
    "                              project_root = cwd.parent.parent.resolve() # Go up two levels\n",
    "                              current_dir = cwd # Set current_dir for module loading later\n",
    "                              self.logger.info(f\"Assuming CWD is 'src', project root set to: {project_root}\")\n",
    "                         # Check if CWD looks like the 'code' directory\n",
    "                         elif cwd.name == 'code':\n",
    "                              project_root = cwd.parent.resolve() # Go up one level\n",
    "                              current_dir = cwd / \"src\" # Assume src exists for module loading\n",
    "                              self.logger.info(f\"Assuming CWD is 'code', project root set to: {project_root}\")\n",
    "                         else: # Otherwise, assume CWD *is* the project root\n",
    "                              project_root = cwd \n",
    "                              current_dir = project_root / \"src\" # Assume src exists for module loading\n",
    "                              self.logger.info(f\"Assuming CWD is project root: {project_root}\")\n",
    "                         \n",
    "                    # Ensure project_root was determined\n",
    "                    if project_root is None:\n",
    "                         self.logger.error(\"FATAL: Could not determine project root directory.\")\n",
    "                         if self.spark: self.spark.stop(); self.spark = None\n",
    "                         return False\n",
    "\n",
    "                    local_data_file = self.config.get(\"local_data_path\", \"fulldataset_ECG5000.csv\").lstrip('/\\\\') \n",
    "                    final_data_path_os = project_root / local_data_file # Use pathlib's / operator\n",
    "                    \n",
    "                    if not final_data_path_os.exists():\n",
    "                         self.logger.error(f\"FATAL: Constructed data path does NOT exist: {final_data_path_os}\")\n",
    "                         # Stop the newly created session if path is invalid\n",
    "                         if self.spark: self.spark.stop(); self.spark = None\n",
    "                         return False # Indicate setup failure\n",
    "                    else:\n",
    "                         self.logger.info(f\"Verified data path exists: {final_data_path_os}\") \n",
    "                         # Convert OS path to file URI for Spark AFTER verification\n",
    "                         final_data_path_uri = final_data_path_os.as_uri() \n",
    "\n",
    "                    self.ingestion_config = {\n",
    "                        \"data_path\": final_data_path_uri, \n",
    "                        \"data_percentage\": self.config.get(\"data_percentage\", 1.0) \n",
    "                    }\n",
    "                    self.logger.info(f\"Data path set in ingestion_config: {self.ingestion_config['data_path']}\")\n",
    "                    self.logger.info(f\"Local data percentage set to: {self.ingestion_config['data_percentage']}\")\n",
    "                    # --- End Construct Local Data Path ---\n",
    "\n",
    "                # Add Python module dependencies for local runs (needs to be done only once per session)\n",
    "                if \"DATABRICKS_RUNTIME_VERSION\" not in os.environ and self.spark:\n",
    "                    modules_to_add = ['global_model_manager.py'] # Add other required modules if needed\n",
    "                    try:\n",
    "                        # Use the current_dir determined earlier (should be src)\n",
    "                        if current_dir is None or not current_dir.exists() or not current_dir.is_dir():\n",
    "                             # Fallback if current_dir wasn't determined correctly\n",
    "                             current_dir = project_root / \"src\" \n",
    "                             self.logger.warning(f\"Re-assuming 'src' directory for module loading: {current_dir}\")\n",
    "                             if not current_dir.exists():\n",
    "                                  raise FileNotFoundError(\"Cannot find assumed 'src' directory for module loading.\")\n",
    "                             \n",
    "                        for module_name in modules_to_add:\n",
    "                             module_path = current_dir / module_name # Path relative to src\n",
    "                             if module_path.exists():\n",
    "                                 self.spark.sparkContext.addPyFile(str(module_path)) # addPyFile needs string path\n",
    "                                 self.logger.debug(f\"Added {module_path} to SparkContext pyFiles.\") \n",
    "                             else:\n",
    "                                  self.logger.error(f\"Could not find module {module_name} at {module_path}\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Failed to add Python module to SparkContext: {e}\")\n",
    "                \n",
    "                return True # Indicate successful setup\n",
    "\n",
    "            except Exception as e:\n",
    "                 self.logger.error(f\"Error during Spark setup: {e}\", exc_info=True)\n",
    "                 if self.spark: self.spark.stop(); self.spark = None # Ensure cleanup on error\n",
    "                 return False # Indicate setup failure\n",
    "        else:\n",
    "             # Spark session already exists\n",
    "             self.logger.debug(\"Spark session already exists.\")\n",
    "             return True\n",
    "            \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executes the main pipeline loop for model training and evaluation.\n",
    "        Iterates through specified partition counts, running the full pipeline (ingestion, preprocessing, training, eval, save) in each iteration.\n",
    "        \"\"\"\n",
    "        # --- Determine number of iterations and which models to run ---\n",
    "        number_iterations_global, number_iterations_local = 0, 0\n",
    "        run_local = self.config.get(\"local_model_config\", {}).get(\"test_local_model\", False)\n",
    "        run_global = self.config.get(\"global_model_config\", {}).get(\"test_global_model\", False)\n",
    "\n",
    "        if not run_local and not run_global:\n",
    "            self.logger.error(\"No model selected for testing. Set 'test_local_model' or 'test_global_model' to True in config.\")\n",
    "            return \n",
    "\n",
    "        if run_local:\n",
    "            number_iterations_local = self.config.get(\"local_model_config\", {}).get(\"num_partitions\", 0)\n",
    "        if run_global:\n",
    "            # Use the partition count from global config to control loop iterations for the experiment\n",
    "            number_iterations_global = self.config.get(\"global_model_config\", {}).get(\"num_partitions\", 0) \n",
    "            \n",
    "        # Determine the overall maximum number of iterations needed\n",
    "        number_iterations = 0\n",
    "        if run_local: number_iterations = max(number_iterations, number_iterations_local)\n",
    "        if run_global: number_iterations = max(number_iterations, number_iterations_global)\n",
    "\n",
    "        min_iterations = self.config.get(\"min_number_iterarations\", 2)\n",
    "        start_iteration = min_iterations\n",
    "        end_iteration = number_iterations\n",
    "        \n",
    "        if end_iteration < start_iteration:\n",
    "             self.logger.warning(f\"Max iterations ({end_iteration}) is less than min iterations ({start_iteration}). No iterations will run.\")\n",
    "             start_iteration = end_iteration + 1 # Make range empty\n",
    "\n",
    "        # --- Initialize report accumulators ---\n",
    "        all_reports_global = {} \n",
    "        all_reports_local = {}  \n",
    "\n",
    "        # --- Setup Spark Session ONCE before the loop ---\n",
    "        if not self._setup_spark():\n",
    "             self.logger.error(\"Initial Spark setup failed. Aborting run.\")\n",
    "             return\n",
    "        # Check data path after setup\n",
    "        if self.ingestion_config.get(\"data_path\") is None:\n",
    "             self.logger.error(\"Initial data path construction failed. Aborting run.\")\n",
    "             # Spark might have been stopped in _setup_spark if path was invalid\n",
    "             if \"DATABRICKS_RUNTIME_VERSION\" not in os.environ and self.spark: self.spark.stop() \n",
    "             return\n",
    "\n",
    "        # ============================================================\n",
    "        # === Main Iteration Loop ===\n",
    "        # ============================================================\n",
    "        try: # Add try block around the loop for final cleanup\n",
    "            for i in range(start_iteration, end_iteration + 1): \n",
    "                self.logger.info(f\"========== Starting Iteration {i} ==========\")\n",
    "                \n",
    "                iteration_run_local = run_local and i <= number_iterations_local \n",
    "                iteration_run_global = run_global # Global runs in all iterations up to its max if enabled\n",
    "\n",
    "                # --- Setup Modules for Iteration ---\n",
    "                # Spark session is already running\n",
    "                current_datetime = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "                # Initialize modules for this iteration\n",
    "                self.evaluator = Evaluator(track_memory=self.config.get(\"track_memory\", False)) \n",
    "                # Create a deep copy of the config for this iteration to avoid side effects\n",
    "                current_iter_config = json.loads(json.dumps(self.config)) \n",
    "                # Update partition counts in the copied config for this iteration\n",
    "                if iteration_run_local: \n",
    "                     current_iter_config.setdefault(\"local_model_config\", {})[\"num_partitions\"] = i\n",
    "                if iteration_run_global: \n",
    "                     current_iter_config.setdefault(\"global_model_config\", {})[\"num_partitions\"] = i\n",
    "                \n",
    "                # Initialize Preprocessor and Ingestion with the iteration-specific config\n",
    "                self.preprocessor = Preprocessor(config=current_iter_config) \n",
    "                # Ingestion uses the config set during initial _setup_spark\n",
    "                self.ingestion = DataIngestion(spark=self.spark, config=self.ingestion_config) \n",
    "\n",
    "                # Define DataFrame variables for this iteration scope\n",
    "                preprocessed_train_df: DataFrame = None\n",
    "                preprocessed_test_df: DataFrame = None\n",
    "                min_max_values: dict = None\n",
    "\n",
    "                try:\n",
    "                    # --- Data Ingestion (runs every iteration) ---\n",
    "                    self.evaluator.start_timer(\"Ingestion\")\n",
    "                    self.logger.info(f\"Iteration {i}: Loading data...\")\n",
    "                    df = self.ingestion.load_data() \n",
    "                    if df.limit(1).count() == 0:\n",
    "                         self.logger.error(f\"Iteration {i}: Data ingestion resulted in empty DataFrame. Skipping iteration.\")\n",
    "                         continue # Skip to next iteration\n",
    "                    self.evaluator.record_time(\"Ingestion\")\n",
    "\n",
    "                    # --- Split, Calculate Min/Max (runs every iteration) ---\n",
    "                    self.evaluator.start_timer(\"Split_MinMax\")\n",
    "                    min_max_values = compute_min_max(df) \n",
    "                    self.logger.info(f\"Iteration {i}: Computed Min-Max values.\") \n",
    "                    train_df, test_df = randomSplit_stratified_via_sampleBy(df, label_col=\"label\", weights=[0.8, 0.2], seed=123)       \n",
    "                    self.evaluator.record_time(\"Split_MinMax\")\n",
    "                    \n",
    "                    # --- Preprocessing Train Data (runs every iteration, includes repartitioning) ---\n",
    "                    self.evaluator.start_timer(\"Preprocessing_Train\")\n",
    "                    target_partitions = current_iter_config.get(\"local_model_config\" if iteration_run_local else \"global_model_config\", {}).get(\"num_partitions\", \"N/A\")\n",
    "                    self.logger.info(f\"Iteration {i}: Preprocessing train data (target partitions={target_partitions})...\")\n",
    "                    preprocessed_train_df = self.preprocessor.run_preprocessing(train_df, min_max_values) \n",
    "                    train_count = preprocessed_train_df.count() # Action to materialize preprocessing\n",
    "                    self.logger.info(f\"Iteration {i}: Preprocessed train data count: {train_count}\")\n",
    "                    self.evaluator.record_time(\"Preprocessing_Train\")\n",
    "\n",
    "                    # --- Preprocessing Test Data (runs every iteration, includes repartitioning) ---\n",
    "                    self.evaluator.start_timer(\"Preprocessing_Test\")\n",
    "                    self.logger.info(f\"Iteration {i}: Preprocessing test data (target partitions={target_partitions})...\")\n",
    "                    preprocessed_test_df = self.preprocessor.run_preprocessing(test_df, min_max_values)\n",
    "                    test_count = preprocessed_test_df.count() # Action\n",
    "                    self.logger.info(f\"Iteration {i}: Preprocessed test data count: {test_count}\")\n",
    "                    self.evaluator.record_time(\"Preprocessing_Test\")\n",
    "\n",
    "                    if train_count == 0 or test_count == 0:\n",
    "                        self.logger.error(f\"Iteration {i}: Preprocessing resulted in empty train or test set. Skipping model steps.\")\n",
    "                        continue \n",
    "\n",
    "                except Exception as e:\n",
    "                     # Handle potential errors during data loading/preprocessing\n",
    "                     if \"Path does not exist\" in str(e):\n",
    "                          spark_path_attempt = self.ingestion_config.get(\"data_path\", \"N/A\") \n",
    "                          self.logger.error(f\"Iteration {i}: Spark failed to find data file during load. Path: {spark_path_attempt}. Error: {e}\", exc_info=False) \n",
    "                     elif isinstance(e, (ConnectionRefusedError, ConnectionResetError)) or \"Connection reset by peer\" in str(e):\n",
    "                          self.logger.error(f\"Iteration {i}: Connection error during data processing: {e}\", exc_info=True)\n",
    "                     else:\n",
    "                          self.logger.error(f\"Iteration {i}: Error during data processing: {e}\", exc_info=True) \n",
    "                     continue # Skip to next iteration\n",
    "                finally:\n",
    "                     # Clean up intermediate raw dataframes for this iteration\n",
    "                     if 'df' in locals(): del df\n",
    "                     if 'train_df' in locals(): del train_df\n",
    "                     if 'test_df' in locals(): del test_df\n",
    "\n",
    "\n",
    "                # Define variables to hold model/prediction results for the iteration\n",
    "                model_ensamble = None \n",
    "                predictions_df = None \n",
    "                \n",
    "                # =================== GLOBAL MODEL ===================\n",
    "                if iteration_run_global:\n",
    "                    # ... (Global model block remains the same) ...\n",
    "                    self.global_model_manager = None \n",
    "                    model_ensamble = None \n",
    "                    global_report = None \n",
    "                    try:\n",
    "                        # Pass the specific global config for this iteration\n",
    "                        global_config = current_iter_config.get(\"global_model_config\", {}) \n",
    "                        if not global_config:\n",
    "                             self.logger.error(f\"Iteration {i}: global_model_config missing. Skipping global model.\")\n",
    "                        else:\n",
    "                            self.global_model_manager = GlobalModelManager(spark=self.spark, config=global_config) \n",
    "                            \n",
    "                            print(f\"\\nIteration {i}: Train global model......\")\n",
    "                            self.evaluator.start_timer(\"Global_Training\")\n",
    "                            model_ensamble = self.global_model_manager.fit(preprocessed_train_df) \n",
    "                            self.evaluator.record_time(\"Global_Training\")\n",
    "                            print(f\"Iteration {i}: Finish Global Training.\")\n",
    "\n",
    "                            if model_ensamble and hasattr(model_ensamble, 'tree') and len(model_ensamble.tree) > 1:\n",
    "                                self.logger.info(f\"Iteration {i}: Global model training successful.\")\n",
    "                                \n",
    "                                print(f\"\\nIteration {i}: Generate predictions with global model......\")\n",
    "                                self.evaluator.start_timer(\"Global_Prediction\")\n",
    "                                predictions_df = predict_with_global_prox_tree(model_ensamble, preprocessed_test_df) \n",
    "                                self.evaluator.record_time(\"Global_Prediction\")\n",
    "                                print(f\"Iteration {i}: Finish Global Prediction.\")\n",
    "                                \n",
    "                                print(f\"\\nIteration {i}: Global Model Predictions Distribution:\")\n",
    "                                predictions_df.groupBy(\"prediction\").count().show() \n",
    "\n",
    "                                print(f\"\\nIteration {i}: Generate metrics for global model......\")\n",
    "                                global_report, class_names = self.evaluator.log_metrics(predictions_df, model=model_ensamble) \n",
    "                                all_reports_global[str(i)] = global_report \n",
    "                                print(f\"Iteration {i}: Finish Global Evaluation.\")\n",
    "                                \n",
    "                                depth = global_config.get(\"tree_params\", {}).get(\"max_depth\", \"NA\")\n",
    "                                \n",
    "                                model_folder = \"models_global\" \n",
    "                                os.makedirs(model_folder, exist_ok=True) \n",
    "                                model_filename = f\"global_model_iter_{i}_parti_{i}_{current_datetime}_depth_{depth}.pkl\" \n",
    "                                model_save_path = os.path.join(model_folder, model_filename)\n",
    "                                try:\n",
    "                                    self.global_model_manager.save_tree(model_save_path) \n",
    "                                    print(f\"Saved global model to {model_save_path}\")\n",
    "                                except Exception as e: self.logger.error(f\"Failed to save global model {model_filename}: {e}\")\n",
    "\n",
    "                            else:\n",
    "                                self.logger.warning(f\"Iteration {i}: Global model training failed or resulted in trivial tree.\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Iteration {i}: Error during global model processing: {e}\", exc_info=True) \n",
    "                    finally:\n",
    "                        # Clean up global model objects specific to this iteration\n",
    "                        self.global_model_manager = None \n",
    "                        if 'model_ensamble' in locals() and model_ensamble is not None: del model_ensamble\n",
    "                        if not iteration_run_local and 'predictions_df' in locals() and predictions_df is not None: del predictions_df \n",
    "                        self.logger.debug(f\"Iteration {i}: Cleaned up global model objects.\")\n",
    "\n",
    "\n",
    "                # =================== LOCAL MODEL ===================\n",
    "                if iteration_run_local:\n",
    "                    # ... (Local model block remains the same) ...\n",
    "                    self.local_model_manager = None \n",
    "                    model_ensamble = None \n",
    "                    if iteration_run_global and 'predictions_df' in locals(): predictions_df = None \n",
    "                    local_report = None \n",
    "                    try:\n",
    "                        # Pass the specific local config for this iteration\n",
    "                        local_config = current_iter_config.get(\"local_model_config\", {})\n",
    "                        if not local_config:\n",
    "                            self.logger.error(f\"Iteration {i}: local_model_config missing. Skipping local model.\")\n",
    "                        else: \n",
    "                            self.local_model_manager = LocalModelManager(config=local_config) \n",
    "                            \n",
    "                            print(f\"\\nIteration {i}: Train local model with {i} partitions......\")\n",
    "                            self.evaluator.start_timer(\"Local_Training\")\n",
    "                            model_ensamble = self.local_model_manager.train_ensemble(preprocessed_train_df) \n",
    "                            self.evaluator.record_time(\"Local_Training\")\n",
    "                            print(f\"Iteration {i}: Finish Local Training.\")\n",
    "                            \n",
    "                            if model_ensamble is not None and hasattr(model_ensamble, 'trees_') and model_ensamble.trees_:\n",
    "                                self.logger.info(f\"Iteration {i}: Local model training successful.\")\n",
    "                                \n",
    "                                print(f\"\\nIteration {i}: Generate predictions with local model......\")\n",
    "                                self.evaluator.start_timer(\"Local_Prediction\")\n",
    "                                self.predictor = PredictionManager(self.spark, model_ensamble) \n",
    "                                predictions_df = self.predictor.generate_predictions_local(preprocessed_test_df) \n",
    "                                self.evaluator.record_time(\"Local_Prediction\")\n",
    "                                print(f\"Iteration {i}: Finish Local Prediction.\")\n",
    "                            \n",
    "                                print(f\"\\nIteration {i}: Local model Predictions Distribution:\")\n",
    "                                predictions_df.groupBy(\"prediction\").count().show() \n",
    "\n",
    "                                print(f\"\\nIteration {i}: Generate metrics for local model......\")\n",
    "                                local_report, class_names = self.evaluator.log_metrics(predictions_df, model=model_ensamble)\n",
    "                                all_reports_local[str(i)] = local_report \n",
    "                                print(f\"Iteration {i}: Finish Local Evaluation.\")\n",
    "                                \n",
    "                                depth = local_config.get(\"tree_params\", {}).get(\"max_depth\", \"NA\") \n",
    "                                \n",
    "                                model_folder = \"models_local\" \n",
    "                                os.makedirs(model_folder, exist_ok=True) \n",
    "                                model_filename = f\"local_model_iter_{i}_parti_{i}_{current_datetime}_depth_{depth}.pkl\" \n",
    "                                model_save_path = os.path.join(model_folder, model_filename)\n",
    "                                try:\n",
    "                                    with open(model_save_path, 'wb') as f: pickle.dump(model_ensamble, f) \n",
    "                                    print(f\"Saved local model ensemble to {model_save_path}\")\n",
    "                                except Exception as e: self.logger.error(f\"Failed to save local model {model_filename}: {e}\")\n",
    "                                \n",
    "                            else:\n",
    "                                self.logger.warning(f\"Iteration {i}: Local model training failed or resulted in empty ensemble.\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                         self.logger.error(f\"Iteration {i}: Error during local model processing: {e}\", exc_info=True) \n",
    "                    finally:\n",
    "                        # Clean up local model objects specific to this iteration\n",
    "                        self.local_model_manager = None \n",
    "                        if 'model_ensamble' in locals() and model_ensamble is not None: del model_ensamble\n",
    "                        if 'predictions_df' in locals() and predictions_df is not None: del predictions_df\n",
    "                        self.logger.debug(f\"Iteration {i}: Cleaned up local model objects.\")\n",
    "\n",
    "\n",
    "                # --- Iteration End ---\n",
    "                # Cleanup preprocessed DataFrames for this iteration \n",
    "                try:\n",
    "                     if 'preprocessed_train_df' in locals() and preprocessed_train_df is not None: \n",
    "                          # preprocessed_train_df.unpersist() # Caching removed, unpersist not needed\n",
    "                          del preprocessed_train_df\n",
    "                     if 'preprocessed_test_df' in locals() and preprocessed_test_df is not None: \n",
    "                          # preprocessed_test_df.unpersist() # Caching removed, unpersist not needed\n",
    "                          del preprocessed_test_df\n",
    "                except Exception as cleanup_e:\n",
    "                     self.logger.warning(f\"Iteration {i}: Error during DataFrame cleanup: {cleanup_e}\")\n",
    "\n",
    "                \n",
    "                self.logger.info(f\"========== Finished Iteration {i} ==========\")\n",
    "                # Optional delay\n",
    "                if self.config.get('delay_time', 0) > 0 and i < end_iteration : \n",
    "                    print(f\"\\nIteration {i}: Waiting for {self.config['delay_time']} seconds before next iteration...\\n\")\n",
    "                    time.sleep(self.config['delay_time'])\n",
    "\n",
    "                # *** Spark session is NOT stopped here anymore ***\n",
    "\n",
    "            # End of main loop\n",
    "\n",
    "        #  Save Accumulated Reports After Loop ===\n",
    "        # ============================================================\n",
    "        finally: # Use finally to ensure Spark stops even if loop errors out\n",
    "             final_datetime = time.strftime(\"%Y-%m-%d-%H-%M-%S\") \n",
    "            \n",
    "             if all_reports_global:\n",
    "                  report_folder = \"logs\" \n",
    "                  os.makedirs(report_folder, exist_ok=True) \n",
    "                  report_filename_global = f\"report_global_model_ALL_{final_datetime}.json\" \n",
    "                  report_save_path_global = os.path.join(report_folder, report_filename_global)\n",
    "                  try:\n",
    "                      with open(report_save_path_global, \"w\") as f: json.dump(all_reports_global, f, indent=2)\n",
    "                      print(f\"Saved ALL global model reports to {report_save_path_global}\") \n",
    "                  except Exception as e: self.logger.error(f\"Failed to save aggregated global report: {e}\")\n",
    "\n",
    "             if all_reports_local:\n",
    "                  report_folder = \"logs\"\n",
    "                  os.makedirs(report_folder, exist_ok=True) \n",
    "                  report_filename_local = f\"report_local_model_ALL_{final_datetime}.json\" \n",
    "                  report_save_path_local = os.path.join(report_folder, report_filename_local)\n",
    "                  try:\n",
    "                      with open(report_save_path_local, \"w\") as f: json.dump(all_reports_local, f, indent=2)\n",
    "                      print(f\"Saved ALL local model reports to {report_save_path_local}\") \n",
    "                  except Exception as e: self.logger.error(f\"Failed to save aggregated local report: {e}\")\n",
    "\n",
    "\n",
    "             # --- Pipeline End ---\n",
    "             # Final Spark stop if running locally \n",
    "             if \"DATABRICKS_RUNTIME_VERSION\" not in os.environ and self.spark:\n",
    "                  self.spark.stop()\n",
    "                  self.spark = None # Reset attribute\n",
    "                  print(f\"\\nFinal Spark session stopped (local mode)!\")\n",
    "\n",
    "             print(\"\\n--- Pipeline execution finished ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1656b6c",
   "metadata": {},
   "source": [
    "## Centralized Configuration\n",
    "\n",
    "The `config.py` file consolidates all key parameters and constants used throughout the project. It simplifies updates and experimentation by centralizing:\n",
    "\n",
    "- **Data Settings**: File paths for Databricks and local environments, target data usage percentage, and label column name.\n",
    "- **Loop Control**: Minimum iterations and optional delays between runs.\n",
    "- **Local Model Config**: Enables local Proximity Forest training with customizable partition count and hyperparameters for individual trees and the ensemble.\n",
    "- **Global Model Config**: Enables distributed Proximity Tree training with tunable depth, splitters, and minimum sample thresholds.\n",
    "- **Partitioning Behavior**: Controls whether `_partition_id` columns are preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "914c6674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "\"\"\"\n",
    "config.py\n",
    "\n",
    "This file holds configuration settings and constants.\n",
    "It stores paths, hyperparameters, and Spark settings in one place,\n",
    "so they can be easily managed and updated as the project grows.\n",
    "Typically, it's created early on, but it can be refined later.\n",
    "\"\"\"\n",
    "\n",
    "config = {\n",
    "    \"databricks_data_path\" : \"/mnt/2025-team6/fulldataset_ECG5000.csv\",\n",
    "    \"local_data_path\" : \"/fulldataset_ECG5000.csv\",    # Relative to project root\n",
    "    \"label_col\" : \"label\",\n",
    "    \"data_percentage\" : 1.0,\n",
    "    \"min_number_iterarations\" : 2, # Minimum number of iterations for the loop\n",
    "    \"delay_time\" : 3,\n",
    "    \n",
    "    \"local_model_config\": {\n",
    "        \"test_local_model\" : True,\n",
    "        \"num_partitions\": 3,  # loop to this number of partitions - This is the number of partitions for the local model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": 2,  #TODO: NONE\n",
    "            \"min_samples_split\": 5,  # From ProximityTree default\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "        \"forest_params\": {\n",
    "            \"random_state\": 123,\n",
    "            \"n_jobs\": -1  # Use all available cores\n",
    "            }\n",
    "    },\n",
    "    \"global_model_config\": {\n",
    "        \"test_global_model\" : True,\n",
    "        \"num_partitions\": 3,  # loop to this number of partitions - This is the number of partitions for the global model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": 1,  \n",
    "            \"min_samples_split\": 5,  # From ProximityTree defaults\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "    },\n",
    "    \"reserve_partition_id\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3aaac0",
   "metadata": {},
   "source": [
    "## Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2c985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__file__ not defined, using CWD: D:\\repos\\BigData-main\\BigData-1\\code\\src\n",
      "__file__ not defined, using CWD: D:\\repos\\BigData-main\\BigData-1\\code\\src\n",
      "__file__ not defined, using CWD: D:\\repos\\BigData-main\\BigData-1\\code\\src\n",
      "__file__ not defined, using CWD: D:\\repos\\BigData-main\\BigData-1\\code\\src\n",
      "2025-05-06 08:08:50,021 - WARNING - __main__ - __file__ not defined, using CWD: D:\\repos\\BigData-main\\BigData-1\\code\\src\n",
      "Assuming CWD is 'src', project root set to: D:\\repos\\BigData-main\\BigData-1\n",
      "Assuming CWD is 'src', project root set to: D:\\repos\\BigData-main\\BigData-1\n",
      "Assuming CWD is 'src', project root set to: D:\\repos\\BigData-main\\BigData-1\n",
      "Assuming CWD is 'src', project root set to: D:\\repos\\BigData-main\\BigData-1\n",
      "2025-05-06 08:08:50,023 - INFO - __main__ - Assuming CWD is 'src', project root set to: D:\\repos\\BigData-main\\BigData-1\n",
      "Verified data path exists: D:\\repos\\BigData-main\\BigData-1\\fulldataset_ECG5000.csv\n",
      "Verified data path exists: D:\\repos\\BigData-main\\BigData-1\\fulldataset_ECG5000.csv\n",
      "Verified data path exists: D:\\repos\\BigData-main\\BigData-1\\fulldataset_ECG5000.csv\n",
      "Verified data path exists: D:\\repos\\BigData-main\\BigData-1\\fulldataset_ECG5000.csv\n",
      "2025-05-06 08:08:50,025 - INFO - __main__ - Verified data path exists: D:\\repos\\BigData-main\\BigData-1\\fulldataset_ECG5000.csv\n",
      "Data path set in ingestion_config: file:///D:/repos/BigData-main/BigData-1/fulldataset_ECG5000.csv\n",
      "Data path set in ingestion_config: file:///D:/repos/BigData-main/BigData-1/fulldataset_ECG5000.csv\n",
      "Data path set in ingestion_config: file:///D:/repos/BigData-main/BigData-1/fulldataset_ECG5000.csv\n",
      "Data path set in ingestion_config: file:///D:/repos/BigData-main/BigData-1/fulldataset_ECG5000.csv\n",
      "2025-05-06 08:08:50,027 - INFO - __main__ - Data path set in ingestion_config: file:///D:/repos/BigData-main/BigData-1/fulldataset_ECG5000.csv\n",
      "Local data percentage set to: 1.0\n",
      "Local data percentage set to: 1.0\n",
      "Local data percentage set to: 1.0\n",
      "Local data percentage set to: 1.0\n",
      "2025-05-06 08:08:50,029 - INFO - __main__ - Local data percentage set to: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline via controller\n",
      "\n",
      "Using local Spark session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== Starting Iteration 2 ==========\n",
      "========== Starting Iteration 2 ==========\n",
      "========== Starting Iteration 2 ==========\n",
      "========== Starting Iteration 2 ==========\n",
      "2025-05-06 08:08:50,129 - INFO - __main__ - ========== Starting Iteration 2 ==========\n",
      "Iteration 2: Loading data...\n",
      "Iteration 2: Loading data...\n",
      "Iteration 2: Loading data...\n",
      "Iteration 2: Loading data...\n",
      "2025-05-06 08:08:50,131 - INFO - __main__ - Iteration 2: Loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path: file:///D:/repos/BigData-main/BigData-1/fulldataset_ECG5000.csv\n",
      "Loading 100.0% of data\n",
      "Data size: 5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2: Computed Min-Max values.\n",
      "Iteration 2: Computed Min-Max values.\n",
      "Iteration 2: Computed Min-Max values.\n",
      "Iteration 2: Computed Min-Max values.\n",
      "2025-05-06 08:08:51,984 - INFO - __main__ - Iteration 2: Computed Min-Max values.\n",
      "Iteration 2: Preprocessing train data (target partitions=2)...\n",
      "Iteration 2: Preprocessing train data (target partitions=2)...\n",
      "Iteration 2: Preprocessing train data (target partitions=2)...\n",
      "Iteration 2: Preprocessing train data (target partitions=2)...\n",
      "2025-05-06 08:08:52,292 - INFO - __main__ - Iteration 2: Preprocessing train data (target partitions=2)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartitioning to <<<< 2 >>>> workers - partitions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2: Preprocessed train data count: 3990\n",
      "Iteration 2: Preprocessed train data count: 3990\n",
      "Iteration 2: Preprocessed train data count: 3990\n",
      "Iteration 2: Preprocessed train data count: 3990\n",
      "2025-05-06 08:08:53,312 - INFO - __main__ - Iteration 2: Preprocessed train data count: 3990\n",
      "Iteration 2: Preprocessing test data (target partitions=2)...\n",
      "Iteration 2: Preprocessing test data (target partitions=2)...\n",
      "Iteration 2: Preprocessing test data (target partitions=2)...\n",
      "Iteration 2: Preprocessing test data (target partitions=2)...\n",
      "2025-05-06 08:08:53,314 - INFO - __main__ - Iteration 2: Preprocessing test data (target partitions=2)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartitioning to <<<< 2 >>>> workers - partitions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2: Preprocessed test data count: 1010\n",
      "Iteration 2: Preprocessed test data count: 1010\n",
      "Iteration 2: Preprocessed test data count: 1010\n",
      "Iteration 2: Preprocessed test data count: 1010\n",
      "2025-05-06 08:08:54,743 - INFO - __main__ - Iteration 2: Preprocessed test data count: 1010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 2: Train global model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2: Global model training successful.\n",
      "Iteration 2: Global model training successful.\n",
      "Iteration 2: Global model training successful.\n",
      "Iteration 2: Global model training successful.\n",
      "2025-05-06 08:10:13,991 - INFO - __main__ - Iteration 2: Global model training successful.\n",
      "Starting external prediction with GlobalProxTree.\n",
      "Starting external prediction with GlobalProxTree.\n",
      "Starting external prediction with GlobalProxTree.\n",
      "Starting external prediction with GlobalProxTree.\n",
      "2025-05-06 08:10:13,993 - INFO - __main__ - Starting external prediction with GlobalProxTree.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: Finish Global Training.\n",
      "\n",
      "Iteration 2: Generate predictions with global model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished external prediction with GlobalProxTree.\n",
      "Finished external prediction with GlobalProxTree.\n",
      "Finished external prediction with GlobalProxTree.\n",
      "Finished external prediction with GlobalProxTree.\n",
      "2025-05-06 08:10:15,886 - INFO - __main__ - Finished external prediction with GlobalProxTree.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: Finish Global Prediction.\n",
      "\n",
      "Iteration 2: Global Model Predictions Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  569|\n",
      "|         4|   32|\n",
      "|         2|  409|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "Iteration 2: Generate metrics for global model......\n",
      "Iteration 2: Finish Global Evaluation.\n",
      "Saved global model to models_global\\global_model_iter_2_parti_2_2025-05-06-08-08-50_depth_1.pkl\n",
      "\n",
      "Iteration 2: Train local model with 2 partitions......\n",
      "Iteration 2: Finish Local Training.\n",
      "\n",
      "Iteration 2: Generate predictions with local model......\n",
      "Iteration 2: Finish Local Prediction.\n",
      "\n",
      "Iteration 2: Local model Predictions Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       1.0|  572|\n",
      "|       4.0|   20|\n",
      "|       3.0|    1|\n",
      "|       2.0|  417|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "Iteration 2: Generate metrics for local model......\n",
      "Iteration 2: Finish Local Evaluation.\n",
      "Saved local model ensemble to models_local\\local_model_iter_2_parti_2_2025-05-06-08-08-50_depth_2.pkl\n",
      "\n",
      "Iteration 2: Waiting for 3 seconds before next iteration...\n",
      "\n",
      "Data Path: file:///D:/repos/BigData-main/BigData-1/fulldataset_ECG5000.csv\n",
      "Loading 100.0% of data\n",
      "Data size: 5000\n",
      "\n",
      "Repartitioning to <<<< 3 >>>> workers - partitions.\n",
      "Repartitioning to <<<< 3 >>>> workers - partitions.\n",
      "\n",
      "Iteration 3: Train global model......\n",
      "Iteration 3: Finish Global Training.\n",
      "\n",
      "Iteration 3: Generate predictions with global model......\n",
      "Iteration 3: Finish Global Prediction.\n",
      "\n",
      "Iteration 3: Global Model Predictions Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  581|\n",
      "|         2|  429|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "Iteration 3: Generate metrics for global model......\n",
      "Iteration 3: Finish Global Evaluation.\n",
      "Saved global model to models_global\\global_model_iter_3_parti_3_2025-05-06-08-12-55_depth_1.pkl\n",
      "\n",
      "Iteration 3: Train local model with 3 partitions......\n",
      "Iteration 3: Finish Local Training.\n",
      "\n",
      "Iteration 3: Generate predictions with local model......\n",
      "Iteration 3: Finish Local Prediction.\n",
      "\n",
      "Iteration 3: Local model Predictions Distribution:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       1.0|  606|\n",
      "|       3.0|    1|\n",
      "|       2.0|  403|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "Iteration 3: Generate metrics for local model......\n",
      "Iteration 3: Finish Local Evaluation.\n",
      "Saved local model ensemble to models_local\\local_model_iter_3_parti_3_2025-05-06-08-12-55_depth_2.pkl\n",
      "Saved ALL global model reports to logs\\report_global_model_ALL_2025-05-06-08-16-54.json\n",
      "Saved ALL local model reports to logs\\report_local_model_ALL_2025-05-06-08-16-54.json\n",
      "\n",
      "Final Spark session stopped (local mode)!\n",
      "\n",
      "--- Pipeline execution finished ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting pipeline via controller\")\n",
    "config = config\n",
    "controller = PipelineController_Loop(config)\n",
    "controller.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
