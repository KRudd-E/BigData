{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f384d08f-23bf-4538-b6f4-c723890fa5a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# Install required packages if running in an environment where they might be missing\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "required_packages = ['aeon', 'psutil', 'pyspark', 'numpy', 'pandas', 'numba'] # Added numba just in case\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        try:\n",
    "            # Use Databricks' recommended %pip magic command if available, otherwise use subprocess\n",
    "            if 'dbutils' in locals() or 'ipykernel' in sys.modules:\n",
    "                 print(f\"Using %pip install {package}\")\n",
    "                 # Cannot execute %pip directly here, user needs to run '%pip install package' in a separate cell if needed.\n",
    "                 # Fallback to subprocess for general environments\n",
    "                 subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            else:\n",
    "                 subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"{package} installed successfully.\")\n",
    "            # Re-import after installation if needed immediately (usually not necessary for top-level scripts)\n",
    "            # __import__(package) \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to install {package}. Please install it manually (e.g., using '%pip install {package}' in Databricks). Error: {e}\")\n",
    "            # Optionally raise the error if installation is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0acd7c7d-d07c-4239-bfe2-89e583418215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "import osos.environ['NUMBA_CACHE_DIR'] = '/tmp/numba_cache'\n",
    "\n",
    "Set environment variable to prevent Numba caching issues on executors. **Run this cell before importing other project modules.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "180f1fe8-068c-4482-81e2-d2d05678d9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable Numba caching via environment variable\n",
    "# This needs to be set *before* Numba tries to cache functions, \n",
    "# especially when code is sent to Spark executors.\n",
    "os.environ['NUMBA_DISABLE_CACHING'] = '1' \n",
    "print(\"Attempted to disable Numba caching by setting NUMBA_DISABLE_CACHING=1\")\n",
    "# Verify if set (optional)\n",
    "print(f\"NUMBA_DISABLE_CACHING set to: {os.environ.get('NUMBA_DISABLE_CACHING')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4e95d0bb-536d-40c8-bbde-cd45cad7c28a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame, Row, SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    ")\n",
    "\n",
    "from aeon.classification.distance_based import (\n",
    "    ProximityForest,\n",
    "    ProximityTree,\n",
    ")\n",
    "\n",
    "from data_ingestion import DataIngestion\n",
    "from evaluation import Evaluator\n",
    "\n",
    "from utilities import (\n",
    "    compute_min_max,\n",
    "    randomSplit_dist,\n",
    "    randomSplit_stratified_via_sampleBy,\n",
    "    show_compact,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22fae8f-6b9d-4e13-bb38-fea6621101d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This module does some basic cleaning and transformations on a Spark DataFrame.\n",
    "It comes after the data ingestion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9b81d69f-3e80-4af1-85da-ddaa55642cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    This class cleans up our ECG data.\n",
    "    It handles missing rows, splits the label from the features, and\n",
    "    does a simple normalization on the feature columns.\n",
    "    It returns a Spark DataFrame ready for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "\n",
    "    def handle_missing_values(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\" Drops rows where every column is null \"\"\"\n",
    "        return df.dropna(how=\"all\")\n",
    "   \n",
    "    \n",
    "    def normalize(self, df: DataFrame, min_max: dict, preserve_partition_id: bool = False) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Normalizes feature columns using precomputed global min and max values.\n",
    "        \"\"\"\n",
    "        feature_cols = [col for col in df.columns if (col != \"label\" and col != \"_partition_id\")]\n",
    "        \n",
    "        normalized_cols = []\n",
    "        for col in feature_cols:\n",
    "            min_val, max_val = min_max[col]\n",
    "            if max_val != min_val:\n",
    "                expr = (F.col(col) - F.lit(min_val)) / (F.lit(max_val - min_val))\n",
    "            else:\n",
    "                expr = F.lit(0.0)  # if all values identical, set to zero or keep original\n",
    "            normalized_cols.append(expr.alias(col))\n",
    "        \n",
    "        cols_to_select = [\"label\"]\n",
    "        if \"_partition_id\" in df.columns:\n",
    "            cols_to_select.append(\"_partition_id\")\n",
    "\n",
    "        return df.select(*normalized_cols, *cols_to_select)\n",
    "\n",
    "    def _repartition_data_NotBalanced(self, df: DataFrame, preserve_partition_id: bool = False) -> DataFrame:\n",
    "        if \"num_partitions\" in self.config:\n",
    "            new_parts = self.config[\"num_partitions\"]  # \n",
    "            #self.logger.info(f\"Repartitioning data to {new_parts} parts\")\n",
    "            return df.repartition(new_parts)\n",
    "        return df\n",
    "    \n",
    "    def _repartition_data_Balanced(self, df: DataFrame, preserve_partition_id: bool = False) -> DataFrame:\n",
    "        \n",
    "        if (\"num_partitions\" in self.config[\"local_model_config\"] \\\n",
    "            or \"num_partitions\" in self.config[\"global_model_config\"]) \\\n",
    "            and \"label_col\" in self.config:\n",
    "            \n",
    "            if self.config[\"local_model_config\"][\"test_local_model\"] is True:\n",
    "                num_parts = self.config[\"local_model_config\"][\"num_partitions\"]\n",
    "            else:\n",
    "                num_parts = self.config[\"global_model_config\"][\"num_partitions\"]\n",
    "                \n",
    "            label_col = self.config[\"label_col\"]\n",
    "            # self.logger.info(f\"Stratified repartitioning into {num_parts} partitions\")\n",
    "            \n",
    "            # Assign partition IDs (0 to num_parts-1 per class)\n",
    "            # Subtracting 1 so that modulo is computed from 0\n",
    "            window = Window.partitionBy(label_col) \\\n",
    "                            .orderBy(F.rand())          #  one shuffles to group all rows of each label together so we can number them\n",
    "                            \n",
    "            df = df.withColumn(\"_partition_id\", ((F.row_number().over(window) - 1) % num_parts).cast(\"int\"))\n",
    "            \n",
    "            # Force exact number of partitions using partition_id\n",
    "            df = df.repartition(num_parts, F.col(\"_partition_id\"))          # one shuffles to repartition by _partition_id to ensure we have num_parts partitions\n",
    "            print(f'Repartitioning to <<<< {num_parts} >>>> workers - partitions.')\n",
    "            \n",
    "            if not preserve_partition_id:\n",
    "                df = df.drop(\"_partition_id\")\n",
    "            return df\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def run_preprocessing(self, df: DataFrame, min_max) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): Input DataFrame to be preprocessed. pyspark Sql DataFrame.\n",
    "        Returns:\n",
    "            DataFrame: Preprocessed DataFrame ready for training.\n",
    "        \n",
    "            \n",
    "        Run all preprocessing steps in order:\n",
    "         1. Drop rows that are completely null.\n",
    "         2. Repartition the data : shuffle the data to balance the partitions.\n",
    "         3. Normalize the feature columns.\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.handle_missing_values(df)\n",
    "        \n",
    "        if self.config[\"local_model_config\"][\"test_local_model\"] is True:\n",
    "            df = self._repartition_data_Balanced(df, preserve_partition_id = self.config[\"reserve_partition_id\"])\n",
    "        elif self.config[\"global_model_config\"][\"test_global_model\"] is True:\n",
    "            df = self._repartition_data_NotBalanced(df, preserve_partition_id = self.config[\"reserve_partition_id\"])\n",
    "        else:\n",
    "            raise ValueError(\"Preprocessing error.\")\n",
    "        \n",
    "        df = self.normalize(df, min_max, preserve_partition_id = self.config[\"reserve_partition_id\"])\n",
    "        \n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "709af5de-783b-48c2-b6ea-688a4fe6f37a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up a logger for this external function if needed\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def predict_with_global_prox_tree(global_tree_model, data_df: DataFrame) -> DataFrame:\n",
    "\n",
    "    logger.info(\"Starting external prediction with GlobalProxTree.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if not (hasattr(global_tree_model, 'predict') and callable(global_tree_model.predict) and hasattr(global_tree_model, 'spark')):\n",
    "        model_type_name = type(global_tree_model).__name__ if global_tree_model is not None else \"None\"\n",
    "        logger.error(f\"Invalid model type provided. Expected GlobalProxTree-like object, got {model_type_name}.\")\n",
    "        raise TypeError(f\"Invalid model type provided to predict_with_global_prox_tree. Expected GlobalProxTree-like object.\")\n",
    "\n",
    "    predictions_df = global_tree_model.predict(data_df)\n",
    "\n",
    "    \n",
    "\n",
    "        # --- Ensure the output DataFrame has a 'label' column for evaluation ---\n",
    "    # The GlobalProxTree.predict method returns 'true_label' and 'prediction'.\n",
    "    # The Evaluator expects 'label' and 'prediction'.\n",
    "    # Rename 'true_label' to 'label' if it exists and 'label' does not.\n",
    "    if \"true_label\" in predictions_df.columns and \"label\" not in predictions_df.columns:\n",
    "        logger.debug(\"Renaming 'true_label' to 'label' in predictions DataFrame for evaluation compatibility.\")\n",
    "        predictions_df = predictions_df.withColumnRenamed(\"true_label\", \"label\")\n",
    "    # If 'label' already exists, no renaming is needed.\n",
    "    # If neither exists, the Evaluator will log a warning.\n",
    "\n",
    "    # Ensure 'prediction' column exists (should be returned by GlobalProxTree.predict)\n",
    "    if \"prediction\" not in predictions_df.columns:\n",
    "         logger.error(\"GlobalProxTree.predict did not return a 'prediction' column.\")\n",
    "         # Depending on severity, you might want to raise an error here\n",
    "         # raise ValueError(\"Prediction column missing from GlobalProxTree output.\")\n",
    "         # For now, we'll let it proceed, and the Evaluator will likely fail gracefully.\n",
    "    \n",
    "    logger.info(\"Finished external prediction with GlobalProxTree.\")\n",
    "\n",
    "    return predictions_df\n",
    "\n",
    "\n",
    "class PredictionManager:\n",
    "    def __init__(self, spark, ensemble: ProximityForest):\n",
    "        \"\"\"\n",
    "        Initialize with a trained ProximityForest model.\n",
    "        Args:\n",
    "            spark: Spark session\n",
    "            ensemble: Trained model from LocalModelManager.train_ensemble()\n",
    "        \n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.ensemble = ensemble\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.addHandler(logging.StreamHandler())\n",
    "        self.logger.setLevel(logging.ERROR)\n",
    "        \n",
    "        # Basic model validation\n",
    "        if not ensemble or not hasattr(ensemble, 'is_fitted') or not ensemble.is_fitted_:\n",
    "            raise ValueError(\"Model is not trained. First call LocalModelManager.train_ensemble()\")\n",
    "\n",
    "\n",
    "    def _create_predict_udf(self):\n",
    "        \"\"\"Create Spark UDF for making predictions.\"\"\"\n",
    "        # Broadcast model to all workers\n",
    "        broadcast_model = self.spark.sparkContext.broadcast(self.ensemble)\n",
    "        \n",
    "        @pandas_udf(DoubleType())\n",
    "        def predict_udf(features: pd.Series) -> pd.Series:\n",
    "            \"\"\"Converts features to AEON format and makes predictions.\"\"\"\n",
    "            def predict_single(feature_array):\n",
    "                try:\n",
    "                    # Reshape to AEON's expected format: (samples, channels, features)\n",
    "                    X = np.ascontiguousarray(feature_array).reshape(1, 1, -1)\n",
    "                    return float(broadcast_model.value.predict(X)[0])\n",
    "                except Exception as e:\n",
    "                    print(f\"Prediction error: {e}\")\n",
    "                    return float(-999)\n",
    "  \n",
    "            return features.apply(predict_single)\n",
    "            \n",
    "        return predict_udf\n",
    "\n",
    "    def generate_predictions_local(self, test_df: DataFrame) -> DataFrame:\n",
    "        \n",
    "        \"\"\"\n",
    "        We take our test data and add a new column to it that will hold the predictions.        \n",
    "\n",
    "        \"\"\"\n",
    "        # First, gotta make sure we actually have some models to use!\n",
    "        if not self.ensemble:\n",
    "            raise ValueError(\"No models available for prediction\")\n",
    "\n",
    "        feature_cols = [col for col in test_df.columns if col != \"label\"]\n",
    "        \n",
    "        test_df = test_df.withColumn(\n",
    "            \"features\", \n",
    "            F.array(*[F.col(c).cast(\"double\") for c in feature_cols])\n",
    "        )\n",
    "        \n",
    "        predict_udf = self._create_predict_udf()\n",
    "      \n",
    "        predictions_df = test_df.withColumn(\n",
    "            \"prediction\", \n",
    "            predict_udf(\"features\")  \n",
    "        ).drop(\"features\")\n",
    "        return predictions_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e42c0464-bdc0-41a7-8dce-4309e60db334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This file is in charge of training our local models.\n",
    "It takes the preprocessed Spark DataFrame and splits it into parts.\n",
    "For each part, it trains a Proximity Tree model.\n",
    "Then, it gathers all the models into one Proximity Forest ensemble that we can use later to make predictions.\n",
    "\n",
    "NOTE: Trains models in parallel across Spark worker nodes. Number of trees = number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "19d9a6ab-c151-4788-bd62-d19cfc275fb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class LocalModelManager:\n",
    "    \"\"\"\n",
    "    This class handles training local models (Proximity Trees) on chunks of our data and then\n",
    "    puts them together into an  Proximity Forest ensemble.\n",
    "    \n",
    "    The steps are pretty simple:\n",
    "      1. Get a preprocessed Spark DataFrame.\n",
    "      2. Split it into parts.\n",
    "      3. Train a Proximity Tree  model on each part .\n",
    "      4. Then, it gathers all the trees into one Proximity Forest ensemble\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        \"\"\"\n",
    "        Init with our settings.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): Settings like:\n",
    "              - num_partitions: How many parts to split the data into.\n",
    "              - tree_params: Extra parameters for the Proximity Tree  model.\n",
    "        \"\"\"       \n",
    "        # Set default configuration\n",
    "        self.config = config\n",
    "        \n",
    "        # List to store trained trees\n",
    "        self.trees = []\n",
    "        \n",
    "        # Final ensemble model\n",
    "        self.ensemble = None\n",
    "        \n",
    "        # Set up a logger so we can see whats going on\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.addHandler(logging.StreamHandler())\n",
    "        self.logger.setLevel(logging.ERROR)\n",
    "        \n",
    "\n",
    "    def _set_forest_classes(self):\n",
    "        \"\"\"Collect all class labels from individual trees and mark the forest as fitted.\"\"\"\n",
    "        all_classes = []\n",
    "        for tree in self.trees:\n",
    "            if hasattr(tree, \"classes_\"):\n",
    "                all_classes.extend(tree.classes_)\n",
    "\n",
    "        unique_classes = np.unique(all_classes)\n",
    "        self.ensemble.classes_ = unique_classes\n",
    "        self.ensemble.n_classes_ = len(unique_classes)\n",
    "\n",
    "        # AEON’s BaseClassifier typically expects a '_class_dictionary' mapping class->int\n",
    "        self.ensemble._class_dictionary = {\n",
    "            cls: idx for idx, cls in enumerate(unique_classes)\n",
    "        }\n",
    "\n",
    "        # Some older AEON versions store the number of classes in a private attribute\n",
    "        self.ensemble._n_classes = len(unique_classes)\n",
    "\n",
    "        # If n_jobs is used, set it explicitly here\n",
    "        if \"n_jobs\" in self.config[\"forest_params\"]:\n",
    "            self.ensemble._n_jobs = self.config[\"forest_params\"][\"n_jobs\"]\n",
    "\n",
    "        # BaseClassifier sets 'is_fitted = True' at the end of fit().\n",
    "        # So we must set the public property 'is_fitted' (not just 'is_fitted_').\n",
    "        # This ensures ._check_is_fitted() passes in predict().\n",
    "        self.ensemble.is_fitted_ = True\n",
    "        self.ensemble.is_fitted = True\n",
    "\n",
    "        \n",
    "    def get_ensemble(self) -> ProximityForest:\n",
    "        \"\"\"\n",
    "        Return the trained Proximity Forest ensemble.\n",
    "        \"\"\"\n",
    "        return self.ensemble\n",
    "\n",
    "    def print_ensemble_details(self):\n",
    "        \"\"\"\n",
    "        Print the details of the aggregated Proximity Forest ensemble.\n",
    "        \"\"\"\n",
    "        if self.ensemble and hasattr(self.ensemble, 'trees_'):\n",
    "            num_trees = len(self.ensemble.trees_)\n",
    "            print(f\"Aggregated Proximity Forest (contains {num_trees} trees):\")\n",
    "            print(f\"  Number of trees (in trees_ attribute): {num_trees}\")\n",
    "            # You might want to print a summary of the parameters used for the forest here\n",
    "            print(f\"  Forest Parameters: {self.ensemble.get_params()}\")\n",
    "            for i, tree in enumerate(self.ensemble.trees_):\n",
    "                print(f\"  Tree {i+1} Details:\")\n",
    "                self._print_tree_node_info(tree.root, depth=2)\n",
    "            print(\"-\" * 20)\n",
    "        else:\n",
    "            print(\"Proximity Forest ensemble has not been trained yet or the 'trees_' attribute is missing.\")\n",
    "\n",
    "    def _print_tree_node_info(self, node, depth):\n",
    "        indent = \"  \" * depth\n",
    "        print(f\"{indent}Node ID: {node.node_id}, Leaf: {node._is_leaf}\")\n",
    "\n",
    "        if node._is_leaf:\n",
    "            print(f\"{indent}  Label: {node.label}, Class Distribution: {node.class_distribution}\")\n",
    "        else:\n",
    "            splitter = node.splitter\n",
    "            if splitter:\n",
    "                exemplars = splitter[0]\n",
    "                distance_info = splitter[1]\n",
    "                distance_measure = list(distance_info.keys())[0]\n",
    "                distance_params = distance_info[distance_measure]\n",
    "\n",
    "                print(f\"{indent}  Splitter:\")\n",
    "                print(f\"{indent}    Distance Measure: {distance_measure}, Parameters: {distance_params}\")\n",
    "                print(f\"{indent}    Exemplar Classes: {list(exemplars.keys())}\")\n",
    "\n",
    "                print(f\"{indent}  Children:\")\n",
    "                for label, child_node in node.children.items():\n",
    "                    print(f\"{indent}    Branch on exemplar of class '{label}':\")\n",
    "                    self._print_tree_node_info(child_node, depth + 1)\n",
    "\n",
    "   \n",
    "   \n",
    "    def train_ensemble(self, df: DataFrame) -> ProximityForest:\n",
    "        \n",
    "        \"\"\"\n",
    "             Train a forest model iin 3 steps:\n",
    "        1. Prepare data partitions\n",
    "        2. Train trees on each partition\n",
    "        3. Combine trees into a forest\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        tree_params = self.config[\"tree_params\"]      \n",
    "        \n",
    "         # Define how to process each partition - inline function\n",
    "        def process_partition(partition_data):\n",
    "            \"\"\"Process one data partition to train a tree.\"\"\"\n",
    "            try:\n",
    "                # Convert Spark rows to pandas DataFrame\n",
    "                pandas_df = pd.DataFrame([row.asDict() for row in partition_data])\n",
    "                if pandas_df.empty:\n",
    "                    return []\n",
    "                \n",
    "                # Prepare features (3D format for AEON) and labes\n",
    "                X = np.ascontiguousarray(pandas_df.drop(\"label\", axis=1).values)\n",
    "                X_3d = X.reshape((X.shape[0], 1, X.shape[1]))  # (samples, 1, features)\n",
    "                y = pandas_df[\"label\"].values\n",
    "                \n",
    "                # Train one tree\n",
    "                tree = ProximityTree(**tree_params)\n",
    "                tree.fit(X_3d, y)\n",
    "                \n",
    "                # Return serialized tree\n",
    "                return [pickle.dumps(tree)]\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to train tree on partition: {str(e)}\")\n",
    "                return []  # Skip failed partitions\n",
    "            \n",
    "        # Run training on all partitions\n",
    "        trees_rdd = df.rdd.mapPartitions(process_partition)\n",
    "        serialized_trees = trees_rdd.collect()\n",
    "        self.trees = [pickle.loads(b) for b in serialized_trees if b is not None]\n",
    "\n",
    "        # Build the forest\n",
    "        if self.trees:\n",
    "            self.ensemble = ProximityForest(\n",
    "                n_trees=len(self.trees),\n",
    "                **self.config[\"forest_params\"]\n",
    "            )\n",
    "            # Manually set forest properties\n",
    "            self.ensemble.trees_ = self.trees\n",
    "            self._set_forest_classes() \n",
    "            return self.ensemble\n",
    "        else:\n",
    "            print(\"Warning: No trees were trained!\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7ec1f901-bb0c-40f9-b83d-87188bea60cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# global_model_manager.py (Optimized Version - Gini Fix)\n",
    "\"\"\"\n",
    "Implements the GlobalModelManager class for training a distribution-friendly \n",
    "proximity tree using Spark DataFrames. Includes optimizations for UDFs, \n",
    "exemplar sampling, count reduction, and reproducibility.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random # Import random for seeding\n",
    "import sys\n",
    "import time\n",
    "from typing import Any, Dict, List, Tuple # Adjusted typing imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import StorageLevel # Import StorageLevel\n",
    "from pyspark.sql import DataFrame, Row, SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf # Import pandas_udf\n",
    "from pyspark.sql.types import (ArrayType, DoubleType, IntegerType, LongType,\n",
    "                               StructField, StructType)\n",
    "\n",
    "# Configure logging for this module\n",
    "# Using a specific logger name is good practice\n",
    "logger_gmm = logging.getLogger(\"GlobalModelManager\") \n",
    "# Ensure handler is added only once\n",
    "if not logger_gmm.handlers:\n",
    "     handler_gmm = logging.StreamHandler(sys.stdout) # Log to stdout\n",
    "     formatter_gmm = logging.Formatter('%(asctime)s - GMM - %(levelname)s - %(message)s')\n",
    "     handler_gmm.setFormatter(formatter_gmm)\n",
    "     logger_gmm.addHandler(handler_gmm)\n",
    "     if logger_gmm.level == logging.NOTSET:\n",
    "          logger_gmm.setLevel(logging.INFO) # Set desired level\n",
    "     logger_gmm.propagate = False # Prevent duplicate logs\n",
    "\n",
    "# Suppress excessive logging from py4j and pyspark itself if needed\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    _NP_GMM = True # Use a distinct name\n",
    "except ImportError: \n",
    "    _NP_GMM = False\n",
    "\n",
    "# Define TreeNode namedtuple (should be defined once globally or imported)\n",
    "TreeNode = collections.namedtuple(\n",
    "    \"TreeNode\", \"node_id parent_id split_on is_leaf prediction children\".split()\n",
    ")\n",
    "\n",
    "# Keep the original efficient euclidean distance function (renamed)\n",
    "def _euclid_gmm(a, b): \n",
    "    \"\"\"Fast Euclidean distance for python *or* NumPy inputs.\"\"\"\n",
    "    # Add basic type/length checks for robustness within UDFs\n",
    "    if a is None or b is None: return float(\"inf\")\n",
    "    # Check if inputs are list-like and have length attribute\n",
    "    len_a = len(a) if hasattr(a, '__len__') else -1\n",
    "    len_b = len(b) if hasattr(b, '__len__') else -1\n",
    "    if len_a != len_b or len_a == -1: return float(\"inf\")\n",
    "    \n",
    "    if _NP_GMM:\n",
    "        try:\n",
    "            # Ensure inputs are numpy arrays for subtraction\n",
    "            a_np = np.asarray(a, dtype=float); b_np = np.asarray(b, dtype=float)\n",
    "            diff = a_np - b_np; dist = float(np.sqrt(np.dot(diff, diff)))\n",
    "            return dist\n",
    "        except Exception as e: \n",
    "            # Avoid logging excessively inside UDF, maybe log sample errors if needed\n",
    "            # logger_gmm.error(f\"Error in _euclid_gmm (NumPy): {e}\") \n",
    "            return float(\"inf\") \n",
    "    else: # Pure Python path\n",
    "        try:\n",
    "            dist = float(math.sqrt(sum((float(x) - float(y)) ** 2 for x, y in zip(a, b)))) # Add float conversion\n",
    "            return dist\n",
    "        except Exception as e: \n",
    "            # logger_gmm.error(f\"Error in _euclid_gmm (Python): {e}\")\n",
    "            return float(\"inf\") \n",
    "\n",
    "# =============================================================================\n",
    "# GlobalModelManager class (Optimised + Enhanced Prediction)\n",
    "# =============================================================================\n",
    "\n",
    "class GlobalModelManager:\n",
    "    \"\"\"\n",
    "    Distribution-friendly proximity-tree learner using Spark DataFrames.\n",
    "    \n",
    "    Optimizations Included:\n",
    "    P1: Pandas UDFs for routing and prediction.\n",
    "    P1: Reduced redundant .count() actions in fit loop.\n",
    "    P2: Distributed exemplar sampling using Window functions.\n",
    "    P3: Seeded RNG for reproducibility.\n",
    "    P2: Uses MEMORY_AND_DISK caching for intermediate DataFrames.\n",
    "    \"\"\"\n",
    "    def __init__(self, spark: SparkSession, config: Dict[str, Any]):\n",
    "        self.logger = logging.getLogger(\"GlobalModelManager\") \n",
    "        self.logger.info(\"Initializing GlobalModelManager.\")\n",
    "        if \"tree_params\" not in config: raise ValueError(\"Config must contain 'tree_params'.\")\n",
    "        \n",
    "        p = config[\"tree_params\"]\n",
    "        self.spark = spark\n",
    "        self.max_depth: int | None = p.get(\"max_depth\") \n",
    "        self.min_samples: int = p.get(\"min_samples_split\", 2) \n",
    "        self.k: int = p.get(\"n_splitters\", 5) \n",
    "        # P3: Add random_state for reproducibility\n",
    "        self.random_state: int | None = p.get(\"random_state\") \n",
    "        #self._rng = random.Random(self.random_state) # Initialize RNG instance with seed\n",
    "        \n",
    "        self.tree: Dict[int, TreeNode] = {0: TreeNode(0, None, None, False, None, {})}\n",
    "        self._next_id: int = 1\n",
    "        self._maj: int = 1 # Default majority class\n",
    "        self.logger.info(f\"Initialized with max_depth={self.max_depth}, min_samples={self.min_samples}, k={self.k}, seed={self.random_state}\")\n",
    "\n",
    "    def _to_ts_df(self, df):\n",
    "        \"\"\" Ensure DataFrame has (row_id, time_series[, true_label]). \"\"\"\n",
    "        self.logger.debug(\"Converting DataFrame to time series format.\")\n",
    "        lbl = None\n",
    "        if \"label\" in df.columns: lbl = \"label\"\n",
    "        elif \"true_label\" in df.columns: lbl = \"true_label\"\n",
    "        else: self.logger.warning(\"No 'label' or 'true_label' column found.\")\n",
    "        \n",
    "        # Add row_id if it doesn't exist\n",
    "        if \"row_id\" not in df.columns:\n",
    "            self.logger.debug(\"Adding row_id column.\")\n",
    "            df = df.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "        else: # Ensure existing row_id is LongType\n",
    "            if df.schema[\"row_id\"].dataType != LongType():\n",
    "                 self.logger.debug(\"Casting existing row_id to LongType.\")\n",
    "                 df = df.withColumn(\"row_id\", F.col(\"row_id\").cast(LongType()))\n",
    "\n",
    "        # Check if time_series column already exists\n",
    "        if \"time_series\" in df.columns:\n",
    "            self.logger.debug(\"'time_series' column already exists.\")\n",
    "            # Ensure label column is named 'true_label' if it exists\n",
    "            if lbl == \"label\": df = df.withColumnRenamed(\"label\", \"true_label\")\n",
    "            # Select necessary columns\n",
    "            select_cols = [\"row_id\", \"time_series\"]\n",
    "            if \"true_label\" in df.columns: select_cols.append(\"true_label\")\n",
    "            return df.select(*select_cols)\n",
    "\n",
    "        # If time_series doesn't exist, create it\n",
    "        cols_to_exclude = {\"row_id\"}\n",
    "        if lbl: cols_to_exclude.add(lbl)\n",
    "        \n",
    "        feat_cols = [c for c in df.columns if c not in cols_to_exclude]\n",
    "        if not feat_cols: raise ValueError(\"No feature columns found to create 'time_series'.\")\n",
    "        self.logger.debug(f\"Creating 'time_series' from feature columns: {feat_cols}\")\n",
    "        \n",
    "        select_exprs = [ F.col(\"row_id\"), F.array(*[F.col(c) for c in feat_cols]).alias(\"time_series\") ]\n",
    "        if lbl: select_exprs.append(F.col(lbl).cast(IntegerType()).alias(\"true_label\"))\n",
    "        \n",
    "        return df.select(*select_exprs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _gini(counts: Dict[int, int]) -> float:\n",
    "        \"\"\" Calculates the Gini impurity for a dictionary of class counts. \"\"\"\n",
    "        tot = sum(counts.values())\n",
    "        if tot == 0: return 0.0\n",
    "        return 1.0 - sum((c / tot) ** 2 for c in counts.values())\n",
    "\n",
    "    def fit(self, df): \n",
    "        \"\"\"Train the proximity tree.\"\"\"\n",
    "        self.logger.info(\"Starting GlobalProxTree fitting process.\")\n",
    "        # P2: Use MEMORY_AND_DISK caching\n",
    "        ts_df = self._to_ts_df(df).persist(StorageLevel.MEMORY_AND_DISK) \n",
    "        initial_row_count = ts_df.count() # P1: Necessary count here\n",
    "        self.logger.info(f\"Input data converted and cached. Row count: {initial_row_count}\")\n",
    "\n",
    "        if initial_row_count == 0: \n",
    "             self.logger.warning(\"Input DataFrame is empty. Cannot train tree.\")\n",
    "             ts_df.unpersist(); return self\n",
    "\n",
    "        # Determine majority class\n",
    "        try: \n",
    "            maj_row = ts_df.groupBy(\"true_label\").count().orderBy(F.desc(\"count\")).first()\n",
    "            if maj_row: self._maj = maj_row[\"true_label\"] \n",
    "            self.logger.info(f\"Overall majority class: {self._maj}\")\n",
    "        except Exception as e: self.logger.error(f\"Error calculating majority class: {e}. Using default: {self._maj}\")\n",
    "\n",
    "        # Initialize assignment DataFrame\n",
    "        assign = ts_df.select(\"row_id\", \"time_series\", \"true_label\") \\\n",
    "                      .withColumn(\"node_id\", F.lit(0)) \\\n",
    "                      .persist(StorageLevel.MEMORY_AND_DISK) # P2: Use MEMORY_AND_DISK\n",
    "        assign_count = assign.count(); self.logger.info(f\"Initial assignment created. Rows: {assign_count}\") # P1: Necessary count\n",
    "        ts_df.unpersist() \n",
    "\n",
    "        # --- Tree Building Loop ---\n",
    "        open_nodes = {0} \n",
    "        depth = 0\n",
    "        while open_nodes and (self.max_depth is None or depth < self.max_depth):\n",
    "            self.logger.info(f\"--- Starting Tree Level {depth} ---\")\n",
    "            self.logger.debug(f\"Open nodes: {open_nodes}\")\n",
    "\n",
    "            # Filter data for current level nodes\n",
    "            cur = assign.filter(F.col(\"node_id\").isin(list(open_nodes))) \\\n",
    "                        .persist(StorageLevel.MEMORY_AND_DISK) # P2: Use MEMORY_AND_DISK\n",
    "            \n",
    "            # --- P1: Calculate node stats ONCE per level ---\n",
    "            self.logger.debug(\"Calculating statistics for current level nodes...\")\n",
    "            node_stats_df = cur.groupBy(\"node_id\", \"true_label\").count()\n",
    "            node_stats_rows = node_stats_df.collect() # Collect stats (expect relatively small)\n",
    "            stats_per_node = collections.defaultdict(dict)\n",
    "            totals_per_node = collections.defaultdict(int)\n",
    "            for r in node_stats_rows:\n",
    "                 stats_per_node[r.node_id][r.true_label] = r[\"count\"]\n",
    "                 totals_per_node[r.node_id] += r[\"count\"]\n",
    "            self.logger.debug(f\"Calculated stats for {len(totals_per_node)} nodes.\")\n",
    "            # --- End P1 ---\n",
    "\n",
    "            # Check if any data remains for open nodes (using precalculated totals)\n",
    "            if not any(totals_per_node.get(nid, 0) > 0 for nid in open_nodes):\n",
    "                 self.logger.info(f\"No data for open nodes at depth {depth}. Stopping tree growth.\")\n",
    "                 cur.unpersist(); break\n",
    "\n",
    "            # --- P2: Distributed Exemplar Sampling ---\n",
    "            self.logger.debug(\"Starting distributed exemplar sampling...\")\n",
    "            # P3: Seed rand with instance RNG state (requires converting int to seed)\n",
    "            window_spec = Window.partitionBy(\"node_id\", \"true_label\").orderBy(F.rand()) # REMOVED SEED \n",
    "#            window_spec = Window.partitionBy(\"node_id\", \"true_label\").orderBy(F.rand(self._rng.randint(0, 1000000))) \n",
    "            sampled_exemplars_df = cur.withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "                                      .filter(F.col(\"rank\") <= self.k) \\\n",
    "                                      .select(\"node_id\", \"true_label\", \"time_series\") \n",
    "            collected_exemplars = sampled_exemplars_df.collect() # Collect ONLY the k*nodes*labels samples\n",
    "            pool: Dict[int, Dict[int, list]] = collections.defaultdict(dict)\n",
    "            for row in collected_exemplars: pool[row.node_id].setdefault(row.true_label, []).append(row.time_series) \n",
    "            self.logger.debug(f\"Finished exemplar sampling. Nodes with pools: {list(pool.keys())}\")\n",
    "            # --- End P2 ---\n",
    "\n",
    "            best_splits: Dict[int, Tuple[str, Dict[int, list]]] = {} \n",
    "            nodes_to_make_leaf: set[int] = set()\n",
    "\n",
    "            # --- Evaluate splits (Driver-side logic using precalculated stats) ---\n",
    "            self.logger.debug(\"Evaluating splits...\")\n",
    "            for nid in list(open_nodes): \n",
    "                self.logger.debug(f\"Evaluating node {nid}...\")\n",
    "                stats = stats_per_node.get(nid, {})\n",
    "                tot_samples_in_node = totals_per_node.get(nid, 0) # P1: Reuse count\n",
    "                self.logger.debug(f\"Node {nid} stats: {stats}, total samples: {tot_samples_in_node}\")\n",
    "\n",
    "                # Leaf conditions (using reused count)\n",
    "                is_leaf = False\n",
    "                if tot_samples_in_node < self.min_samples: is_leaf = True; reason=\"min_samples\"\n",
    "                elif len(stats) <= 1: is_leaf = True; reason=\"pure\"\n",
    "                elif nid not in pool or not pool[nid] or len(pool[nid]) < 2: is_leaf = True; reason=\"exemplars\"\n",
    "                \n",
    "                if is_leaf: self.logger.info(f\"Node {nid} becoming leaf: {reason}.\"); nodes_to_make_leaf.add(nid); continue\n",
    "\n",
    "                # Find best split for non-leaf node\n",
    "                parent_gini = self._gini(stats)\n",
    "                best_gain = -1.0; best_exemplars_for_split = None\n",
    "                node_pool = pool[nid]; available_labels = list(node_pool.keys())\n",
    "\n",
    "                self.logger.debug(f\"Node {nid}: Evaluating {self.k} candidates. Parent Gini: {parent_gini:.4f}\")\n",
    "                for k_idx in range(self.k):\n",
    "                    candidate_ex = {}\n",
    "                    possible = True\n",
    "                    for lbl in available_labels:\n",
    "                        if node_pool[lbl]: \n",
    "                            #candidate_ex[lbl] = self._rng.choice(node_pool[lbl]) # P3: Use seeded RNG\n",
    "                            candidate_ex[lbl] = random.choice(node_pool[lbl]) \n",
    "                        else: possible = False; break \n",
    "                    if not possible or len(candidate_ex) < 2: continue \n",
    "\n",
    "                    bc_ex = self.spark.sparkContext.broadcast(candidate_ex)\n",
    "                    \n",
    "                    # Standard UDF for Gini calculation step (Pandas UDF less obvious benefit here)\n",
    "                    @F.udf(IntegerType())\n",
    "                    def nearest_lbl_udf_local(ts):\n",
    "                        ex_val = bc_ex.value; best_d, best_l = float(\"inf\"), None\n",
    "                        for l, ex_ts in ex_val.items():\n",
    "                            d = _euclid_gmm(ts, ex_ts); \n",
    "                            if d < best_d: best_d, best_l = d, l\n",
    "                        return best_l\n",
    "\n",
    "                    # Filter data for the current node *before* applying UDF\n",
    "                    node_data_df = cur.filter(F.col(\"node_id\") == nid) \n",
    "                    \n",
    "                    # Calculate weighted Gini impurity (DataFrame based)\n",
    "                    split_impurity_df = node_data_df.withColumn(\"branch\", nearest_lbl_udf_local(\"time_series\")) \\\n",
    "                                               .groupBy(\"branch\", \"true_label\").count()\n",
    "                    branch_totals = split_impurity_df.groupBy(\"branch\").agg(F.sum(\"count\").alias(\"branch_total\"))\n",
    "                    gini_per_branch = split_impurity_df.join(branch_totals, \"branch\") \\\n",
    "                                             .withColumn(\"prob_sq\", (F.col(\"count\") / F.col(\"branch_total\")) ** 2) \\\n",
    "                                             .groupBy(\"branch\", \"branch_total\").agg(F.sum(\"prob_sq\").alias(\"s\")) \\\n",
    "                                             .withColumn(\"branch_gini\", 1.0 - F.col(\"s\")) # <-- Corrected: sum(\"prob_sq\")\n",
    "                    weighted_gini_row = gini_per_branch.withColumn(\"weighted_gini\", (F.col(\"branch_total\") / tot_samples_in_node) * F.col(\"branch_gini\")) \\\n",
    "                                                  .agg(F.sum(\"weighted_gini\").alias(\"total_weighted_gini\")) \\\n",
    "                                                  .first()\n",
    "                    bc_ex.unpersist(False) \n",
    "\n",
    "                    if weighted_gini_row and weighted_gini_row[\"total_weighted_gini\"] is not None:\n",
    "                        current_impurity = weighted_gini_row[\"total_weighted_gini\"]\n",
    "                        current_gain = parent_gini - current_impurity\n",
    "                        self.logger.debug(f\"Node {nid}, Candidate {k_idx+1}: Impurity={current_impurity:.4f}, Gain={current_gain:.4f}\")\n",
    "                        if current_gain > best_gain:\n",
    "                            best_gain = current_gain; best_exemplars_for_split = candidate_ex\n",
    "                            self.logger.debug(f\"Node {nid}: New best split found (Gain: {best_gain:.4f})\")\n",
    "                    else: self.logger.warning(f\"Node {nid}, Candidate {k_idx+1}: Could not calculate impurity.\")\n",
    "\n",
    "                # Decide if node becomes leaf\n",
    "                if best_gain <= 1e-9: \n",
    "                    self.logger.info(f\"Node {nid} becoming leaf: best gain ({best_gain:.4f}) too low.\")\n",
    "                    nodes_to_make_leaf.add(nid)\n",
    "                else:\n",
    "                    self.logger.info(f\"Node {nid}: Selected best split with gain {best_gain:.4f}.\")\n",
    "                    best_splits[nid] = (\"euclidean\", best_exemplars_for_split)\n",
    "\n",
    "            # --- Finalize leaves ---\n",
    "            self.logger.debug(f\"Nodes to finalize as leaves: {nodes_to_make_leaf}\")\n",
    "            for nid in list(nodes_to_make_leaf): \n",
    "                if nid in open_nodes: \n",
    "                    stats = stats_per_node.get(nid, {}) # P1: Reuse stats\n",
    "                    maj_lbl = self._maj \n",
    "                    if stats: maj_lbl = max(stats.items(), key=lambda kv: (kv[1], -kv[0]))[0] \n",
    "                    self.tree[nid] = self.tree[nid]._replace(is_leaf=True, prediction=maj_lbl, children={}, split_on=None)\n",
    "                    self.logger.info(f\"Node {nid} finalized as leaf. Prediction: {maj_lbl}.\")\n",
    "                    open_nodes.remove(nid) \n",
    "\n",
    "            # --- Create children and update assignments ---\n",
    "            if not best_splits: self.logger.info(\"No nodes split.\"); cur.unpersist(); break \n",
    "\n",
    "            self.logger.debug(\"Creating child nodes...\")\n",
    "            split_map = {}; new_open_nodes_for_next_level = set()\n",
    "            for pid, (measure, exemplars) in best_splits.items():\n",
    "                child_dict = {}\n",
    "                for branch_label in exemplars: \n",
    "                    cid = self._next_id; self._next_id += 1\n",
    "                    self.tree[cid] = TreeNode(cid, pid, None, False, None, {}) \n",
    "                    child_dict[branch_label] = cid\n",
    "                    split_map[(pid, branch_label)] = cid\n",
    "                    new_open_nodes_for_next_level.add(cid)\n",
    "                self.tree[pid] = self.tree[pid]._replace(split_on=(measure, exemplars), children=child_dict, is_leaf=False)\n",
    "                self.logger.debug(f\"Parent node {pid} updated. Children: {list(child_dict.values())}\")\n",
    "\n",
    "            open_nodes = new_open_nodes_for_next_level\n",
    "            self.logger.debug(f\"New open_nodes for next level: {open_nodes}\")\n",
    "\n",
    "            # --- P1: Use Pandas UDF for routing ---\n",
    "            bc_split_map = self.spark.sparkContext.broadcast(split_map)\n",
    "            bc_best_exemplars = self.spark.sparkContext.broadcast({pid: ex for pid, (_, ex) in best_splits.items()})\n",
    "            self.logger.debug(\"Broadcasted split map and best exemplars for routing.\")\n",
    "            _euclid_gmm_local_route = _euclid_gmm # Local ref for UDF\n",
    "\n",
    "            @F.pandas_udf(IntegerType())\n",
    "            def route_pandas_udf(pid_series: pd.Series, ts_series: pd.Series) -> pd.Series:\n",
    "                split_map_val = bc_split_map.value; exs_map_val = bc_best_exemplars.value\n",
    "                results = []\n",
    "                for pid, ts in zip(pid_series, ts_series):\n",
    "                    if pid not in exs_map_val: results.append(pid); continue \n",
    "                    split_exemplars = exs_map_val[pid]\n",
    "                    best_d, best_lbl = float(\"inf\"), None\n",
    "                    for lbl, ex_ts in split_exemplars.items():\n",
    "                        d = _euclid_gmm_local_route(ts, ex_ts) \n",
    "                        if d < best_d: best_d, best_lbl = d, lbl\n",
    "                    results.append(split_map_val.get((pid, best_lbl), pid))\n",
    "                return pd.Series(results, dtype=pd.Int64Dtype()) # Use nullable Int\n",
    "\n",
    "            # Apply the route UDF\n",
    "            old_assign = assign \n",
    "            self.logger.info(\"Applying route_pandas_udf to update assignments...\")\n",
    "            assign = assign.withColumn(\"node_id\", route_pandas_udf(\"node_id\", \"time_series\")) \\\n",
    "                           .persist(StorageLevel.MEMORY_AND_DISK) # P2: Use MEMORY_AND_DISK\n",
    "            assign_updated_count = assign.count() # P1: Necessary action\n",
    "            self.logger.info(f\"Assignment DataFrame updated. Rows: {assign_updated_count}\")\n",
    "\n",
    "            # Unpersist intermediates\n",
    "            old_assign.unpersist()\n",
    "            bc_split_map.unpersist(blocking=False)\n",
    "            bc_best_exemplars.unpersist(blocking=False)\n",
    "            cur.unpersist() \n",
    "            self.logger.debug(f\"Unpersisted intermediates for depth {depth}.\")\n",
    "\n",
    "            depth += 1 \n",
    "        # --- End of Tree Building Loop ---\n",
    "        self.logger.info(f\"Tree building loop finished at depth {depth}.\")\n",
    "\n",
    "        # --- Final Dangling Node Check --- \n",
    "        self.logger.debug(\"Performing final check for dangling internal nodes.\")\n",
    "        nodes_to_finalize = [nid for nid, nd in self.tree.items() if not nd.is_leaf and not nd.children]\n",
    "        if nodes_to_finalize:\n",
    "             self.logger.warning(f\"Found {len(nodes_to_finalize)} dangling nodes: {nodes_to_finalize}\")\n",
    "             # Filter final assignment DF for these nodes\n",
    "             dangling_df = assign.filter(F.col(\"node_id\").isin(nodes_to_finalize))\n",
    "             dangling_stats_rows = dangling_df.groupBy(\"node_id\", \"true_label\").count().collect()\n",
    "             stats_by_node = collections.defaultdict(dict)\n",
    "             for r in dangling_stats_rows: stats_by_node[r[\"node_id\"]][r[\"true_label\"]] = r[\"count\"]\n",
    "             for nid in nodes_to_finalize:\n",
    "                 stats = stats_by_node.get(nid, {}); maj_lbl = self._maj \n",
    "                 if stats: maj_lbl = max(stats.items(), key=lambda kv: (kv[1], -kv[0]))[0]\n",
    "                 self.tree[nid] = self.tree[nid]._replace(is_leaf=True, prediction=maj_lbl, split_on=None)\n",
    "                 self.logger.info(f\"Dangling node {nid} finalized as leaf. Prediction: {maj_lbl}.\")\n",
    "        \n",
    "        assign.unpersist()\n",
    "        self.logger.info(\"GlobalProxTree fitting process finished.\")\n",
    "        return self\n",
    "\n",
    "    # --- P1: Prediction uses Pandas UDF ---\n",
    "    def predict(self, df):\n",
    "        \"\"\" Predicts class labels using Pandas UDF for traversal. \"\"\"\n",
    "        self.logger.info(\"Starting GlobalProxTree prediction (using Pandas UDF).\")\n",
    "        df_ts = self._to_ts_df(df) # Ensure correct format\n",
    "\n",
    "        if not self.tree or 0 not in self.tree or (not self.tree[0].children and self.tree[0].prediction is None):\n",
    "             self.logger.warning(\"Tree not fitted/empty. Returning default predictions.\")\n",
    "             default_pred = F.lit(self._maj).cast(IntegerType()).alias(\"prediction\")\n",
    "             sel_cols = [\"row_id\", \"time_series\"] + ([\"true_label\"] if \"true_label\" in df_ts.columns else []) + [default_pred]\n",
    "             return df_ts.select(*sel_cols)\n",
    "\n",
    "        self.logger.debug(\"Converting tree to plain dict for broadcast.\")\n",
    "        plain_tree = {nid: node._asdict() for nid, node in self.tree.items()}\n",
    "        bc_tree = self.spark.sparkContext.broadcast(plain_tree)\n",
    "        self.logger.debug(f\"Broadcasted plain tree ({len(plain_tree)} nodes).\")\n",
    "\n",
    "        # Need _euclid_gmm available in the UDF scope\n",
    "        _euclid_gmm_local_pred = _euclid_gmm # Create local ref for UDF\n",
    "\n",
    "        @F.pandas_udf(IntegerType())\n",
    "        def traverse_pandas_udf(ts_series: pd.Series) -> pd.Series:\n",
    "            tree_dict_pd = bc_tree.value # Access broadcast value once per batch\n",
    "            predictions = []\n",
    "            \n",
    "            for ts in ts_series: # Iterate through the Pandas Series\n",
    "                if ts is None: predictions.append(None); continue\n",
    "                \n",
    "                node_id = 0\n",
    "                MAX_TRAVERSAL_DEPTH = 50; current_depth = 0\n",
    "                \n",
    "                while node_id in tree_dict_pd and current_depth < MAX_TRAVERSAL_DEPTH:\n",
    "                    current_node = tree_dict_pd[node_id]\n",
    "                    if current_node.get('is_leaf', False):\n",
    "                        predictions.append(current_node.get('prediction')); break \n",
    "                    \n",
    "                    split_info = current_node.get('split_on') \n",
    "                    children = current_node.get('children')\n",
    "                    if not split_info or not children: predictions.append(current_node.get('prediction')); break # Fallback\n",
    "\n",
    "                    _, exemplars = split_info \n",
    "                    if not exemplars: predictions.append(current_node.get('prediction')); break # Fallback\n",
    "\n",
    "                    min_dist_all = float(\"inf\"); best_branch_id_all = None \n",
    "                    for branch_id, exemplar_ts in exemplars.items():\n",
    "                        d = _euclid_gmm_local_pred(ts, exemplar_ts) \n",
    "                        if d < min_dist_all: min_dist_all = d; best_branch_id_all = branch_id\n",
    "\n",
    "                    if best_branch_id_all is not None and best_branch_id_all in children:\n",
    "                        node_id = children[best_branch_id_all]\n",
    "                    else: # Fallback to nearest existing child\n",
    "                        min_dist_existing = float(\"inf\"); next_node_id_found = None \n",
    "                        for ex_br_id, ex_ch_id in children.items():\n",
    "                            if ex_br_id in exemplars: \n",
    "                                d = _euclid_gmm_local_pred(ts, exemplars[ex_br_id]) \n",
    "                                if d < min_dist_existing: min_dist_existing = d; next_node_id_found = ex_ch_id\n",
    "                        if next_node_id_found is not None: node_id = next_node_id_found\n",
    "                        else: predictions.append(current_node.get('prediction')); break # Ultimate fallback\n",
    "                    \n",
    "                    current_depth += 1\n",
    "                else: # Handle while loop exit\n",
    "                     if current_depth >= MAX_TRAVERSAL_DEPTH:\n",
    "                          last_node = tree_dict_pd.get(node_id)\n",
    "                          pred = last_node.get('prediction') if last_node and last_node.get('is_leaf') else None\n",
    "                          predictions.append(pred)\n",
    "                     else: predictions.append(None) \n",
    "                          \n",
    "            return pd.Series(predictions, dtype=pd.Int64Dtype()) # Use nullable int\n",
    "\n",
    "        self.logger.info(\"Applying prediction Pandas UDF...\")\n",
    "        out_df = df_ts.withColumn(\"pred_raw\", traverse_pandas_udf(\"time_series\")) \\\n",
    "                      .withColumn(\"prediction\", F.coalesce(F.col(\"pred_raw\"), F.lit(self._maj)).cast(IntegerType())) \\\n",
    "                      .drop(\"pred_raw\")\n",
    "        \n",
    "        bc_tree.unpersist(blocking=False) \n",
    "        self.logger.debug(\"Unpersisted broadcasted tree.\")\n",
    "\n",
    "        # Select final output columns\n",
    "        select_cols = [\"row_id\", \"time_series\"] + ([\"true_label\"] if \"true_label\" in out_df.columns else []) + [\"prediction\"]\n",
    "        return out_df.select(*select_cols)\n",
    "\n",
    "\n",
    "    def print_tree(self) -> str:\n",
    "        \"\"\" Returns a human-readable string representation of the tree. \"\"\"\n",
    "        # (Keep existing print_tree logic)\n",
    "        self.logger.debug(\"print_tree started.\")\n",
    "        lines = []\n",
    "        def rec(nid: int, depth: int):\n",
    "             nd = self.tree.get(nid)\n",
    "             if nd is None: lines.append(\"  \" * depth + f\"#{nid} MISSING\"); return\n",
    "             ind = \"  \" * depth\n",
    "             if nd.is_leaf: lines.append(f\"{ind}Leaf {nid} → {nd.prediction}\")\n",
    "             else:\n",
    "                 meas, ex = nd.split_on or (None, {})\n",
    "                 lines.append(f\"{ind}Node {nid} split={meas} labels={list(ex.keys())}\")\n",
    "                 for lbl, cid in sorted(nd.children.items()):\n",
    "                     lines.append(f\"{ind}  ├─ lbl={lbl} → child {cid}\")\n",
    "                     rec(cid, depth + 2)\n",
    "        rec(0, 0)\n",
    "        tree_str = \"\\n\".join(lines)\n",
    "        self.logger.debug(\"print_tree finished.\")\n",
    "        return tree_str\n",
    "\n",
    "\n",
    "    def save_tree(self, path: str):\n",
    "        \"\"\" Pickles the essential state of the manager to a file. \"\"\"\n",
    "        self.logger.info(f\"Saving GlobalModelManager state to {path}\")\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True) \n",
    "            state = {\n",
    "                \"max_depth\": self.max_depth, \"min_samples\": self.min_samples,\n",
    "                \"k\": self.k, \"tree\": self.tree, \n",
    "                \"_next_id\": self._next_id, \"_maj\": self._maj,\n",
    "                \"random_state\": self.random_state # P3: Save seed\n",
    "            }\n",
    "            with open(path, \"wb\") as fh: pickle.dump(state, fh)\n",
    "            self.logger.info(f\"Successfully saved state to {path}.\")\n",
    "        except Exception as e: self.logger.error(f\"Failed to save tree state: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_tree(cls, spark: SparkSession, path: str) -> \"GlobalModelManager\":\n",
    "        \"\"\" Loads the manager state from a pickled file. \"\"\"\n",
    "        logger_gmm.info(f\"Loading GlobalModelManager state from {path}\") # Use class logger\n",
    "        try:\n",
    "            with open(path, \"rb\") as fh: state: Dict[str, Any] = pickle.load(fh)\n",
    "            logger_gmm.debug(\"State loaded successfully.\")\n",
    "        except Exception as e: logger_gmm.error(f\"Failed to load tree state: {e}\", exc_info=True); raise\n",
    "\n",
    "        # Reconstruct config for initialization\n",
    "        loaded_config = {\n",
    "            \"tree_params\": {\n",
    "                \"max_depth\": state.get(\"max_depth\"), \n",
    "                \"min_samples_split\": state.get(\"min_samples\", 2), \n",
    "                \"n_splitters\": state.get(\"k\", 5), \n",
    "                \"random_state\": state.get(\"random_state\") # P3: Load seed\n",
    "            }\n",
    "        }\n",
    "        logger_gmm.debug(f\"Reconstructed config: {loaded_config}\")\n",
    "\n",
    "        # Create instance and restore state\n",
    "        inst = cls(spark, loaded_config)\n",
    "        inst.tree = state.get(\"tree\", {0: TreeNode(0, None, None, False, None, {})}) \n",
    "        inst._next_id = state.get(\"_next_id\", 1) \n",
    "        inst._maj = state.get(\"_maj\", 1) \n",
    "        # P3: Re-initialize RNG if state loaded\n",
    "        inst._rng = random.Random(inst.random_state) \n",
    "        logger_gmm.info(f\"Instance created. Tree size: {len(inst.tree)} nodes.\")\n",
    "        return inst\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1e0da2d-3073-4fd9-b6c6-e7c967bf982f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This file ties the whole pipeline together.\n",
    "It iterates through different partition configurations, running the \n",
    "full pipeline (ingestion, preprocessing, training, prediction, evaluation) \n",
    "in each iteration to assess the impact of partitioning on both local and global models.\n",
    "\n",
    "NOTE: This structure involves re-running ingestion and preprocessing in each \n",
    "iteration, which includes potentially expensive data shuffles. This is done \n",
    "intentionally for this specific experiment but is less efficient than \n",
    "preprocessing once outside the loop if only varying local model partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a92fb359-6d14-44af-8cc2-e50e89c68b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PipelineController_Loop:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the controller with the pipeline configuration.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.spark = None # SparkSession managed once for the entire run\n",
    "        self.ingestion_config = {} # Populated during _setup_spark\n",
    "        \n",
    "        # Logger setup\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "            if self.logger.level == logging.NOTSET:\n",
    "                 self.logger.setLevel(logging.INFO) \n",
    "            # Prevent duplicate messages if root logger also has handlers\n",
    "            self.logger.propagate = True \n",
    "\n",
    "    def _setup_spark(self):\n",
    "        \"\"\"\n",
    "        Setup or retrieve the Spark session based on the environment.\n",
    "        Constructs appropriate data paths.\n",
    "        Adds necessary Python files to Spark context for local runs.\n",
    "        Returns True if setup is successful, False otherwise.\n",
    "        \"\"\"\n",
    "        # Configure Spark Session only if one doesn't exist \n",
    "        if self.spark is None:\n",
    "            try:\n",
    "                if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "                    # Databricks environment\n",
    "                    self.spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "                    print(\"\\nUsing Databricks Spark session.\")\n",
    "                    self.ingestion_config = {\n",
    "                        \"data_path\": self.config.get(\"databricks_data_path\", \"/mnt/2025-team6/fulldataset_ECG5000.csv\"),\n",
    "                        \"data_percentage\": self.config.get(\"data_percentage\", 0.05) \n",
    "                    }\n",
    "                else:\n",
    "                    # Local environment setup\n",
    "                    # Stop existing local session if present before creating new one\n",
    "                    existing_spark = SparkSession.getActiveSession()\n",
    "                    if existing_spark:\n",
    "                         self.logger.info(\"Stopping existing Spark session before starting new one.\")\n",
    "                         existing_spark.stop()\n",
    "\n",
    "                    self.spark = SparkSession.builder \\\n",
    "                        .appName(f\"LocalPipeline_Run_{time.time()}\") \\\n",
    "                        .master(\"local[6]\") \\\n",
    "                        .config(\"spark.driver.memory\", \"12g\") \\\n",
    "                        .config(\"spark.executor.memory\", \"12g\") \\\n",
    "                        .config(\"spark.driver.maxResultSize\", \"12g\") \\\n",
    "                        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "                        .getOrCreate()\n",
    "                    print(\"\\nUsing local Spark session.\")\n",
    "\n",
    "                    # --- Construct Local Data Path (Robustly) ---\n",
    "                    project_root = None\n",
    "                    current_dir = None\n",
    "                    try:\n",
    "                         # __file__ is the path of the current script (controller_loop.py)\n",
    "                         current_script_path = os.path.abspath(__file__) \n",
    "                         current_dir = pathlib.Path(current_script_path).parent # Directory containing this script (src)\n",
    "                         # Assumes script is in src, parent is code, parent.parent is project root\n",
    "                         project_root = current_dir.parent.parent.resolve() \n",
    "                         self.logger.info(f\"Project root determined using __file__: {project_root}\")\n",
    "                    except NameError:\n",
    "                         # Fallback if __file__ is not defined (e.g., interactive/notebook)\n",
    "                         # *** CORRECTED FALLBACK LOGIC ***\n",
    "                         # Assume CWD is the directory containing the notebook/where python was launched\n",
    "                         cwd = pathlib.Path(os.getcwd()).resolve() \n",
    "                         self.logger.warning(f\"__file__ not defined, using CWD: {cwd}\")\n",
    "                         # Check if CWD looks like the 'src' directory\n",
    "                         if cwd.name == 'src':\n",
    "                              project_root = cwd.parent.parent.resolve() # Go up two levels\n",
    "                              current_dir = cwd # Set current_dir for module loading later\n",
    "                              self.logger.info(f\"Assuming CWD is 'src', project root set to: {project_root}\")\n",
    "                         # Check if CWD looks like the 'code' directory\n",
    "                         elif cwd.name == 'code':\n",
    "                              project_root = cwd.parent.resolve() # Go up one level\n",
    "                              current_dir = cwd / \"src\" # Assume src exists for module loading\n",
    "                              self.logger.info(f\"Assuming CWD is 'code', project root set to: {project_root}\")\n",
    "                         else: # Otherwise, assume CWD *is* the project root\n",
    "                              project_root = cwd \n",
    "                              current_dir = project_root / \"src\" # Assume src exists for module loading\n",
    "                              self.logger.info(f\"Assuming CWD is project root: {project_root}\")\n",
    "                         \n",
    "                    # Ensure project_root was determined\n",
    "                    if project_root is None:\n",
    "                         self.logger.error(\"FATAL: Could not determine project root directory.\")\n",
    "                         if self.spark: self.spark.stop(); self.spark = None\n",
    "                         return False\n",
    "\n",
    "                    local_data_file = self.config.get(\"local_data_path\", \"fulldataset_ECG5000.csv\").lstrip('/\\\\') \n",
    "                    final_data_path_os = project_root / local_data_file # Use pathlib's / operator\n",
    "                    \n",
    "                    if not final_data_path_os.exists():\n",
    "                         self.logger.error(f\"FATAL: Constructed data path does NOT exist: {final_data_path_os}\")\n",
    "                         # Stop the newly created session if path is invalid\n",
    "                         if self.spark: self.spark.stop(); self.spark = None\n",
    "                         return False # Indicate setup failure\n",
    "                    else:\n",
    "                         self.logger.info(f\"Verified data path exists: {final_data_path_os}\") \n",
    "                         # Convert OS path to file URI for Spark AFTER verification\n",
    "                         final_data_path_uri = final_data_path_os.as_uri() \n",
    "\n",
    "                    self.ingestion_config = {\n",
    "                        \"data_path\": final_data_path_uri, \n",
    "                        \"data_percentage\": self.config.get(\"data_percentage\", 1.0) \n",
    "                    }\n",
    "                    self.logger.info(f\"Data path set in ingestion_config: {self.ingestion_config['data_path']}\")\n",
    "                    self.logger.info(f\"Local data percentage set to: {self.ingestion_config['data_percentage']}\")\n",
    "                    # --- End Construct Local Data Path ---\n",
    "\n",
    "                # Add Python module dependencies for local runs (needs to be done only once per session)\n",
    "                if \"DATABRICKS_RUNTIME_VERSION\" not in os.environ and self.spark:\n",
    "                    modules_to_add = ['global_model_manager.py'] # Add other required modules if needed\n",
    "                    try:\n",
    "                        # Use the current_dir determined earlier (should be src)\n",
    "                        if current_dir is None or not current_dir.exists() or not current_dir.is_dir():\n",
    "                             # Fallback if current_dir wasn't determined correctly\n",
    "                             current_dir = project_root / \"src\" \n",
    "                             self.logger.warning(f\"Re-assuming 'src' directory for module loading: {current_dir}\")\n",
    "                             if not current_dir.exists():\n",
    "                                  raise FileNotFoundError(\"Cannot find assumed 'src' directory for module loading.\")\n",
    "                             \n",
    "                        for module_name in modules_to_add:\n",
    "                             module_path = current_dir / module_name # Path relative to src\n",
    "                             if module_path.exists():\n",
    "                                 self.spark.sparkContext.addPyFile(str(module_path)) # addPyFile needs string path\n",
    "                                 self.logger.debug(f\"Added {module_path} to SparkContext pyFiles.\") \n",
    "                             else:\n",
    "                                  self.logger.error(f\"Could not find module {module_name} at {module_path}\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Failed to add Python module to SparkContext: {e}\")\n",
    "                \n",
    "                return True # Indicate successful setup\n",
    "\n",
    "            except Exception as e:\n",
    "                 self.logger.error(f\"Error during Spark setup: {e}\", exc_info=True)\n",
    "                 if self.spark: self.spark.stop(); self.spark = None # Ensure cleanup on error\n",
    "                 return False # Indicate setup failure\n",
    "        else:\n",
    "             # Spark session already exists\n",
    "             self.logger.debug(\"Spark session already exists.\")\n",
    "             return True\n",
    "            \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executes the main pipeline loop for model training and evaluation.\n",
    "        Iterates through specified partition counts, running the full pipeline \n",
    "        (ingestion, preprocessing, training, eval, save) in each iteration.\n",
    "        \"\"\"\n",
    "        # --- Determine number of iterations and which models to run ---\n",
    "        number_iterations_global, number_iterations_local = 0, 0\n",
    "        run_local = self.config.get(\"local_model_config\", {}).get(\"test_local_model\", False)\n",
    "        run_global = self.config.get(\"global_model_config\", {}).get(\"test_global_model\", False)\n",
    "\n",
    "        if not run_local and not run_global:\n",
    "            self.logger.error(\"No model selected for testing. Set 'test_local_model' or 'test_global_model' to True in config.\")\n",
    "            return \n",
    "\n",
    "        if run_local:\n",
    "            number_iterations_local = self.config.get(\"local_model_config\", {}).get(\"num_partitions\", 0)\n",
    "        if run_global:\n",
    "            # Use the partition count from global config to control loop iterations for the experiment\n",
    "            number_iterations_global = self.config.get(\"global_model_config\", {}).get(\"num_partitions\", 0) \n",
    "            \n",
    "        # Determine the overall maximum number of iterations needed\n",
    "        number_iterations = 0\n",
    "        if run_local: number_iterations = max(number_iterations, number_iterations_local)\n",
    "        if run_global: number_iterations = max(number_iterations, number_iterations_global)\n",
    "\n",
    "        min_iterations = self.config.get(\"min_number_iterarations\", 2)\n",
    "        start_iteration = min_iterations\n",
    "        end_iteration = number_iterations\n",
    "        \n",
    "        if end_iteration < start_iteration:\n",
    "             self.logger.warning(f\"Max iterations ({end_iteration}) is less than min iterations ({start_iteration}). No iterations will run.\")\n",
    "             start_iteration = end_iteration + 1 # Make range empty\n",
    "\n",
    "        # --- Initialize report accumulators ---\n",
    "        all_reports_global = {} \n",
    "        all_reports_local = {}  \n",
    "\n",
    "        # --- Setup Spark Session ONCE before the loop ---\n",
    "        self._setup_spark() \n",
    "        if not self.spark:\n",
    "             self.logger.error(\"Initial Spark setup failed. Aborting run.\")\n",
    "             return\n",
    "        # Check data path after setup\n",
    "        if self.ingestion_config.get(\"data_path\") is None:\n",
    "             self.logger.error(\"Initial data path construction failed. Aborting run.\")\n",
    "             # Spark might have been stopped in _setup_spark if path was invalid\n",
    "             if \"DATABRICKS_RUNTIME_VERSION\" not in os.environ and self.spark: self.spark.stop() \n",
    "             return\n",
    "\n",
    "        # ============================================================\n",
    "        # === Main Iteration Loop ===\n",
    "        # ============================================================\n",
    "        try: # Add try block around the loop for final cleanup\n",
    "            for i in range(start_iteration, end_iteration + 1): \n",
    "                self.logger.info(f\"========== Starting Iteration {i} ==========\")\n",
    "                \n",
    "                iteration_run_local = run_local and i <= number_iterations_local \n",
    "                iteration_run_global = run_global # Global runs in all iterations up to its max if enabled\n",
    "\n",
    "                # --- Setup Modules for Iteration ---\n",
    "                # Spark session is already running\n",
    "                current_datetime = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "                # Initialize modules for this iteration\n",
    "                self.evaluator = Evaluator(track_memory=self.config.get(\"track_memory\", False)) \n",
    "                # Create a deep copy of the config for this iteration to avoid side effects\n",
    "                current_iter_config = json.loads(json.dumps(self.config)) \n",
    "                # Update partition counts in the copied config for this iteration\n",
    "                if iteration_run_local: \n",
    "                     current_iter_config.setdefault(\"local_model_config\", {})[\"num_partitions\"] = i\n",
    "                if iteration_run_global: \n",
    "                     current_iter_config.setdefault(\"global_model_config\", {})[\"num_partitions\"] = i\n",
    "                \n",
    "                # Initialize Preprocessor and Ingestion with the iteration-specific config\n",
    "                self.preprocessor = Preprocessor(config=current_iter_config) \n",
    "                # Ingestion uses the config set during initial _setup_spark\n",
    "                self.ingestion = DataIngestion(spark=self.spark, config=self.ingestion_config) \n",
    "\n",
    "                # Define DataFrame variables for this iteration scope\n",
    "                preprocessed_train_df: DataFrame = None\n",
    "                preprocessed_test_df: DataFrame = None\n",
    "                min_max_values: dict = None\n",
    "\n",
    "                try:\n",
    "                    # --- Data Ingestion (runs every iteration) ---\n",
    "                    self.evaluator.start_timer(\"Ingestion\")\n",
    "                    self.logger.info(f\"Iteration {i}: Loading data...\")\n",
    "                    df = self.ingestion.load_data() \n",
    "                    if df.limit(1).count() == 0:\n",
    "                         self.logger.error(f\"Iteration {i}: Data ingestion resulted in empty DataFrame. Skipping iteration.\")\n",
    "                         continue # Skip to next iteration\n",
    "                    self.evaluator.record_time(\"Ingestion\")\n",
    "\n",
    "                    # --- Split, Calculate Min/Max (runs every iteration) ---\n",
    "                    self.evaluator.start_timer(\"Split_MinMax\")\n",
    "                    min_max_values = compute_min_max(df) \n",
    "                    self.logger.info(f\"Iteration {i}: Computed Min-Max values.\") \n",
    "                    train_df, test_df = randomSplit_stratified_via_sampleBy(df, label_col=\"label\", weights=[0.8, 0.2], seed=123)       \n",
    "                    self.evaluator.record_time(\"Split_MinMax\")\n",
    "                    \n",
    "                    # --- Preprocessing Train Data (runs every iteration, includes repartitioning) ---\n",
    "                    self.evaluator.start_timer(\"Preprocessing_Train\")\n",
    "                    target_partitions = current_iter_config.get(\"local_model_config\" if iteration_run_local else \"global_model_config\", {}).get(\"num_partitions\", \"N/A\")\n",
    "                    self.logger.info(f\"Iteration {i}: Preprocessing train data (target partitions={target_partitions})...\")\n",
    "                    preprocessed_train_df = self.preprocessor.run_preprocessing(train_df, min_max_values) \n",
    "                    train_count = preprocessed_train_df.count() # Action to materialize preprocessing\n",
    "                    self.logger.info(f\"Iteration {i}: Preprocessed train data count: {train_count}\")\n",
    "                    self.evaluator.record_time(\"Preprocessing_Train\")\n",
    "\n",
    "                    # --- Preprocessing Test Data (runs every iteration, includes repartitioning) ---\n",
    "                    self.evaluator.start_timer(\"Preprocessing_Test\")\n",
    "                    self.logger.info(f\"Iteration {i}: Preprocessing test data (target partitions={target_partitions})...\")\n",
    "                    preprocessed_test_df = self.preprocessor.run_preprocessing(test_df, min_max_values)\n",
    "                    test_count = preprocessed_test_df.count() # Action\n",
    "                    self.logger.info(f\"Iteration {i}: Preprocessed test data count: {test_count}\")\n",
    "                    self.evaluator.record_time(\"Preprocessing_Test\")\n",
    "\n",
    "                    if train_count == 0 or test_count == 0:\n",
    "                        self.logger.error(f\"Iteration {i}: Preprocessing resulted in empty train or test set. Skipping model steps.\")\n",
    "                        continue \n",
    "\n",
    "                except Exception as e:\n",
    "                     # Handle potential errors during data loading/preprocessing\n",
    "                     if \"Path does not exist\" in str(e):\n",
    "                          spark_path_attempt = self.ingestion_config.get(\"data_path\", \"N/A\") \n",
    "                          self.logger.error(f\"Iteration {i}: Spark failed to find data file during load. Path: {spark_path_attempt}. Error: {e}\", exc_info=False) \n",
    "                     elif isinstance(e, (ConnectionRefusedError, ConnectionResetError)) or \"Connection reset by peer\" in str(e):\n",
    "                          self.logger.error(f\"Iteration {i}: Connection error during data processing: {e}\", exc_info=True)\n",
    "                     else:\n",
    "                          self.logger.error(f\"Iteration {i}: Error during data processing: {e}\", exc_info=True) \n",
    "                     continue # Skip to next iteration\n",
    "                finally:\n",
    "                     # Clean up intermediate raw dataframes for this iteration\n",
    "                     if 'df' in locals(): del df\n",
    "                     if 'train_df' in locals(): del train_df\n",
    "                     if 'test_df' in locals(): del test_df\n",
    "\n",
    "\n",
    "                # Define variables to hold model/prediction results for the iteration\n",
    "                model_ensamble = None \n",
    "                predictions_df = None \n",
    "                \n",
    "                # =================== GLOBAL MODEL ===================\n",
    "                if iteration_run_global:\n",
    "                    # ... (Global model block remains the same) ...\n",
    "                    self.global_model_manager = None \n",
    "                    model_ensamble = None \n",
    "                    global_report = None \n",
    "                    try:\n",
    "                        # Pass the specific global config for this iteration\n",
    "                        global_config = current_iter_config.get(\"global_model_config\", {}) \n",
    "                        if not global_config:\n",
    "                             self.logger.error(f\"Iteration {i}: global_model_config missing. Skipping global model.\")\n",
    "                        else:\n",
    "                            self.global_model_manager = GlobalModelManager(spark=self.spark, config=global_config) \n",
    "                            \n",
    "                            print(f\"\\nIteration {i}: Train global model......\")\n",
    "                            self.evaluator.start_timer(\"Global_Training\")\n",
    "                            model_ensamble = self.global_model_manager.fit(preprocessed_train_df) \n",
    "                            self.evaluator.record_time(\"Global_Training\")\n",
    "                            print(f\"Iteration {i}: Finish Global Training.\")\n",
    "\n",
    "                            if model_ensamble and hasattr(model_ensamble, 'tree') and len(model_ensamble.tree) > 1:\n",
    "                                self.logger.info(f\"Iteration {i}: Global model training successful.\")\n",
    "                                \n",
    "                                print(f\"\\nIteration {i}: Generate predictions with global model......\")\n",
    "                                self.evaluator.start_timer(\"Global_Prediction\")\n",
    "                                predictions_df = predict_with_global_prox_tree(model_ensamble, preprocessed_test_df) \n",
    "                                self.evaluator.record_time(\"Global_Prediction\")\n",
    "                                print(f\"Iteration {i}: Finish Global Prediction.\")\n",
    "                                \n",
    "                                print(f\"\\nIteration {i}: Global Model Predictions Distribution:\")\n",
    "                                predictions_df.groupBy(\"prediction\").count().show() \n",
    "\n",
    "                                print(f\"\\nIteration {i}: Generate metrics for global model......\")\n",
    "                                global_report, class_names = self.evaluator.log_metrics(predictions_df, model=model_ensamble) \n",
    "                                all_reports_global[str(i)] = global_report \n",
    "                                print(f\"Iteration {i}: Finish Global Evaluation.\")\n",
    "                                \n",
    "                                depth = global_config.get(\"tree_params\", {}).get(\"max_depth\", \"NA\")\n",
    "                                \n",
    "                                model_folder = \"models_global\" \n",
    "                                os.makedirs(model_folder, exist_ok=True) \n",
    "                                model_filename = f\"global_model_iter_{i}_parti_{i}_{current_datetime}_depth_{depth}.pkl\" \n",
    "                                model_save_path = os.path.join(model_folder, model_filename)\n",
    "                                try:\n",
    "                                    self.global_model_manager.save_tree(model_save_path) \n",
    "                                    print(f\"Saved global model to {model_save_path}\")\n",
    "                                except Exception as e: self.logger.error(f\"Failed to save global model {model_filename}: {e}\")\n",
    "\n",
    "                            else:\n",
    "                                self.logger.warning(f\"Iteration {i}: Global model training failed or resulted in trivial tree.\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Iteration {i}: Error during global model processing: {e}\", exc_info=True) \n",
    "                    finally:\n",
    "                        # Clean up global model objects specific to this iteration\n",
    "                        self.global_model_manager = None \n",
    "                        if 'model_ensamble' in locals() and model_ensamble is not None: del model_ensamble\n",
    "                        if not iteration_run_local and 'predictions_df' in locals() and predictions_df is not None: del predictions_df \n",
    "                        self.logger.debug(f\"Iteration {i}: Cleaned up global model objects.\")\n",
    "\n",
    "\n",
    "                # =================== LOCAL MODEL ===================\n",
    "                if iteration_run_local:\n",
    "                    # ... (Local model block remains the same) ...\n",
    "                    self.local_model_manager = None \n",
    "                    model_ensamble = None \n",
    "                    if iteration_run_global and 'predictions_df' in locals(): predictions_df = None \n",
    "                    local_report = None \n",
    "                    try:\n",
    "                        # Pass the specific local config for this iteration\n",
    "                        local_config = current_iter_config.get(\"local_model_config\", {})\n",
    "                        if not local_config:\n",
    "                            self.logger.error(f\"Iteration {i}: local_model_config missing. Skipping local model.\")\n",
    "                        else: \n",
    "                            self.local_model_manager = LocalModelManager(config=local_config) \n",
    "                            \n",
    "                            print(f\"\\nIteration {i}: Train local model with {i} partitions......\")\n",
    "                            self.evaluator.start_timer(\"Local_Training\")\n",
    "                            model_ensamble = self.local_model_manager.train_ensemble(preprocessed_train_df) \n",
    "                            self.evaluator.record_time(\"Local_Training\")\n",
    "                            print(f\"Iteration {i}: Finish Local Training.\")\n",
    "                            \n",
    "                            if model_ensamble is not None and hasattr(model_ensamble, 'trees_') and model_ensamble.trees_:\n",
    "                                self.logger.info(f\"Iteration {i}: Local model training successful.\")\n",
    "                                \n",
    "                                print(f\"\\nIteration {i}: Generate predictions with local model......\")\n",
    "                                self.evaluator.start_timer(\"Local_Prediction\")\n",
    "                                self.predictor = PredictionManager(self.spark, model_ensamble) \n",
    "                                predictions_df = self.predictor.generate_predictions_local(preprocessed_test_df) \n",
    "                                self.evaluator.record_time(\"Local_Prediction\")\n",
    "                                print(f\"Iteration {i}: Finish Local Prediction.\")\n",
    "                            \n",
    "                                print(f\"\\nIteration {i}: Local model Predictions Distribution:\")\n",
    "                                predictions_df.groupBy(\"prediction\").count().show() \n",
    "\n",
    "                                print(f\"\\nIteration {i}: Generate metrics for local model......\")\n",
    "                                local_report, class_names = self.evaluator.log_metrics(predictions_df, model=model_ensamble)\n",
    "                                all_reports_local[str(i)] = local_report \n",
    "                                print(f\"Iteration {i}: Finish Local Evaluation.\")\n",
    "                                \n",
    "                                depth = local_config.get(\"tree_params\", {}).get(\"max_depth\", \"NA\") \n",
    "                                \n",
    "                                model_folder = \"models_local\" \n",
    "                                os.makedirs(model_folder, exist_ok=True) \n",
    "                                model_filename = f\"local_model_iter_{i}_parti_{i}_{current_datetime}_depth_{depth}.pkl\" \n",
    "                                model_save_path = os.path.join(model_folder, model_filename)\n",
    "                                try:\n",
    "                                    with open(model_save_path, 'wb') as f: pickle.dump(model_ensamble, f) \n",
    "                                    print(f\"Saved local model ensemble to {model_save_path}\")\n",
    "                                except Exception as e: self.logger.error(f\"Failed to save local model {model_filename}: {e}\")\n",
    "                                \n",
    "                            else:\n",
    "                                self.logger.warning(f\"Iteration {i}: Local model training failed or resulted in empty ensemble.\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                         self.logger.error(f\"Iteration {i}: Error during local model processing: {e}\", exc_info=True) \n",
    "                    finally:\n",
    "                        # Clean up local model objects specific to this iteration\n",
    "                        self.local_model_manager = None \n",
    "                        if 'model_ensamble' in locals() and model_ensamble is not None: del model_ensamble\n",
    "                        if 'predictions_df' in locals() and predictions_df is not None: del predictions_df\n",
    "                        self.logger.debug(f\"Iteration {i}: Cleaned up local model objects.\")\n",
    "\n",
    "\n",
    "                # --- Iteration End ---\n",
    "                # Cleanup preprocessed DataFrames for this iteration \n",
    "                try:\n",
    "                     if 'preprocessed_train_df' in locals() and preprocessed_train_df is not None: \n",
    "                          # preprocessed_train_df.unpersist() # Caching removed, unpersist not needed\n",
    "                          del preprocessed_train_df\n",
    "                     if 'preprocessed_test_df' in locals() and preprocessed_test_df is not None: \n",
    "                          # preprocessed_test_df.unpersist() # Caching removed, unpersist not needed\n",
    "                          del preprocessed_test_df\n",
    "                except Exception as cleanup_e:\n",
    "                     self.logger.warning(f\"Iteration {i}: Error during DataFrame cleanup: {cleanup_e}\")\n",
    "\n",
    "                \n",
    "                self.logger.info(f\"========== Finished Iteration {i} ==========\")\n",
    "                # Optional delay\n",
    "                if self.config.get('delay_time', 0) > 0 and i < end_iteration : \n",
    "                    print(f\"\\nIteration {i}: Waiting for {self.config['delay_time']} seconds before next iteration...\\n\")\n",
    "                    time.sleep(self.config['delay_time'])\n",
    "\n",
    "                # *** Spark session is NOT stopped here anymore ***\n",
    "\n",
    "            # End of main loop\n",
    "\n",
    "        # ============================================================\n",
    "        # === Step 3: Save Accumulated Reports After Loop ===\n",
    "        # ============================================================\n",
    "        finally: # Use finally to ensure Spark stops even if loop errors out\n",
    "             final_datetime = time.strftime(\"%Y-%m-%d-%H-%M-%S\") \n",
    "            \n",
    "             if all_reports_global:\n",
    "                  report_folder = \"logs\" \n",
    "                  os.makedirs(report_folder, exist_ok=True) \n",
    "                  report_filename_global = f\"report_global_model_ALL_{final_datetime}.json\" \n",
    "                  report_save_path_global = os.path.join(report_folder, report_filename_global)\n",
    "                  try:\n",
    "                      with open(report_save_path_global, \"w\") as f: json.dump(all_reports_global, f, indent=2)\n",
    "                      print(f\"Saved ALL global model reports to {report_save_path_global}\") \n",
    "                  except Exception as e: self.logger.error(f\"Failed to save aggregated global report: {e}\")\n",
    "\n",
    "             if all_reports_local:\n",
    "                  report_folder = \"logs\"\n",
    "                  os.makedirs(report_folder, exist_ok=True) \n",
    "                  report_filename_local = f\"report_local_model_ALL_{final_datetime}.json\" \n",
    "                  report_save_path_local = os.path.join(report_folder, report_filename_local)\n",
    "                  try:\n",
    "                      with open(report_save_path_local, \"w\") as f: json.dump(all_reports_local, f, indent=2)\n",
    "                      print(f\"Saved ALL local model reports to {report_save_path_local}\") \n",
    "                  except Exception as e: self.logger.error(f\"Failed to save aggregated local report: {e}\")\n",
    "\n",
    "\n",
    "             # --- Pipeline End ---\n",
    "             # Final Spark stop if running locally \n",
    "             if \"DATABRICKS_RUNTIME_VERSION\" not in os.environ and self.spark:\n",
    "                  self.spark.stop()\n",
    "                  self.spark = None # Reset attribute\n",
    "                  print(f\"\\nFinal Spark session stopped (local mode)!\")\n",
    "\n",
    "             print(\"\\n--- Pipeline execution finished ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6560f5e4-f4d9-4d69-8003-5fb520b885e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# config.py\n",
    "\"\"\"\n",
    "config.py\n",
    "\n",
    "This file holds configuration settings and constants.\n",
    "It stores paths, hyperparameters, and Spark settings in one place,\n",
    "so they can be easily managed and updated as the project grows.\n",
    "Typically, it's created early on, but it can be refined later.\n",
    "\"\"\"\n",
    "\n",
    "config = {\n",
    "    \"databricks_data_path\" : \"/mnt/2025-team6/fulldataset_ECG5000.csv\",\n",
    "    \"local_data_path\" : \"/fulldataset_ECG5000.csv\",\n",
    "    \"label_col\" : \"label\",\n",
    "    \"data_percentage\" : 1.0,\n",
    "    \"min_number_iterarations\" : 1, # Minimum number of iterations for the loop\n",
    "    \"delay_time\" : 3,\n",
    "    \n",
    "    \"local_model_config\": {\n",
    "        \"test_local_model\" : False,\n",
    "        \"num_partitions\": 2,  # loop to this number of partitions - This is the number of partitions for the local model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": 1,  #TODO: NONE\n",
    "            \"min_samples_split\": 5,  # From ProximityTree default\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "        \"forest_params\": {\n",
    "            \"random_state\": 123,\n",
    "            \"n_jobs\": -1  # Use all available cores\n",
    "            }\n",
    "    },\n",
    "    \"global_model_config\": {\n",
    "        \"test_global_model\" : True,\n",
    "        \"num_partitions\": 1,  # loop to this number of partitions - This is the number of partitions for the global model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": None,  \n",
    "            \"min_samples_split\": 5,  # From ProximityTree defaults\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "    },\n",
    "    \"reserve_partition_id\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb080e3a-3da4-4476-bd6a-da972cf1c903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting pipeline via controller\")\n",
    "config = config\n",
    "try:\n",
    "  controller = PipelineController_Loop(config)\n",
    "  controller.run()\n",
    "except Exception as err:                   # <-- catches anything that went wrong\n",
    "    print(\"Pipeline failed:\", err)         # <-- quick, human‑readable message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94999fb6-961c-4cf3-9cab-7a45cb59a955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# config.py\n",
    "\"\"\"\n",
    "config.py\n",
    "\n",
    "This file holds configuration settings and constants.\n",
    "It stores paths, hyperparameters, and Spark settings in one place,\n",
    "so they can be easily managed and updated as the project grows.\n",
    "Typically, it's created early on, but it can be refined later.\n",
    "\"\"\"\n",
    "\n",
    "config = {\n",
    "    \"databricks_data_path\" : \"/mnt/2025-team6/fulldataset_ECG5000.csv\",\n",
    "    \"local_data_path\" : \"/fulldataset_ECG5000.csv\",\n",
    "    \"label_col\" : \"label\",\n",
    "    \"data_percentage\" : 1.0,\n",
    "    \"min_number_iterarations\" : 13, # Minimum number of iterations for the loop\n",
    "    \"delay_time\" : 3,\n",
    "    \n",
    "    \"local_model_config\": {\n",
    "        \"test_local_model\" : False,\n",
    "        \"num_partitions\": 2,  # loop to this number of partitions - This is the number of partitions for the local model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": 1,  #TODO: NONE\n",
    "            \"min_samples_split\": 5,  # From ProximityTree default\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "        \"forest_params\": {\n",
    "            \"random_state\": 123,\n",
    "            \"n_jobs\": -1  # Use all available cores\n",
    "            }\n",
    "    },\n",
    "    \"global_model_config\": {\n",
    "        \"test_global_model\" : True,\n",
    "        \"num_partitions\": 14,  # loop to this number of partitions - This is the number of partitions for the global model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": None,  \n",
    "            \"min_samples_split\": 5,  # From ProximityTree defaults\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "    },\n",
    "    \"reserve_partition_id\": False\n",
    "}\n",
    "print(\"Starting pipeline via controller\")\n",
    "config = config\n",
    "try:\n",
    "    controller = PipelineController_Loop(config)\n",
    "    controller.run()\n",
    "except Exception as err:                   # <-- catches anything that went wrong\n",
    "    print(\"Pipeline failed:\", err)         # <-- quick, human‑readable message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d28c38-da6c-423d-aa72-71ece733cf86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# config.py\n",
    "\"\"\"\n",
    "config.py\n",
    "\n",
    "This file holds configuration settings and constants.\n",
    "It stores paths, hyperparameters, and Spark settings in one place,\n",
    "so they can be easily managed and updated as the project grows.\n",
    "Typically, it's created early on, but it can be refined later.\n",
    "\"\"\"\n",
    "\n",
    "config = {\n",
    "    \"databricks_data_path\" : \"/mnt/2025-team6/fulldataset_ECG5000.csv\",\n",
    "    \"local_data_path\" : \"/fulldataset_ECG5000.csv\",\n",
    "    \"label_col\" : \"label\",\n",
    "    \"data_percentage\" : 1.0,\n",
    "    \"min_number_iterarations\" : 15, # Minimum number of iterations for the loop\n",
    "    \"delay_time\" : 3,\n",
    "    \n",
    "    \"local_model_config\": {\n",
    "        \"test_local_model\" : False,\n",
    "        \"num_partitions\": 2,  # loop to this number of partitions - This is the number of partitions for the local model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": 1,  #TODO: NONE\n",
    "            \"min_samples_split\": 5,  # From ProximityTree default\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "        \"forest_params\": {\n",
    "            \"random_state\": 123,\n",
    "            \"n_jobs\": -1  # Use all available cores\n",
    "            }\n",
    "    },\n",
    "    \"global_model_config\": {\n",
    "        \"test_global_model\" : True,\n",
    "        \"num_partitions\": 16,  # loop to this number of partitions - This is the number of partitions for the global model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": None,  \n",
    "            \"min_samples_split\": 5,  # From ProximityTree defaults\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "    },\n",
    "    \"reserve_partition_id\": False\n",
    "}\n",
    "print(\"Starting pipeline via controller\")\n",
    "config = config\n",
    "try:\n",
    "    controller = PipelineController_Loop(config)\n",
    "    controller.run()\n",
    "except Exception as err:                   # <-- catches anything that went wrong\n",
    "    print(\"Pipeline failed:\", err)         # <-- quick, human‑readable message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39ce0bf4-4186-4f75-a6f6-1ecabbc4b115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# config.py\n",
    "\"\"\"\n",
    "config.py\n",
    "\n",
    "This file holds configuration settings and constants.\n",
    "It stores paths, hyperparameters, and Spark settings in one place,\n",
    "so they can be easily managed and updated as the project grows.\n",
    "Typically, it's created early on, but it can be refined later.\n",
    "\"\"\"\n",
    "\n",
    "config = {\n",
    "    \"databricks_data_path\" : \"/mnt/2025-team6/fulldataset_ECG5000.csv\",\n",
    "    \"local_data_path\" : \"/fulldataset_ECG5000.csv\",\n",
    "    \"label_col\" : \"label\",\n",
    "    \"data_percentage\" : 1.0,\n",
    "    \"min_number_iterarations\" : 17, # Minimum number of iterations for the loop\n",
    "    \"delay_time\" : 3,\n",
    "    \n",
    "    \"local_model_config\": {\n",
    "        \"test_local_model\" : False,\n",
    "        \"num_partitions\": 2,  # loop to this number of partitions - This is the number of partitions for the local model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": 1,  #TODO: NONE\n",
    "            \"min_samples_split\": 5,  # From ProximityTree default\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "        \"forest_params\": {\n",
    "            \"random_state\": 123,\n",
    "            \"n_jobs\": -1  # Use all available cores\n",
    "            }\n",
    "    },\n",
    "    \"global_model_config\": {\n",
    "        \"test_global_model\" : True,\n",
    "        \"num_partitions\": 18,  # loop to this number of partitions - This is the number of partitions for the global model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": None,  \n",
    "            \"min_samples_split\": 5,  # From ProximityTree defaults\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "    },\n",
    "    \"reserve_partition_id\": False\n",
    "}\n",
    "print(\"Starting pipeline via controller\")\n",
    "config = config\n",
    "try:\n",
    "    controller = PipelineController_Loop(config)\n",
    "    controller.run()\n",
    "except Exception as err:                   # <-- catches anything that went wrong\n",
    "    print(\"Pipeline failed:\", err)         # <-- quick, human‑readable message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ead1407-7217-4e7a-b402-77c7eb5e590a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# config.py\n",
    "\"\"\"\n",
    "config.py\n",
    "\n",
    "This file holds configuration settings and constants.\n",
    "It stores paths, hyperparameters, and Spark settings in one place,\n",
    "so they can be easily managed and updated as the project grows.\n",
    "Typically, it's created early on, but it can be refined later.\n",
    "\"\"\"\n",
    "\n",
    "config = {\n",
    "    \"databricks_data_path\" : \"/mnt/2025-team6/fulldataset_ECG5000.csv\",\n",
    "    \"local_data_path\" : \"/fulldataset_ECG5000.csv\",\n",
    "    \"label_col\" : \"label\",\n",
    "    \"data_percentage\" : 1.0,\n",
    "    \"min_number_iterarations\" : 19, # Minimum number of iterations for the loop\n",
    "    \"delay_time\" : 3,\n",
    "    \n",
    "    \"local_model_config\": {\n",
    "        \"test_local_model\" : False,\n",
    "        \"num_partitions\": 2,  # loop to this number of partitions - This is the number of partitions for the local model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": 1,  #TODO: NONE\n",
    "            \"min_samples_split\": 5,  # From ProximityTree default\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "        \"forest_params\": {\n",
    "            \"random_state\": 123,\n",
    "            \"n_jobs\": -1  # Use all available cores\n",
    "            }\n",
    "    },\n",
    "    \"global_model_config\": {\n",
    "        \"test_global_model\" : True,\n",
    "        \"num_partitions\": 20,  # loop to this number of partitions - This is the number of partitions for the global model\n",
    "        \"tree_params\": {\n",
    "            \"n_splitters\": 5,  # Matches ProximityTree default\n",
    "            \"max_depth\": None,  \n",
    "            \"min_samples_split\": 5,  # From ProximityTree defaults\n",
    "            \"random_state\": 123\n",
    "            },\n",
    "    },\n",
    "    \"reserve_partition_id\": False\n",
    "}\n",
    "print(\"Starting pipeline via controller\")\n",
    "config = config\n",
    "try:\n",
    "    controller = PipelineController_Loop(config)\n",
    "    controller.run()\n",
    "except Exception as err:                   # <-- catches anything that went wrong\n",
    "    print(\"Pipeline failed:\", err)         # <-- quick, human‑readable message"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "main",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
