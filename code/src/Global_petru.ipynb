{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc7c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "from aeon.classification.distance_based import ProximityTree, ProximityForest\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from data_ingestion import DataIngestion\n",
    "from preprocessing import Preprocessor\n",
    "from prediction_manager import PredictionManager\n",
    "from local_model_manager import LocalModelManager\n",
    "from evaluation import Evaluator\n",
    "from utilities import show_compact\n",
    "import time\n",
    "import json\n",
    "from random import sample\n",
    "from dtaidistance import dtw\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"GenericRDD\").getOrCreate()\n",
    "\n",
    "# Access the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30b2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]},\n",
    "    {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
    "    {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
    "    {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
    "    {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
    "    {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
    "    {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
    "    {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
    "    {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
    "    {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.4, 1.9, 2.4, 2.9]},\n",
    "    {'label': 1, 'time_series': [1.4, 2.0, 2.6, 3.2, 3.8]},\n",
    "    {'label': 2, 'time_series': [2.2, 3.1, 4.0, 4.9, 5.8]},\n",
    "    {'label': 2, 'time_series': [2.6, 3.2, 3.8, 4.4, 5.0]},\n",
    "    {'label': 3, 'time_series': [1.2, 2.0, 2.8, 3.6, 4.4]},\n",
    "    {'label': 3, 'time_series': [0.6, 1.3, 2.0, 2.7, 3.4]},\n",
    "    {'label': 4, 'time_series': [6.3, 6.5, 6.7, 6.9, 7.1]},\n",
    "    {'label': 4, 'time_series': [7.0, 7.8, 8.6, 9.4, 10.2]},\n",
    "    {'label': 4, 'time_series': [6.5, 7.0, 7.5, 8.0, 8.5]},\n",
    "    {'label': 1, 'time_series': [0.5, 1.0, 1.5, 2.0, 2.5]},\n",
    "    {'label': 2, 'time_series': [0.6, 1.4, 1.3, 2.1, 2.5]},\n",
    "    {'label': 3, 'time_series': [0.3, 1.7, 1.6, 2.2, 2.6]},\n",
    "    {'label': 4, 'time_series': [0.2, 1.9, 1.6, 2.3, 2.7]},\n",
    "    {'label': 4, 'time_series': [0.9, 1.7, 1.2, 2.4, 2.8]}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(tsdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669a74f",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee4551f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5], 'partition_id': 0},\n",
       " {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5], 'partition_id': 0},\n",
       " {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5], 'partition_id': 0},\n",
       " {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [2.2, 3.1, 4.0, 4.9, 5.8], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [2.6, 3.2, 3.8, 4.4, 5.0], 'partition_id': 0},\n",
       " {'label': 3, 'time_series': [1.2, 2.0, 2.8, 3.6, 4.4], 'partition_id': 0},\n",
       " {'label': 3, 'time_series': [0.6, 1.3, 2.0, 2.7, 3.4], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [6.3, 6.5, 6.7, 6.9, 7.1], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [7.0, 7.8, 8.6, 9.4, 10.2], 'partition_id': 0},\n",
       " {'label': 2, 'time_series': [0.6, 1.4, 1.3, 2.1, 2.5], 'partition_id': 0},\n",
       " {'label': 3, 'time_series': [0.3, 1.7, 1.6, 2.2, 2.6], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [0.2, 1.9, 1.6, 2.3, 2.7], 'partition_id': 0},\n",
       " {'label': 4, 'time_series': [0.9, 1.7, 1.2, 2.4, 2.8], 'partition_id': 0},\n",
       " {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0], 'partition_id': 1},\n",
       " {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [0.9, 1.4, 1.9, 2.4, 2.9], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [1.4, 2.0, 2.6, 3.2, 3.8], 'partition_id': 1},\n",
       " {'label': 4, 'time_series': [6.5, 7.0, 7.5, 8.0, 8.5], 'partition_id': 1},\n",
       " {'label': 1, 'time_series': [0.5, 1.0, 1.5, 2.0, 2.5], 'partition_id': 1}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repartition_sparkdf(df, num_partitions):\n",
    "    rdd = df.rdd\n",
    "    rdd = rdd.repartition(num_partitions)\n",
    "    rdd = rdd.mapPartitionsWithIndex(\n",
    "            lambda idx, iter: [{**row.asDict(), \"partition_id\": idx} for row in iter]\n",
    "        )\n",
    "    return rdd\n",
    "\n",
    "# example usage\n",
    "rdd = repartition_sparkdf(df, 2)\n",
    "print(rdd.getNumPartitions())  # should print 1\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be92f01",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee6bc902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 1,\n",
       "  'time_series': [0.8, 1.7, 2.5, 3.2, 4.0],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 2,\n",
       "  'time_series': [3.0, 3.8, 4.6, 5.4, 6.2],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 2,\n",
       "  'time_series': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 3,\n",
       "  'time_series': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 4,\n",
       "  'time_series': [5.5, 6.6, 7.7, 8.8, 9.9],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 4,\n",
       "  'time_series': [6.1, 6.2, 6.3, 6.4, 6.5],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [0.7, 1.3, 1.9, 2.5, 3.1],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 2,\n",
       "  'time_series': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 3,\n",
       "  'time_series': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 4,\n",
       "  'time_series': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [1.3, 2.3, 3.3, 4.3, 5.3],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 2,\n",
       "  'time_series': [2.2, 3.1, 4.0, 4.9, 5.8],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 2,\n",
       "  'time_series': [2.6, 3.2, 3.8, 4.4, 5.0],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 3,\n",
       "  'time_series': [1.2, 2.0, 2.8, 3.6, 4.4],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 3,\n",
       "  'time_series': [0.6, 1.3, 2.0, 2.7, 3.4],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 4,\n",
       "  'time_series': [7.0, 7.8, 8.6, 9.4, 10.2],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 2,\n",
       "  'time_series': [0.6, 1.4, 1.3, 2.1, 2.5],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 3,\n",
       "  'time_series': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 4,\n",
       "  'time_series': [0.2, 1.9, 1.6, 2.3, 2.7],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 4,\n",
       "  'time_series': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [6.5, 7.0, 7.5, 8.0, 8.5],\n",
       "  'exemplar_3_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [6.5, 7.0, 7.5, 8.0, 8.5],\n",
       "  'exemplar_3_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [0.9, 1.8, 2.7, 3.6, 4.5],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [6.5, 7.0, 7.5, 8.0, 8.5],\n",
       "  'exemplar_3_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [1.5, 2.1, 2.7, 3.3, 3.9],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [6.5, 7.0, 7.5, 8.0, 8.5],\n",
       "  'exemplar_3_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [0.6, 1.2, 1.8, 2.4, 3.0],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [6.5, 7.0, 7.5, 8.0, 8.5],\n",
       "  'exemplar_3_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [6.5, 7.0, 7.5, 8.0, 8.5],\n",
       "  'exemplar_3_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [0.5, 1.0, 1.5, 2.0, 2.5],\n",
       "  'partition_id': 1,\n",
       "  'exemplar_1': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [6.5, 7.0, 7.5, 8.0, 8.5],\n",
       "  'exemplar_3_label': 4}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_exemplars(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    if not partition_data:\n",
    "        return iter([])\n",
    "    \n",
    "    # Group data by class\n",
    "    grouped_data_by_class = {}\n",
    "    for row in partition_data:\n",
    "        label = row.get('label')\n",
    "        if label is not None:\n",
    "            grouped_data_by_class.setdefault(label, []).append(row)\n",
    "    \n",
    "    # Select one exemplar per class and track exemplar-to-label mapping\n",
    "    chosen_exemplars = []\n",
    "    exemplar_labels = {}  # Map to track exemplar labels\n",
    "    \n",
    "    for label, instances in grouped_data_by_class.items():\n",
    "        if instances:  # Ensure there are instances for the class\n",
    "            exemplar = sample(instances, 1)[0]\n",
    "            chosen_exemplars.append((exemplar['time_series'], label))  # Store tuple of (time_series, label)\n",
    "    \n",
    "    # Remove chosen exemplars from the working data\n",
    "    exemplar_time_series = [ex[0] for ex in chosen_exemplars]\n",
    "    filtered_partition = [\n",
    "        row for row in partition_data\n",
    "        if row['time_series'] not in exemplar_time_series\n",
    "    ]\n",
    "    \n",
    "    # Return rows with individual exemplar columns and their labels\n",
    "    result = []\n",
    "    for row in filtered_partition:\n",
    "        new_row = {**row}\n",
    "        # Add each exemplar and its label as columns\n",
    "        for i, (exemplar, label) in enumerate(chosen_exemplars, 1):\n",
    "            new_row[f\"exemplar_{i}\"] = exemplar\n",
    "            new_row[f\"exemplar_{i}_label\"] = label\n",
    "        result.append(new_row)\n",
    "    \n",
    "    return iter(result)\n",
    "\n",
    "# example usage\n",
    "rdd_with_exemplars = rdd.mapPartitions(choose_exemplars)\n",
    "rdd_with_exemplars.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a538e",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da68b739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 1,\n",
       "  'time_series': [0.8, 1.7, 2.5, 3.2, 4.0],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4,\n",
       "  'dtw_distance_exemplar_1': 1.2806248474865694,\n",
       "  'dtw_distance_exemplar_2': 3.683748091278773,\n",
       "  'dtw_distance_exemplar_3': 1.2884098726725126,\n",
       "  'dtw_distance_exemplar_4': 9.707213812418061},\n",
       " {'label': 2,\n",
       "  'time_series': [3.0, 3.8, 4.6, 5.4, 6.2],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4,\n",
       "  'dtw_distance_exemplar_1': 2.463736998950984,\n",
       "  'dtw_distance_exemplar_2': 1.284523257866513,\n",
       "  'dtw_distance_exemplar_3': 2.9154759474226504,\n",
       "  'dtw_distance_exemplar_4': 4.741307836451879},\n",
       " {'label': 2,\n",
       "  'time_series': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'partition_id': 0,\n",
       "  'exemplar_1': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_1_label': 1,\n",
       "  'exemplar_2': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_2_label': 2,\n",
       "  'exemplar_3': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_3_label': 3,\n",
       "  'exemplar_4': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_4_label': 4,\n",
       "  'dtw_distance_exemplar_1': 2.946183972531247,\n",
       "  'dtw_distance_exemplar_2': 1.385640646055102,\n",
       "  'dtw_distance_exemplar_3': 3.5242020373412193,\n",
       "  'dtw_distance_exemplar_4': 4.089009660052175}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_dtw_distance(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        time_series = row.get('time_series', [])\n",
    "        \n",
    "        # Check for individual exemplar columns (exemplar_1, exemplar_2, etc.)\n",
    "        exemplar_columns = {k: v for k, v in row.items() if k.startswith('exemplar_') and isinstance(v, list)}\n",
    "        \n",
    "        if not exemplar_columns:\n",
    "            # Try to get exemplars from the 'exemplars' list if individual columns aren't found\n",
    "            exemplars = row.get('exemplars', [])\n",
    "            if not exemplars:\n",
    "                continue  # Skip if no exemplars found\n",
    "            \n",
    "            # Calculate DTW distances for each exemplar in the list\n",
    "            updated_row = {**row}\n",
    "            for i, exemplar in enumerate(exemplars):\n",
    "                dtw_distance = dtw.distance(time_series, exemplar)\n",
    "                updated_row[f\"dtw_distance_exemplar_{i+1}\"] = dtw_distance\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "        else:\n",
    "            # Calculate DTW distances for each exemplar column\n",
    "            updated_row = {**row}\n",
    "            for col_name, exemplar in exemplar_columns.items():\n",
    "                # Extract index from column name (e.g., \"exemplar_1\" -> \"1\")\n",
    "                idx = col_name.split('_')[1]\n",
    "                dtw_distance = dtw.distance(time_series, exemplar)\n",
    "                updated_row[f\"dtw_distance_exemplar_{idx}\"] = dtw_distance\n",
    "            \n",
    "            updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "# example usage\n",
    "rdd_with_dtw = rdd_with_exemplars.mapPartitions(calc_dtw_distance)\n",
    "rdd_with_dtw.collect()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6deac",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04397cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 1,\n",
       "  'time_series': [0.8, 1.7, 2.5, 3.2, 4.0],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 2,\n",
       "  'time_series': [3.0, 3.8, 4.6, 5.4, 6.2],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 2,\n",
       "  'time_series': [3.3, 4.1, 4.9, 5.7, 6.5],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 3,\n",
       "  'time_series': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 4,\n",
       "  'time_series': [5.5, 6.6, 7.7, 8.8, 9.9],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 4,\n",
       "  'time_series': [6.1, 6.2, 6.3, 6.4, 6.5],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [0.7, 1.3, 1.9, 2.5, 3.1],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'closest_exemplar_original_label': 3},\n",
       " {'label': 2,\n",
       "  'time_series': [1.9, 2.8, 3.7, 4.6, 5.5],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 3,\n",
       "  'time_series': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 4,\n",
       "  'time_series': [6.0, 7.0, 8.0, 9.0, 10.0],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 1,\n",
       "  'time_series': [1.3, 2.3, 3.3, 4.3, 5.3],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 2,\n",
       "  'time_series': [2.2, 3.1, 4.0, 4.9, 5.8],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 2,\n",
       "  'time_series': [2.6, 3.2, 3.8, 4.4, 5.0],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'closest_exemplar_original_label': 3},\n",
       " {'label': 3,\n",
       "  'time_series': [1.2, 2.0, 2.8, 3.6, 4.4],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 3,\n",
       "  'time_series': [0.6, 1.3, 2.0, 2.7, 3.4],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'closest_exemplar_original_label': 3},\n",
       " {'label': 4,\n",
       "  'time_series': [7.0, 7.8, 8.6, 9.4, 10.2],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'closest_exemplar_data': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'closest_exemplar_original_label': 4},\n",
       " {'label': 2,\n",
       "  'time_series': [0.6, 1.4, 1.3, 2.1, 2.5],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'closest_exemplar_original_label': 3},\n",
       " {'label': 3,\n",
       "  'time_series': [0.3, 1.7, 1.6, 2.2, 2.6],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'closest_exemplar_original_label': 3},\n",
       " {'label': 4,\n",
       "  'time_series': [0.2, 1.9, 1.6, 2.3, 2.7],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'closest_exemplar_original_label': 3},\n",
       " {'label': 4,\n",
       "  'time_series': [0.9, 1.7, 1.2, 2.4, 2.8],\n",
       "  'partition_id': 0,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'closest_exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'closest_exemplar_original_label': 3},\n",
       " {'label': 1,\n",
       "  'time_series': [1.2, 2.4, 3.6, 4.8, 6.0],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'closest_exemplar_data': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'closest_exemplar_original_label': 2},\n",
       " {'label': 1,\n",
       "  'time_series': [1.0, 1.8, 2.6, 3.4, 4.2],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 1,\n",
       "  'time_series': [0.9, 1.8, 2.7, 3.6, 4.5],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 1,\n",
       "  'time_series': [1.5, 2.1, 2.7, 3.3, 3.9],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 1,\n",
       "  'time_series': [0.6, 1.2, 1.8, 2.4, 3.0],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 1,\n",
       "  'time_series': [1.4, 2.0, 2.6, 3.2, 3.8],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'closest_exemplar_original_label': 1},\n",
       " {'label': 1,\n",
       "  'time_series': [0.5, 1.0, 1.5, 2.0, 2.5],\n",
       "  'partition_id': 1,\n",
       "  'closest_exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'closest_exemplar_data': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'closest_exemplar_original_label': 1}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_closest_exemplar(iterator):\n",
    "    partition_data = list(iterator)\n",
    "    updated_rows = []\n",
    "    \n",
    "    for row in partition_data:\n",
    "        # Get all DTW distances\n",
    "        dtw_distances = {k: v for k, v in row.items() if k.startswith('dtw_distance_exemplar_')}\n",
    "        \n",
    "        if not dtw_distances:\n",
    "            # Create a simplified row without exemplar columns\n",
    "            simplified_row = {k: v for k, v in row.items() \n",
    "                             if not k.startswith('exemplar_')}\n",
    "            updated_rows.append(simplified_row)\n",
    "            continue\n",
    "        \n",
    "        # Find the closest exemplar based on the minimum DTW distance\n",
    "        closest_exemplar_key = min(dtw_distances, key=dtw_distances.get)\n",
    "        min_distance = dtw_distances[closest_exemplar_key]\n",
    "        \n",
    "        # Extract exemplar number from the key (e.g., \"dtw_distance_exemplar_1\" -> \"1\")\n",
    "        exemplar_num = closest_exemplar_key.split('_')[-1]\n",
    "        \n",
    "        # Get the corresponding exemplar time series data and its original label\n",
    "        exemplar_key = f'exemplar_{exemplar_num}'\n",
    "        exemplar_label_key = f'exemplar_{exemplar_num}_label'\n",
    "        exemplar_time_series = row.get(exemplar_key, None)\n",
    "        exemplar_original_label = row.get(exemplar_label_key, None)\n",
    "        \n",
    "        # Create a new row without the DTW distance columns and exemplar columns\n",
    "        updated_row = {k: v for k, v in row.items() \n",
    "                      if not k.startswith('dtw_distance_exemplar_') and not k.startswith('exemplar_')}\n",
    "        \n",
    "        # Add information about the closest exemplar\n",
    "        updated_row['closest_exemplar_id'] = closest_exemplar_key\n",
    "        updated_row['closest_exemplar_data'] = exemplar_time_series\n",
    "        updated_row['closest_exemplar_original_label'] = exemplar_original_label\n",
    "        \n",
    "        updated_rows.append(updated_row)\n",
    "    \n",
    "    return iter(updated_rows)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_closest_exemplar = rdd_with_dtw.mapPartitions(assign_closest_exemplar)\n",
    "rdd_with_closest_exemplar.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471e7d5",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9daa07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'partition_id': 0,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_4',\n",
       "  'exemplar_data': [6.3, 6.5, 6.7, 6.9, 7.1],\n",
       "  'exemplar_label': 4,\n",
       "  'gini_reduction': 0.16625},\n",
       " {'partition_id': 0,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'exemplar_data': [1.1, 2.1, 3.1, 4.1, 5.1],\n",
       "  'exemplar_label': 1,\n",
       "  'gini_reduction': 0.08738095238095245},\n",
       " {'partition_id': 0,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_3',\n",
       "  'exemplar_data': [2.0, 2.5, 3.0, 3.5, 4.0],\n",
       "  'exemplar_label': 3,\n",
       "  'gini_reduction': 0.0009340659340658641},\n",
       " {'partition_id': 0,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'exemplar_data': [2.1, 3.3, 4.5, 5.7, 6.9],\n",
       "  'exemplar_label': 2,\n",
       "  'gini_reduction': 0.11735294117647066},\n",
       " {'partition_id': 1,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_1',\n",
       "  'exemplar_data': [0.9, 1.4, 1.9, 2.4, 2.9],\n",
       "  'exemplar_label': 1,\n",
       "  'gini_reduction': 0.0},\n",
       " {'partition_id': 1,\n",
       "  'exemplar_id': 'dtw_distance_exemplar_2',\n",
       "  'exemplar_data': [2.4, 3.5, 4.6, 5.7, 6.8],\n",
       "  'exemplar_label': 2,\n",
       "  'gini_reduction': 0.0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gini(labels):\n",
    "    if not labels:\n",
    "        return 0\n",
    "    label_counts = {}\n",
    "    for label in labels:\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    total = sum(label_counts.values())\n",
    "    gini = 1 - sum((count / total) ** 2 for count in label_counts.values()) if total > 0 else 0\n",
    "    return gini\n",
    "\n",
    "def evaluate_splits_within_partition(index, iterator):\n",
    "    partition_data = list(iterator)\n",
    "    results = []\n",
    "    \n",
    "    # Calculate Gini impurity before splitting\n",
    "    labels = [row.get('label') for row in partition_data if row.get('label') is not None]\n",
    "    before_split_gini = calculate_gini(labels)\n",
    "    \n",
    "    # Get all unique exemplars in the partition\n",
    "    unique_exemplars = set(\n",
    "        row['closest_exemplar_id'] for row in partition_data\n",
    "        if row.get('closest_exemplar_id') is not None\n",
    "    )\n",
    "    \n",
    "    # Create a mapping of exemplar_id to exemplar_data and exemplar_label\n",
    "    exemplar_data_map = {}\n",
    "    exemplar_label_map = {}\n",
    "    \n",
    "    for row in partition_data:\n",
    "        exemplar_id = row.get('closest_exemplar_id')\n",
    "        if exemplar_id and exemplar_id not in exemplar_data_map:\n",
    "            exemplar_data_map[exemplar_id] = row.get('closest_exemplar_data')\n",
    "            # Use the original exemplar label instead of the row's label\n",
    "            exemplar_label_map[exemplar_id] = row.get('closest_exemplar_original_label')\n",
    "    \n",
    "    # Evaluate all possible splits\n",
    "    for exemplar_id in unique_exemplars:\n",
    "        # Split the data based on the current exemplar\n",
    "        yes_split = [r for r in partition_data if r.get('closest_exemplar_id') == exemplar_id]\n",
    "        no_split = [r for r in partition_data if r.get('closest_exemplar_id') != exemplar_id]\n",
    "        \n",
    "        # Calculate Gini for each daughter node\n",
    "        yes_labels = [r.get('label') for r in yes_split if r.get('label') is not None]\n",
    "        no_labels = [r.get('label') for r in no_split if r.get('label') is not None]\n",
    "        \n",
    "        yes_gini = calculate_gini(yes_labels)\n",
    "        no_gini = calculate_gini(no_labels)\n",
    "        \n",
    "        # Calculate weighted Gini after split\n",
    "        total_size = len(yes_split) + len(no_split)\n",
    "        weighted_gini = (yes_gini * len(yes_split) / total_size + no_gini * len(no_split) / total_size) if total_size > 0 else float('inf')\n",
    "        \n",
    "        # Calculate Gini reduction\n",
    "        gini_reduction = before_split_gini - weighted_gini if total_size > 0 else float('-inf')\n",
    "        \n",
    "        # Get the exemplar data and label for this exemplar_id\n",
    "        exemplar_data = exemplar_data_map.get(exemplar_id)\n",
    "        exemplar_label = exemplar_label_map.get(exemplar_id)\n",
    "        \n",
    "        # Add this split evaluation to results\n",
    "        results.append({\n",
    "            \"partition_id\": index,\n",
    "            \"exemplar_id\": exemplar_id,\n",
    "            \"exemplar_data\": exemplar_data,\n",
    "            \"exemplar_label\": exemplar_label,\n",
    "            \"gini_reduction\": gini_reduction   \n",
    "        })\n",
    "    \n",
    "    return iter(results)\n",
    "\n",
    "# Example usage\n",
    "rdd_with_splits = rdd_with_closest_exemplar.mapPartitionsWithIndex(evaluate_splits_within_partition)\n",
    "rdd_with_splits.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff060386",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9275b",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `ellipsis`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkTypeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     72\u001b[39m spark = (\n\u001b[32m     73\u001b[39m     SparkSession.builder\n\u001b[32m     74\u001b[39m     .appName(\u001b[33m\"\u001b[39m\u001b[33mGlobalProximityTree\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m     .getOrCreate()\n\u001b[32m     76\u001b[39m )\n\u001b[32m     78\u001b[39m tsdata = [...]        \u001b[38;5;66;03m# <‑‑ paste the 34‑item list exactly as in your post\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtsdata\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# cols: label, time_series\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Make sure the time_series column is an array<double>\u001b[39;00m\n\u001b[32m     82\u001b[39m df = df.withColumn(\u001b[33m\"\u001b[39m\u001b[33mtime_series\u001b[39m\u001b[33m\"\u001b[39m, F.col(\u001b[33m\"\u001b[39m\u001b[33mtime_series\u001b[39m\u001b[33m\"\u001b[39m).cast(ArrayType(DoubleType())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\session.py:1443\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd.DataFrame):\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m).createDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m   1441\u001b[39m         data, schema, samplingRatio, verifySchema\n\u001b[32m   1442\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1483\u001b[39m     rdd, struct = \u001b[38;5;28mself\u001b[39m._createFromRDD(data.map(prepare), schema, samplingRatio)\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m     rdd, struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1487\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\session.py:1093\u001b[39m, in \u001b[36mSparkSession._createFromLocal\u001b[39m\u001b[34m(self, data, schema)\u001b[39m\n\u001b[32m   1090\u001b[39m     data = \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m     converter = _create_converter(struct)\n\u001b[32m   1095\u001b[39m     tupled_data: Iterable[Tuple] = \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\session.py:955\u001b[39m, in \u001b[36mSparkSession._inferSchemaFromList\u001b[39m\u001b[34m(self, data, names)\u001b[39m\n\u001b[32m    953\u001b[39m infer_array_from_first_element = \u001b[38;5;28mself\u001b[39m._jconf.legacyInferArrayTypeFromFirstElement()\n\u001b[32m    954\u001b[39m prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m schema = \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m    970\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    971\u001b[39m         message_parameters={},\n\u001b[32m    972\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\session.py:958\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    953\u001b[39m infer_array_from_first_element = \u001b[38;5;28mself\u001b[39m._jconf.legacyInferArrayTypeFromFirstElement()\n\u001b[32m    954\u001b[39m prefer_timestamp_ntz = is_timestamp_ntz_preferred()\n\u001b[32m    955\u001b[39m schema = reduce(\n\u001b[32m    956\u001b[39m     _merge_type,\n\u001b[32m    957\u001b[39m     (\n\u001b[32m--> \u001b[39m\u001b[32m958\u001b[39m         \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[32m    966\u001b[39m     ),\n\u001b[32m    967\u001b[39m )\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[32m    969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m    970\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    971\u001b[39m         message_parameters={},\n\u001b[32m    972\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py:1684\u001b[39m, in \u001b[36m_infer_schema\u001b[39m\u001b[34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[39m\n\u001b[32m   1681\u001b[39m     items = \u001b[38;5;28msorted\u001b[39m(row.\u001b[34m__dict__\u001b[39m.items())\n\u001b[32m   1683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1684\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1685\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1686\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33mdata_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1687\u001b[39m     )\n\u001b[32m   1689\u001b[39m fields = []\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[31mPySparkTypeError\u001b[39m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `ellipsis`."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# global_proximity_tree.py\n",
    "#\n",
    "# Build ONE proximity‑tree in a truly distributed way.\n",
    "# ----------------------------------------------------\n",
    "# 1.  Driver picks one exemplar per class  (global!)\n",
    "# 2.  Broadcast the exemplar list (a few KB)\n",
    "# 3.  Workers tag every row with its nearest exemplar\n",
    "# 4.  Spark counts      (node_id, branch_id, class)  →   n\n",
    "# 5.  Driver decides the best split, creates children\n",
    "# 6.  Repeat steps 2‑5 until all nodes are pure or max_depth reached\n",
    "#\n",
    "# NOTE: for brevity we use plain Euclidean distance; swap dtw.distance if you like.\n",
    "#       The code runs on the same SparkSession as your previous script.\n",
    "from random import sample\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "import collections\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "from random import choice\n",
    "import math, json, collections, itertools\n",
    "\n",
    "\n",
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]},\n",
    "    {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
    "    {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
    "    {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
    "    {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
    "    {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
    "    {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
    "    {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
    "    {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
    "    {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.4, 1.9, 2.4, 2.9]},\n",
    "    {'label': 1, 'time_series': [1.4, 2.0, 2.6, 3.2, 3.8]},\n",
    "    {'label': 2, 'time_series': [2.2, 3.1, 4.0, 4.9, 5.8]},\n",
    "    {'label': 2, 'time_series': [2.6, 3.2, 3.8, 4.4, 5.0]},\n",
    "    {'label': 3, 'time_series': [1.2, 2.0, 2.8, 3.6, 4.4]},\n",
    "    {'label': 3, 'time_series': [0.6, 1.3, 2.0, 2.7, 3.4]},\n",
    "    {'label': 4, 'time_series': [6.3, 6.5, 6.7, 6.9, 7.1]},\n",
    "    {'label': 4, 'time_series': [7.0, 7.8, 8.6, 9.4, 10.2]},\n",
    "    {'label': 4, 'time_series': [6.5, 7.0, 7.5, 8.0, 8.5]},\n",
    "    {'label': 1, 'time_series': [0.5, 1.0, 1.5, 2.0, 2.5]},\n",
    "    {'label': 2, 'time_series': [0.6, 1.4, 1.3, 2.1, 2.5]},\n",
    "    {'label': 3, 'time_series': [0.3, 1.7, 1.6, 2.2, 2.6]},\n",
    "    {'label': 4, 'time_series': [0.2, 1.9, 1.6, 2.3, 2.7]},\n",
    "    {'label': 4, 'time_series': [0.9, 1.7, 1.2, 2.4, 2.8]}\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# helper: tiny distance function (Euclidean) --------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "def euclid(a, b):\n",
    "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 0.  create Spark session & DataFrame --------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"GlobalProximityTree\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "tsdata = [...]        # <‑‑ paste the 34‑item list exactly as in your post\n",
    "df = spark.createDataFrame(tsdata)        # cols: label, time_series\n",
    "\n",
    "# Make sure the time_series column is an array<double>\n",
    "df = df.withColumn(\"time_series\", F.col(\"time_series\").cast(ArrayType(DoubleType())))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.  driver picks ONE exemplar per class (cheap: input is small) -----------\n",
    "# ---------------------------------------------------------------------------\n",
    "exemplar_rows = (\n",
    "    df\n",
    "    .groupBy(\"label\")               # one group per class\n",
    "    .agg(F.shuffle(F.collect_list(\"time_series\")).alias(\"bag\"))\n",
    "    .select(\"label\", F.expr(\"bag[0]\").alias(\"time_series\"))   # first random element\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Turn into a driver‑side dict {label: vector}\n",
    "GLOBAL_EXEMPLARS = {row[\"label\"]: row[\"time_series\"] for row in exemplar_rows}\n",
    "print(\"broadcasted exemplars:\", GLOBAL_EXEMPLARS)\n",
    "\n",
    "ex_bc = spark.sparkContext.broadcast(GLOBAL_EXEMPLARS)   # a few KB\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2.  spark ⇢ workers: tag every row with nearest exemplar -------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "def tag_nearest(row):\n",
    "    vec   = row.time_series\n",
    "    label = row.label\n",
    "    best_id, best_dist = None, float(\"inf\")\n",
    "    for ex_label, ex_vec in ex_bc.value.items():\n",
    "        d = euclid(vec, ex_vec)\n",
    "        if d < best_dist:\n",
    "            best_id, best_dist = ex_label, d\n",
    "    # We start with a single node (node_id = 0)\n",
    "    return (0, best_id, label)        # (node_id, branch_id == exemplarLabel, true class)\n",
    "\n",
    "schema = StructType().add(\"node_id\", IntegerType()) \\\n",
    "                     .add(\"branch_id\", IntegerType()) \\\n",
    "                     .add(\"true_label\", IntegerType())\n",
    "\n",
    "tagged = df.rdd.map(tag_nearest).toDF(schema)\n",
    "# ┌ node_id ┬ branch_id ┬ true_label ┐\n",
    "# └      0  ┴     1     ┴     1     ┘  etc.\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.  iterate breadth‑first until tree finished ------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "TreeNode  = collections.namedtuple(\n",
    "    \"TreeNode\",\n",
    "    \"node_id parent_id split_on is_leaf prediction children\".split()\n",
    ")\n",
    "\n",
    "tree = {0: TreeNode(0, None, None, False, None, {})}   # root placeholder\n",
    "open_nodes = {0}\n",
    "max_depth  = 3\n",
    "\n",
    "for depth in range(max_depth):\n",
    "    if not open_nodes:\n",
    "        break\n",
    "\n",
    "    # 3a.  count class distribution per (node_id, branch_id)\n",
    "    counts = (\n",
    "        tagged\n",
    "        .where(F.col(\"node_id\").isin(list(open_nodes)))\n",
    "        .groupBy(\"node_id\", \"branch_id\", \"true_label\")\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    # bring tiny histogram to driver\n",
    "    hist = {}\n",
    "    for r in counts.collect():\n",
    "        hist.setdefault(r.node_id, {}).setdefault(r.branch_id, {})[r.true_label] = r[\"count\"]\n",
    "\n",
    "    # 3b.  decide splits on the driver\n",
    "    next_open = set()\n",
    "    for nid in open_nodes:\n",
    "        branches = hist.get(nid, {})\n",
    "        # compute parent gini\n",
    "        total_per_class = collections.Counter()\n",
    "        for br in branches.values():\n",
    "            total_per_class.update(br)\n",
    "        n_total = sum(total_per_class.values())\n",
    "        gini_parent = 1 - sum((c/n_total)**2 for c in total_per_class.values())\n",
    "\n",
    "        # the split is already fixed: one branch per exemplar;\n",
    "        # we only need to mark children as leaf or internal\n",
    "        for br_id, cls_count in branches.items():\n",
    "            n = sum(cls_count.values())\n",
    "            if len(cls_count) == 1:          # pure → leaf\n",
    "                pred = next(iter(cls_count))\n",
    "                leaf = TreeNode(node_id=len(tree),\n",
    "                                parent_id=nid,\n",
    "                                split_on=None,\n",
    "                                is_leaf=True,\n",
    "                                prediction=pred,\n",
    "                                children={})\n",
    "                tree[leaf.node_id] = leaf\n",
    "                tree[nid].children[br_id] = leaf.node_id\n",
    "            else:                            # impure → internal node\n",
    "                child_id = len(tree)\n",
    "                twin  = TreeNode(child_id, nid, None, False, None, {})\n",
    "                tree[child_id] = twin\n",
    "                tree[nid].children[br_id] = child_id\n",
    "                next_open.add(child_id)\n",
    "\n",
    "        # mark the current node as decided (its split is “nearest exemplar”)\n",
    "        tree[nid] = tree[nid]._replace(split_on=\"nearest_exemplar\")\n",
    "\n",
    "    # 3c.  update tagged DataFrame with new node_id for impure children\n",
    "    if next_open:\n",
    "        mapping = {old: new for nid in open_nodes\n",
    "                             for br, new in tree[nid].children.items()\n",
    "                             if new in next_open}\n",
    "\n",
    "        # broadcast the dict { (parentId,branchId) : childId }\n",
    "        map_bc = spark.sparkContext.broadcast(mapping)\n",
    "\n",
    "        # update node_id column (narrow map, no shuffle)\n",
    "        def push_down(r):\n",
    "            key = (r.node_id, r.branch_id)\n",
    "            new_nid = map_bc.value.get(key, r.node_id)\n",
    "            return (new_nid, r.branch_id, r.true_label)\n",
    "\n",
    "        tagged = tagged.rdd.map(push_down).toDF(schema)\n",
    "\n",
    "    open_nodes = next_open    # loop\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4.  pretty‑print the tree --------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "def show(nid, indent=\"\"):\n",
    "    node = tree[nid]\n",
    "    if node.is_leaf:\n",
    "        print(f\"{indent}Leaf ⇒ predict {node.prediction}\")\n",
    "    else:\n",
    "        print(f\"{indent}Node {nid}: split = nearest exemplar\")\n",
    "        for br, child in node.children.items():\n",
    "            print(f\"{indent}  if nearest == class‑{br} exemplar →\")\n",
    "            show(child, indent + \"      \")\n",
    "\n",
    "print(\"\\nFinal tree\")\n",
    "show(0)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c9ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"GlobalProximityTree\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d0bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.8, 2.7, 3.6, 4.5]},\n",
    "    {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
    "    {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
    "    {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
    "    {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
    "    {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
    "    {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
    "    {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
    "    {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
    "    {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.4, 1.9, 2.4, 2.9]},\n",
    "    {'label': 1, 'time_series': [1.4, 2.0, 2.6, 3.2, 3.8]},\n",
    "    {'label': 2, 'time_series': [2.2, 3.1, 4.0, 4.9, 5.8]},\n",
    "    {'label': 2, 'time_series': [2.6, 3.2, 3.8, 4.4, 5.0]},\n",
    "    {'label': 3, 'time_series': [1.2, 2.0, 2.8, 3.6, 4.4]},\n",
    "    {'label': 3, 'time_series': [0.6, 1.3, 2.0, 2.7, 3.4]},\n",
    "    {'label': 4, 'time_series': [6.3, 6.5, 6.7, 6.9, 7.1]},\n",
    "    {'label': 4, 'time_series': [7.0, 7.8, 8.6, 9.4, 10.2]},\n",
    "    {'label': 4, 'time_series': [6.5, 7.0, 7.5, 8.0, 8.5]},\n",
    "    {'label': 1, 'time_series': [0.5, 1.0, 1.5, 2.0, 2.5]},\n",
    "    {'label': 2, 'time_series': [0.6, 1.4, 1.3, 2.1, 2.5]},\n",
    "    {'label': 3, 'time_series': [0.3, 1.7, 1.6, 2.2, 2.6]},\n",
    "    {'label': 4, 'time_series': [0.2, 1.9, 1.6, 2.3, 2.7]},\n",
    "    {'label': 4, 'time_series': [0.9, 1.7, 1.2, 2.4, 2.8]}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(tsdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe7b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "from random import choice\n",
    "import math, json, collections, itertools\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn(\"time_series\", F.col(\"time_series\").cast(ArrayType(DoubleType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e4e440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcasted exemplars: [Row(label=1, time_series=[1.1, 2.1, 3.1, 4.1, 5.1]), Row(label=2, time_series=[2.1, 3.3, 4.5, 5.7, 6.9]), Row(label=3, time_series=[0.3, 1.7, 1.6, 2.2, 2.6]), Row(label=4, time_series=[7.0, 7.8, 8.6, 9.4, 10.2])]\n"
     ]
    }
   ],
   "source": [
    "exemplar_rows = (\n",
    "    df\n",
    "    .groupBy(\"label\")               # one group per class\n",
    "    .agg(F.shuffle(F.collect_list(\"time_series\")).alias(\"bag\"))\n",
    "    .select(\"label\", F.expr(\"bag[0]\").alias(\"time_series\"))   # first random element\n",
    "    .collect()\n",
    ")\n",
    "print(\"broadcasted exemplars:\", exemplar_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95cf5d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcasted exemplars: {1: [1.1, 2.1, 3.1, 4.1, 5.1], 2: [2.1, 3.3, 4.5, 5.7, 6.9], 3: [0.3, 1.7, 1.6, 2.2, 2.6], 4: [7.0, 7.8, 8.6, 9.4, 10.2]}\n"
     ]
    }
   ],
   "source": [
    "# Turn into a driver‑side dict {label: vector}\n",
    "GLOBAL_EXEMPLARS = {row[\"label\"]: row[\"time_series\"] for row in exemplar_rows}\n",
    "print(\"broadcasted exemplars:\", GLOBAL_EXEMPLARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da8861d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1.1, 2.1, 3.1, 4.1, 5.1]\n",
      "2 [2.1, 3.3, 4.5, 5.7, 6.9]\n",
      "3 [0.3, 1.7, 1.6, 2.2, 2.6]\n",
      "4 [7.0, 7.8, 8.6, 9.4, 10.2]\n"
     ]
    }
   ],
   "source": [
    "ex_bc = spark.sparkContext.broadcast(GLOBAL_EXEMPLARS)\n",
    "for i in ex_bc.value:\n",
    "    print(i, ex_bc.value[i]) # a few KB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01db03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid(a, b):\n",
    "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2.  spark ⇢ workers: tag every row with nearest exemplar -------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "def tag_nearest_euclidian(row):\n",
    "    vec   = row.time_series\n",
    "    label = row.label\n",
    "    best_id, best_dist = None, float(\"inf\")\n",
    "    for ex_label, ex_vec in ex_bc.value.items():\n",
    "        d = dtw(vec, ex_vec)\n",
    "        if d < best_dist:\n",
    "            best_id, best_dist = ex_label, d\n",
    "    # We start with a single node (node_id = 0)\n",
    "    return (0, best_id, label)        # (node_id, branch_id == exemplarLabel, true class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2.  spark ⇢ workers: tag every row with nearest exemplar -------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "def tag_nearest_dtw(row):\n",
    "    vec   = row.time_series\n",
    "    label = row.label\n",
    "    best_id, best_dist = None, float(\"inf\")\n",
    "    for ex_label, ex_vec in ex_bc.value.items():\n",
    "        d = euclid(vec, ex_vec)\n",
    "        if d < best_dist:\n",
    "            best_id, best_dist = ex_label, d\n",
    "    # We start with a single node (node_id = 0)\n",
    "    return (0, best_id, label)        # (node_id, branch_id == exemplarLabel, true class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
