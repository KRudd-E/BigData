{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c9ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"GlobalProximityTree\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdata = [\n",
    "    {'label': 1, 'time_series': [1.2, 2.4, 3.6, 4.8, 6.0]},\n",
    "    {'label': 1, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 1, 'time_series': [1.5, 2.1, 2.7, 3.3, 3.9]},\n",
    "    {'label': 1, 'time_series': [0.8, 1.7, 2.5, 3.2, 4.0]},\n",
    "    {'label': 2, 'time_series': [2.1, 3.3, 4.5, 5.7, 6.9]},\n",
    "    {'label': 2, 'time_series': [3.0, 3.8, 4.6, 5.4, 6.2]},\n",
    "    {'label': 2, 'time_series': [3.3, 4.1, 4.9, 5.7, 6.5]},\n",
    "    {'label': 3, 'time_series': [0.5, 1.5, 2.5, 3.5, 4.5]},\n",
    "    {'label': 3, 'time_series': [2.0, 2.5, 3.0, 3.5, 4.0]},\n",
    "    {'label': 4, 'time_series': [5.5, 6.6, 7.7, 8.8, 9.9]},\n",
    "    {'label': 4, 'time_series': [6.1, 6.2, 6.3, 6.4, 6.5]},\n",
    "    {'label': 1, 'time_series': [0.7, 1.3, 1.9, 2.5, 3.1]},\n",
    "    {'label': 1, 'time_series': [1.1, 2.1, 3.1, 4.1, 5.1]},\n",
    "    {'label': 1, 'time_series': [0.6, 1.2, 1.8, 2.4, 3.0]},\n",
    "    {'label': 2, 'time_series': [2.4, 3.5, 4.6, 5.7, 6.8]},\n",
    "    {'label': 2, 'time_series': [1.9, 2.8, 3.7, 4.6, 5.5]},\n",
    "    {'label': 3, 'time_series': [1.0, 1.8, 2.6, 3.4, 4.2]},\n",
    "    {'label': 4, 'time_series': [6.0, 7.0, 8.0, 9.0, 10.0]},\n",
    "    {'label': 1, 'time_series': [1.3, 2.3, 3.3, 4.3, 5.3]},\n",
    "    {'label': 1, 'time_series': [0.9, 1.4, 1.9, 2.4, 2.9]},\n",
    "\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(tsdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe7b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "from random import choice\n",
    "import math, json, collections, itertools\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn(\"time_series\", F.col(\"time_series\").cast(ArrayType(DoubleType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e4e440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcasted exemplars: [Row(label=1, time_series=[1.1, 2.1, 3.1, 4.1, 5.1]), Row(label=2, time_series=[2.4, 3.5, 4.6, 5.7, 6.8]), Row(label=3, time_series=[1.0, 1.8, 2.6, 3.4, 4.2]), Row(label=4, time_series=[6.1, 6.2, 6.3, 6.4, 6.5])]\n"
     ]
    }
   ],
   "source": [
    "exemplar_rows = (\n",
    "    df\n",
    "    .groupBy(\"label\")               # one group per class\n",
    "    .agg(F.shuffle(F.collect_list(\"time_series\")).alias(\"bag\"))\n",
    "    .select(\"label\", F.expr(\"bag[0]\").alias(\"time_series\"))   # first random element\n",
    "    .collect()\n",
    ")\n",
    "print(\"broadcasted exemplars:\", exemplar_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95cf5d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcasted exemplars: {1: [1.1, 2.1, 3.1, 4.1, 5.1], 2: [2.4, 3.5, 4.6, 5.7, 6.8], 3: [1.0, 1.8, 2.6, 3.4, 4.2], 4: [6.1, 6.2, 6.3, 6.4, 6.5]}\n"
     ]
    }
   ],
   "source": [
    "# Turn into a driver‑side dict {label: vector}\n",
    "GLOBAL_EXEMPLARS = {row[\"label\"]: row[\"time_series\"] for row in exemplar_rows}\n",
    "print(\"broadcasted exemplars:\", GLOBAL_EXEMPLARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8861d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1.1, 2.1, 3.1, 4.1, 5.1]\n",
      "2 [2.4, 3.5, 4.6, 5.7, 6.8]\n",
      "3 [1.0, 1.8, 2.6, 3.4, 4.2]\n",
      "4 [6.1, 6.2, 6.3, 6.4, 6.5]\n"
     ]
    }
   ],
   "source": [
    "ex_bc = spark.sparkContext.broadcast(GLOBAL_EXEMPLARS)\n",
    "for i in ex_bc.value:\n",
    "    print(i, ex_bc.value[i]) # a few KB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01db03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid(a, b):\n",
    "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e9bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c348f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2.  spark ⇢ workers: tag every row with nearest exemplar -------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "def tag_nearest_euclidian(row):\n",
    "    vec   = row.time_series\n",
    "    label = row.label\n",
    "    best_id, best_dist = None, float(\"inf\")\n",
    "    for ex_label, ex_vec in ex_bc.value.items():\n",
    "        d = euclid(vec, ex_vec)\n",
    "        if d < best_dist:\n",
    "            best_id, best_dist = ex_label, d\n",
    "            print(\"best_id\", best_id, \"best_dist\", best_dist, \"ex_label\", ex_label, \"ex_vec\", ex_vec)\n",
    "    # We start with a single node (node_id = 0)\n",
    "    return (0, best_id, label)        # (node_id, branch_id == exemplarLabel, true class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b6653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2.  spark ⇢ workers: tag every row with nearest exemplar -------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "def tag_nearest_dtw(row):\n",
    "    vec   = row.time_series\n",
    "    label = row.label\n",
    "    best_id, best_dist = None, float(\"inf\")\n",
    "    for ex_label, ex_vec in ex_bc.value.items():\n",
    "        d = dtw.distance(vec, ex_vec)\n",
    "        if d < best_dist:\n",
    "            best_id, best_dist = ex_label, d\n",
    "    # We start with a single node (node_id = 0)\n",
    "    return (0, best_id, label)        # (node_id, branch_id == exemplarLabel, true class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1123d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T, functions as F\n",
    "\n",
    "schema = (T.StructType()\n",
    "            .add(\"node_id\",     T.IntegerType())\n",
    "            .add(\"branch_id\",   T.IntegerType())\n",
    "            .add(\"true_label\",  T.IntegerType())\n",
    "            .add(\"dist_calc\",   T.DoubleType()))        # debug column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ceb600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_nearest_euclid_debug(row):\n",
    "    vec   = row.time_series\n",
    "    label = row.label\n",
    "\n",
    "    best_id, best_dist = None, float(\"inf\")\n",
    "    for ex_label, ex_vec in ex_bc.value.items():\n",
    "        d = euclid(vec, ex_vec)\n",
    "        if d < best_dist:\n",
    "            best_id, best_dist = ex_label, d\n",
    "\n",
    "    # ► We send the debug fields back to the driver as extra columns\n",
    "    return (0,            # node_id  (root)\n",
    "            best_id,      # branch_id   == exemplar chosen\n",
    "            label,        # true label\n",
    "            best_dist)    # DEBUG: distance to that exemplar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f803db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tagged = df.rdd.map(tag_nearest_euclid_debug).toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8279658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2.  spark ⇢ workers: tag every row with nearest exemplar -------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "def tag_nearest_dtw_debug(row):\n",
    "    vec   = row.time_series\n",
    "    label = row.label\n",
    "    best_id, best_dist = None, float(\"inf\")\n",
    "    for ex_label, ex_vec in ex_bc.value.items():\n",
    "        d = dtw.distance(vec, ex_vec)\n",
    "        if d < best_dist:\n",
    "            best_id, best_dist = ex_label, d\n",
    "    # We start with a single node (node_id = 0)\n",
    "    return (0, best_id, label, best_dist)        # (node_id, branch_id == exemplarLabel, true class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0e0c274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------------------+\n",
      "|node_id|branch_id|true_label|dist_calc         |\n",
      "+-------+---------+----------+------------------+\n",
      "|0      |1        |1         |1.2845232578665131|\n",
      "|0      |3        |1         |0.0               |\n",
      "|0      |3        |1         |0.670820393249937 |\n",
      "|0      |3        |1         |0.3741657386773941|\n",
      "|0      |2        |2         |0.3872983346207417|\n",
      "|0      |2        |2         |0.9486832980505135|\n",
      "|0      |2        |2         |1.161895003862225 |\n",
      "|0      |3        |3         |0.6708203932499369|\n",
      "|0      |3        |3         |1.3038404810405297|\n",
      "|0      |4        |4         |4.449719092257398 |\n",
      "|0      |4        |4         |0.0               |\n",
      "|0      |3        |1         |1.6881943016134133|\n",
      "|0      |1        |1         |0.0               |\n",
      "|0      |3        |1         |1.8973665961010278|\n",
      "|0      |2        |2         |0.0               |\n",
      "|0      |1        |2         |1.378404875209022 |\n",
      "|0      |3        |3         |0.0               |\n",
      "|0      |4        |4         |4.748684028233506 |\n",
      "|0      |1        |1         |0.4472135954999578|\n",
      "|0      |3        |1         |1.830300521772313 |\n",
      "+-------+---------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagged.show( truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38b69e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: only the rows that picked exemplar 1\n",
    "# tagged.where(F.col(\"branch_id\") == 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db385b33",
   "metadata": {},
   "source": [
    "node_id – which node of the tree the row is currently at (starts at 0, the root).\n",
    "\n",
    "branch_id – ID of the exemplar (here equal to its class label) to which this row is closest.\n",
    "\n",
    "true_label – the actual class label of the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3994ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(tagged_df, open_nodes, debug=False):\n",
    "    \"\"\"\n",
    "    3a — Count #rows for every (node_id, branch_id, true_label) among the open nodes.\n",
    "    Returns a tiny nested dict:\n",
    "      { node_id: { branch_id: { true_label: count, … }, … }, … }\n",
    "    \"\"\"\n",
    "    if not open_nodes:\n",
    "        return {}\n",
    "\n",
    "    # filter to only the node_ids we're actually expanding\n",
    "    filtered = tagged_df.where(F.col(\"node_id\").isin(open_nodes))\n",
    "    counts   = (\n",
    "      filtered\n",
    "        .groupBy(\"node_id\", \"branch_id\", \"true_label\")\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(\">>> FILTERED ROWS FOR THESE open_nodes:\", open_nodes)\n",
    "        filtered.show(2, truncate=False)\n",
    "        print(\">>> AGGREGATED COUNTS:\")\n",
    "        counts.show(2, truncate=False)\n",
    "        counts.printSchema()\n",
    "\n",
    "    # collect to driver (should be very small!)\n",
    "    hist = {}\n",
    "    for r in counts.collect():\n",
    "        hist.setdefault(r.node_id, {}) \\\n",
    "            .setdefault(r.branch_id, {})[r.true_label] = r[\"count\"]\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "239dde8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_node(nid, branches, tree):\n",
    "    \"\"\"\n",
    "    3 b — Given one node‑id and its branch histogram, append children\n",
    "    to the global `tree` dict and return the set of *impure* children\n",
    "    that must be processed in the next layer.\n",
    "    \"\"\"\n",
    "    next_open = set()\n",
    "\n",
    "    for br_id, cls_count in branches.items():\n",
    "        if len(cls_count) == 1:                 # ——— pure → leaf\n",
    "            pred = next(iter(cls_count))\n",
    "            leaf = TreeNode(\n",
    "                node_id=len(tree),\n",
    "                parent_id=nid,\n",
    "                split_on=None,\n",
    "                is_leaf=True,\n",
    "                prediction=pred,\n",
    "                children={}\n",
    "            )\n",
    "            tree[leaf.node_id]      = leaf\n",
    "            tree[nid].children[br_id] = leaf.node_id\n",
    "\n",
    "        else:                                   # ——— impure → internal\n",
    "            child_id = len(tree)\n",
    "            twin     = TreeNode(child_id, nid, None, False, None, {})\n",
    "            tree[child_id]           = twin\n",
    "            tree[nid].children[br_id] = child_id\n",
    "            next_open.add(child_id)\n",
    "\n",
    "    # mark parent as decided\n",
    "    tree[nid] = tree[nid]._replace(split_on=\"nearest_exemplar\")\n",
    "    return next_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "374ffe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(labels):\n",
    "    if not labels:\n",
    "        return 0.0\n",
    "    counts = collections.Counter(labels)\n",
    "    total = sum(counts.values())\n",
    "    return 1.0 - sum((cnt/total)**2 for cnt in counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f6ac4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_node_gini(nid, branches, tree, depth, max_depth, min_samples):\n",
    "    # 1) compute parent Gini\n",
    "    parent_counts = collections.Counter()\n",
    "    for bc in branches.values():\n",
    "        parent_counts.update(bc)\n",
    "    labels_parent = list(parent_counts.elements())\n",
    "    gini_parent  = calculate_gini(labels_parent)\n",
    "\n",
    "    # 2) stop if too few or too deep\n",
    "    if len(labels_parent) < min_samples or depth >= max_depth:\n",
    "        majority = parent_counts.most_common(1)[0][0]\n",
    "        tree[nid] = tree[nid]._replace(\n",
    "            is_leaf=True,\n",
    "            prediction=majority,\n",
    "            split_on=None,\n",
    "            gini_parent=gini_parent\n",
    "        )\n",
    "        return set()\n",
    "\n",
    "    # 3) stop if already pure\n",
    "    if gini_parent == 0.0:\n",
    "        tree[nid] = tree[nid]._replace(\n",
    "            is_leaf=True,\n",
    "            prediction=labels_parent[0],\n",
    "            split_on=None,\n",
    "            gini_parent=gini_parent\n",
    "        )\n",
    "        return set()\n",
    "\n",
    "    # 4) compute weighted‑child Gini\n",
    "    total = len(labels_parent)\n",
    "    weighted = 0.0\n",
    "    for bc in branches.values():\n",
    "        child_labels = sum(([lbl]*cnt for lbl,cnt in bc.items()), [])\n",
    "        weighted += (len(child_labels)/total) * calculate_gini(child_labels)\n",
    "\n",
    "    # 5) stop if no improvement\n",
    "    if weighted >= gini_parent:\n",
    "        majority = parent_counts.most_common(1)[0][0]\n",
    "        tree[nid] = tree[nid]._replace(\n",
    "            is_leaf=True,\n",
    "            prediction=majority,\n",
    "            split_on=None,\n",
    "            gini_parent=gini_parent\n",
    "        )\n",
    "        return set()\n",
    "\n",
    "    # 6) otherwise create children\n",
    "    next_open = set()\n",
    "    tree[nid] = tree[nid]._replace(\n",
    "        split_on=\"nearest_exemplar\",\n",
    "        gini_parent=gini_parent\n",
    "    )\n",
    "    for br_id, bc in branches.items():\n",
    "        child_labels = sum(([lbl]*cnt for lbl,cnt in bc.items()), [])\n",
    "        if calculate_gini(child_labels) == 0.0:\n",
    "            # pure → leaf\n",
    "            leaf = TreeNode(\n",
    "                node_id    = len(tree),\n",
    "                parent_id  = nid,\n",
    "                split_on   = None,\n",
    "                is_leaf    = True,\n",
    "                prediction = child_labels[0],\n",
    "                children   = {},\n",
    "                gini_parent=gini_parent\n",
    "            )\n",
    "            tree[leaf.node_id] = leaf\n",
    "            tree[nid].children[br_id] = leaf.node_id\n",
    "        else:\n",
    "            # impure → internal\n",
    "            cid = len(tree)\n",
    "            internal = TreeNode(\n",
    "                node_id    = cid,\n",
    "                parent_id  = nid,\n",
    "                split_on   = None,  # will be set when we visit it\n",
    "                is_leaf    = False,\n",
    "                prediction = None,\n",
    "                children   = {},\n",
    "                gini_parent=gini_parent\n",
    "            )\n",
    "            tree[cid] = internal\n",
    "            tree[nid].children[br_id] = cid\n",
    "            next_open.add(cid)\n",
    "\n",
    "    return next_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50aaa645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_rows_down(tagged_df, tree, open_nodes, next_open, schema):\n",
    "    \"\"\"\n",
    "    3 c — Rows that belonged to a parent now get the child node‑id\n",
    "    (narrow map, no shuffle).  Returns a **new** DataFrame.\n",
    "    \"\"\"\n",
    "    if not next_open:\n",
    "        return tagged_df                       # nothing to change\n",
    "\n",
    "    mapping = {                                 # (parent,branch) ➜ child\n",
    "        (nid, br): child_id\n",
    "        for nid in open_nodes\n",
    "        for br, child_id in tree[nid].children.items()\n",
    "        if child_id in next_open\n",
    "    }\n",
    "    bc = spark.sparkContext.broadcast(mapping)\n",
    "    \n",
    "    def _push(r):\n",
    "        key   = (r.node_id, r.branch_id)\n",
    "        new_n = bc.value.get(key, r.node_id)\n",
    "\n",
    "        return (\n",
    "            new_n,               # node_id\n",
    "            r.time_series,       # carry the series through\n",
    "            r.branch_id,\n",
    "            r.true_label,\n",
    "            r.dist_calc\n",
    "        )\n",
    "\n",
    "    return tagged_df.rdd.map(_push).toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feccdb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def hist_to_dataframe(hist):\n",
    "    rows = []\n",
    "    for node_id, branches in hist.items():\n",
    "        for br_id, cls_dict in branches.items():\n",
    "            for lbl, cnt in cls_dict.items():\n",
    "                rows.append((node_id, br_id, lbl, cnt))\n",
    "    return pd.DataFrame(rows,\n",
    "                        columns=[\"node_id\", \"branch_id\", \"true_label\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74a11db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> FILTERED ROWS FOR THESE open_nodes: {0}\n",
      "+-------+-------------------------+---------+----------+------------------+\n",
      "|node_id|time_series              |branch_id|true_label|dist_calc         |\n",
      "+-------+-------------------------+---------+----------+------------------+\n",
      "|0      |[1.2, 2.4, 3.6, 4.8, 6.0]|1        |1         |1.224744871391589 |\n",
      "|0      |[1.0, 1.8, 2.6, 3.4, 4.2]|3        |1         |0.6708203932499369|\n",
      "+-------+-------------------------+---------+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      ">>> AGGREGATED COUNTS:\n",
      "+-------+---------+----------+-----+\n",
      "|node_id|branch_id|true_label|count|\n",
      "+-------+---------+----------+-----+\n",
      "|0      |1        |1         |3    |\n",
      "|0      |3        |1         |6    |\n",
      "+-------+---------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- node_id: integer (nullable = false)\n",
      " |-- branch_id: integer (nullable = false)\n",
      " |-- true_label: integer (nullable = false)\n",
      " |-- count: long (nullable = false)\n",
      "\n",
      "\n",
      "=== depth=0, open_nodes={0} ===\n",
      "{0: {1: {1: 3, 2: 1, 3: 1}, 2: {2: 4, 4: 1}, 3: {1: 6, 3: 2}, 4: {4: 2}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>branch_id</th>\n",
       "      <th>true_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   node_id  branch_id  true_label  count\n",
       "0        0          1           1      3\n",
       "1        0          1           3      1\n",
       "2        0          1           2      1\n",
       "3        0          3           1      6\n",
       "4        0          3           3      2\n",
       "5        0          2           2      4\n",
       "6        0          2           4      1\n",
       "7        0          4           4      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o213.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 20.0 failed 1 times, most recent failure: Lost task 7.0 in stage 20.0 (TID 159) (razvan.petru1-everest.nord executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2174, in verify_struct\n    verifier(v)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2200, in verify\n    if not verify_nullability(obj):\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2003, in verify_nullability\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [CANNOT_BE_NONE] Argument `obj` can not be None.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:88)\r\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2174, in verify_struct\n    verifier(v)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2200, in verify\n    if not verify_nullability(obj):\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2003, in verify_nullability\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [CANNOT_BE_NONE] Argument `obj` can not be None.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:88)\r\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     72\u001b[39m tagged = (\n\u001b[32m     73\u001b[39m     assign_df.rdd\n\u001b[32m     74\u001b[39m     .map(tag_row)\n\u001b[32m     75\u001b[39m     .toDF(tagged_schema)\n\u001b[32m     76\u001b[39m     .cache()\n\u001b[32m     77\u001b[39m )\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# ──────────── 3) BUILD histogram & decide splits ────────────\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m#  Build your histogram, split_node_gini, push_rows_down exactly as before,\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m#    but operating on `tagged`.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m hist = \u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtagged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopen_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hist:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo more splits at depth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, stopping.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mhistogram\u001b[39m\u001b[34m(tagged_df, open_nodes, debug)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# collect to driver (should be very small!)\u001b[39;00m\n\u001b[32m     26\u001b[39m hist = {}\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcounts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     28\u001b[39m     hist.setdefault(r.node_id, {}) \\\n\u001b[32m     29\u001b[39m         .setdefault(r.branch_id, {})[r.true_label] = r[\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hist\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1243\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[32m   1244\u001b[39m \n\u001b[32m   1245\u001b[39m \u001b[33;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1260\u001b[39m \u001b[33;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[32m   1261\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o213.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 20.0 failed 1 times, most recent failure: Lost task 7.0 in stage 20.0 (TID 159) (razvan.petru1-everest.nord executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2174, in verify_struct\n    verifier(v)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2200, in verify\n    if not verify_nullability(obj):\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2003, in verify_nullability\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [CANNOT_BE_NONE] Argument `obj` can not be None.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:88)\r\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n    process()\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\spark-3.5.5-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2174, in verify_struct\n    verifier(v)\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2200, in verify\n    if not verify_nullability(obj):\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Petru\\anaconda3\\envs\\bigdata_env\\Lib\\site-packages\\pyspark\\sql\\types.py\", line 2003, in verify_nullability\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [CANNOT_BE_NONE] Argument `obj` can not be None.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:88)\r\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import random, collections\n",
    "\n",
    "assign_df = df.rdd \\\n",
    "    .map(lambda r: (0, r.time_series, r.label)) \\\n",
    "    .toDF([\"node_id\",\"time_series\",\"true_label\"]) \\\n",
    "    .cache()\n",
    "#\n",
    "# Schema for the “tagged” DataFrame\n",
    "\n",
    "tagged_schema = StructType([\n",
    "    StructField(\"node_id\",     IntegerType(),           False),\n",
    "    StructField(\"time_series\", ArrayType(DoubleType()), False),\n",
    "    StructField(\"branch_id\",   IntegerType(),           False),\n",
    "    StructField(\"true_label\",  IntegerType(),           False),\n",
    "    StructField(\"dist_calc\",   DoubleType(),            False),\n",
    "])\n",
    "\n",
    "TreeNode  = collections.namedtuple(\n",
    "    \"TreeNode\",\n",
    "    \"node_id parent_id split_on is_leaf prediction children gini_parent\".split()\n",
    ")\n",
    "\n",
    "tree = {\n",
    "    0: TreeNode(\n",
    "        node_id         = 0,\n",
    "        parent_id       = None,\n",
    "        split_on        = None,\n",
    "        is_leaf         = False,\n",
    "        prediction      = None,\n",
    "        children        = {},\n",
    "        gini_parent     = None\n",
    "        \n",
    "    )\n",
    "}\n",
    "open_nodes = {0}\n",
    "max_depth  = 10\n",
    "min_samples =5\n",
    "\n",
    "\n",
    "for depth in range(max_depth):\n",
    "\n",
    "    # ---------------------------------------------------------- 3 a\n",
    "    if not open_nodes:                # nothing more to grow → stop\n",
    "        break\n",
    "    \n",
    " # ──────────── 1) SAMPLE local exemplars ────────────\n",
    "    exemplars = {}\n",
    "    # pull out only the rows at the currently open nodes\n",
    "    for row in assign_df.filter(F.col(\"node_id\").isin(open_nodes)).collect():\n",
    "        node, series, lbl = row\n",
    "        exemplars.setdefault((node, lbl), []).append(series)\n",
    "    \n",
    "    # for each (node, class) pick one exemplar at random\n",
    "    exemplars = { k: random.choice(v) for k, v in exemplars.items() }\n",
    "    ex_bc = spark.sparkContext.broadcast(exemplars)\n",
    "\n",
    "    # ──────────── 2) TAG every row with nearest exemplar ────────────\n",
    "    def tag_row(r):\n",
    "        nid, vec, true_lbl = r\n",
    "        best_branch, best_dist = None, float(\"inf\")\n",
    "        for (node_key, ex_lbl), ex_vec in ex_bc.value.items():\n",
    "            if node_key != nid: \n",
    "                continue\n",
    "            d = dtw.distance(vec, ex_vec)\n",
    "            if d < best_dist:\n",
    "                best_branch, best_dist = ex_lbl, d\n",
    "        return (nid, vec, best_branch, true_lbl, best_dist)\n",
    "        \n",
    "  \n",
    "    \n",
    "    tagged = (\n",
    "        assign_df.rdd\n",
    "        .map(tag_row)\n",
    "        .toDF(tagged_schema)\n",
    "        .cache()\n",
    "    )\n",
    "\n",
    "    # ──────────── 3) BUILD histogram & decide splits ────────────\n",
    "    #  Build your histogram, split_node_gini, push_rows_down exactly as before,\n",
    "    #    but operating on `tagged`.\n",
    "    \n",
    "    hist = histogram(tagged, open_nodes, debug=(depth == 0))\n",
    "    if not hist:\n",
    "        print(f\"No more splits at depth={depth}, stopping.\")\n",
    "        break\n",
    "\n",
    "    # ----- (optional) pretty print for debugging ---------------\n",
    "    import pprint\n",
    "    print(f\"\\n=== depth={depth}, open_nodes={open_nodes} ===\")\n",
    "    pprint.pprint(hist)\n",
    "    df_hist = hist_to_dataframe(hist)      # convert **once**\n",
    "    display(df_hist)                       # or  print(df_hist)\n",
    "\n",
    "    # ----------------------------------------------------------  decide splits\n",
    "    next_open = set()\n",
    "    for nid in open_nodes:\n",
    "        children = split_node_gini(\n",
    "            nid,\n",
    "            hist[nid],      # branches for this node\n",
    "            tree,\n",
    "            depth,\n",
    "            max_depth,\n",
    "            min_samples\n",
    "        )\n",
    "        next_open |= children              # union of new internal nodes\n",
    "\n",
    "    # ──────────── 4) PUSH rows down to child node_ids ────────────\n",
    "    \n",
    "    assign_df = (\n",
    "        push_rows_down(tagged, tree, open_nodes, next_open, tagged_schema)\n",
    "        .select(\"node_id\",\"time_series\",\"true_label\")\n",
    "        .cache()\n",
    "    )\n",
    "                \n",
    "    \n",
    "    open_nodes = next_open                # next layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4820f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- node_id: integer (nullable = true)\n",
      " |-- branch_id: integer (nullable = true)\n",
      " |-- true_label: integer (nullable = true)\n",
      " |-- dist_calc: double (nullable = true)\n",
      "\n",
      "+-------+-------+---------+----------+\n",
      "|summary|node_id|branch_id|true_label|\n",
      "+-------+-------+---------+----------+\n",
      "|    min|      0|        1|         1|\n",
      "|    max|      2|        4|         4|\n",
      "+-------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagged.printSchema()\n",
    "tagged.select(\"node_id\", \"branch_id\", \"true_label\").summary(\"min\", \"max\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4feb820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>branch_id</th>\n",
       "      <th>true_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [node_id, branch_id, true_label, count]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def hist_to_dataframe(hist):\n",
    "    rows = []\n",
    "    for node_id, branches in hist.items():\n",
    "        for br_id, cls_dict in branches.items():\n",
    "            for lbl, cnt in cls_dict.items():\n",
    "                rows.append((node_id, br_id, lbl, cnt))\n",
    "    return pd.DataFrame(rows,\n",
    "                        columns=[\"node_id\", \"branch_id\", \"true_label\", \"count\"])\n",
    "\n",
    "df_hist = hist_to_dataframe(histogram(tagged, open_nodes))\n",
    "display(df_hist)          # in a notebook – or print(df_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7dfedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\petru\\anaconda3\\envs\\bigdata_env\\lib\\site-packages (0.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# helper: pretty‑print / draw the learned tree\n",
    "# ------------------------------------------------------------------\n",
    "def show_tree(tree: dict, root_id: int = 0, graphviz: bool = False,\n",
    "              file_name: str = \"proximity_tree\") -> None:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree       : the dict that the loop filled  {node_id : TreeNode}\n",
    "    root_id    : normally 0\n",
    "    graphviz   : draw a .png with Graphviz in addition to console print\n",
    "    file_name  : base name for the .dot and .png files\n",
    "    \"\"\"\n",
    "    # ---- 1.  ASCII print -----------------------------------------\n",
    "    def _ascii(nid: int, indent: str = \"\"):\n",
    "        node = tree[nid]\n",
    "        if node.is_leaf:\n",
    "            print(f\"{indent}*[{nid}]  LEAF  → predict class {node.prediction}\")\n",
    "        else:\n",
    "            print(f\"{indent}*[{nid}]  split = nearest_exemplar\")\n",
    "            for br_id, child_id in node.children.items():\n",
    "                print(f\"{indent}  ├─ branch {br_id}\")\n",
    "                _ascii(child_id, indent + \"  │   \")\n",
    "\n",
    "    print(\"\\n===== Proximity‑Tree (depth ≤ {}) =====\".format(max_depth))\n",
    "    _ascii(root_id)\n",
    "    print(\"========================================\\n\")\n",
    "\n",
    "    # ---- 2.  optional Graphviz (.png) -----------------------------\n",
    "    if graphviz:\n",
    "        try:\n",
    "            import graphviz                              # pip install graphviz\n",
    "            dot = graphviz.Digraph(format=\"png\")\n",
    "            for nid, node in tree.items():\n",
    "                label = f\"{nid}\\\\nleaf→{node.prediction}\" if node.is_leaf \\\n",
    "                        else str(nid)\n",
    "                shape = \"box\" if node.is_leaf else \"ellipse\"\n",
    "                dot.node(str(nid), label, shape=shape)\n",
    "\n",
    "            for nid, node in tree.items():\n",
    "                for br, child in node.children.items():\n",
    "                    dot.edge(str(nid), str(child), label=str(br))\n",
    "\n",
    "            dot.render(file_name, cleanup=True)\n",
    "            print(f\"Graphviz output written to {file_name}.png\")\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"‑‑ Graphviz not installed; skipped picture generation ‑‑\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695cf90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Proximity‑Tree (depth ≤ 10) =====\n",
      "*[0]  split = nearest_exemplar\n",
      "  ├─ branch 2\n",
      "  │   *[1]  LEAF  → predict class 2\n",
      "  ├─ branch 3\n",
      "  │   *[2]  LEAF  → predict class 1\n",
      "  ├─ branch 4\n",
      "  │   *[3]  LEAF  → predict class 4\n",
      "  ├─ branch 1\n",
      "  │   *[4]  LEAF  → predict class 1\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# … your breadth‑first loop finished …\n",
    "\n",
    "show_tree(tree, graphviz=False)             # just ASCII\n",
    "# show_tree(tree, graphviz=True)            # ASCII + PNG (needs `pip install graphviz`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491a882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 0 \n",
      "   · {'node_id': 0, 'branch_id': 4, 'true_label': 4, 'dist_calc': 4.449719092257398}\n",
      "   · {'node_id': 0, 'branch_id': 4, 'true_label': 4, 'dist_calc': 0.0}\n",
      "   · {'node_id': 0, 'branch_id': 1, 'true_label': 1, 'dist_calc': 0.22360679774997902}\n",
      "  🌿Node 4 [from branch 1]\n",
      "  🌿Node 1 [from branch 2]\n",
      "     · {'node_id': 1, 'branch_id': 2, 'true_label': 1, 'dist_calc': 0.9746794344808963}\n",
      "     · {'node_id': 1, 'branch_id': 2, 'true_label': 2, 'dist_calc': 2.024845673131659}\n",
      "     · {'node_id': 1, 'branch_id': 2, 'true_label': 2, 'dist_calc': 2.0371548787463363}\n",
      "  🌿Node 2 [from branch 3]\n",
      "     · {'node_id': 2, 'branch_id': 3, 'true_label': 1, 'dist_calc': 0.6708203932499369}\n",
      "     · {'node_id': 2, 'branch_id': 3, 'true_label': 1, 'dist_calc': 1.3416407864998738}\n",
      "     · {'node_id': 2, 'branch_id': 3, 'true_label': 1, 'dist_calc': 0.6855654600401043}\n",
      "  🌿Node 3 [from branch 4]\n"
     ]
    }
   ],
   "source": [
    "def print_tree_with_points(tree, tagged_df, max_rows=5):\n",
    "    \"\"\"\n",
    "    Pretty‑print the tree and, under each node, show up to `max_rows`\n",
    "    example rows that sit in that node (taken from the *current* tagged DF).\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 1. helper to fetch rows of one node (returns a list of dicts)       #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def sample_rows(node_id, n=max_rows):\n",
    "        rows = (\n",
    "            tagged_df.where(F.col(\"node_id\") == node_id)\n",
    "                     .limit(n)                         # <‑‑ avoid huge output\n",
    "                     .collect()\n",
    "        )\n",
    "        return [r.asDict() for r in rows]\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 2. breadth‑first walk & print                                       #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    queue = collections.deque([(0, 0, None)])  # (node_id, depth, via_branch)\n",
    "\n",
    "    while queue:\n",
    "        nid, depth, via_br = queue.popleft()\n",
    "        node = tree[nid]\n",
    "\n",
    "        indent = \"  \" * depth\n",
    "        branch_info = f\"[from branch {via_br}]\" if via_br is not None else \"\"\n",
    "        leaf_flag   = \"🌿\" if node.is_leaf else \"\"\n",
    "        print(f\"{indent}{leaf_flag}Node {nid} {branch_info}\")\n",
    "\n",
    "        # ------------- show a few rows ---------------------------------\n",
    "        for row in sample_rows(nid):\n",
    "            print(f\"{indent}   · {row}\")\n",
    "\n",
    "        # ------------- enqueue children -------------------------------\n",
    "        for br_id, child_id in sorted(node.children.items()):\n",
    "            queue.append((child_id, depth + 1, br_id))\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# call it right after the training loop (when `tagged` & `tree` exist)\n",
    "print_tree_with_points(tree, tagged, max_rows=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ab4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════════╗\n",
    "# ║  Pretty‑print the tree AND show a few sample rows per (node,     ║\n",
    "# ║  branch) so you can see exactly which points flow where.         ║\n",
    "# ╚══════════════════════════════════════════════════════════════════╝\n",
    "def print_tree_with_points(tree, tagged_df, max_rows=5, show_branch_rows=True):\n",
    "    \"\"\"\n",
    "    Print every node top‑down; under each node print up to `max_rows`\n",
    "    sample rows that currently sit *inside that node*.\n",
    "\n",
    "    If `show_branch_rows=True` we also indent once more and show a few\n",
    "    rows *per branch* before they flow into their child node – handy to\n",
    "    verify the “routing” in an internal node.\n",
    "    \"\"\"\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────\n",
    "    # helpers\n",
    "    # ───────────────────────────────────────────────────────────\n",
    "    def sample_rows(node_id, branch_id=None, n=max_rows):\n",
    "        \"\"\"\n",
    "        Return at most `n` Row‑dicts that are inside *node_id*.\n",
    "        If `branch_id` is not None, we additionally filter that branch.\n",
    "        \"\"\"\n",
    "        cond = (F.col(\"node_id\") == node_id)\n",
    "        if branch_id is not None:\n",
    "            cond &= (F.col(\"branch_id\") == branch_id)\n",
    "\n",
    "        return (tagged_df\n",
    "                .where(cond)\n",
    "                .limit(n)              # avoid huge output\n",
    "                .collect())\n",
    "\n",
    "    # ───────────────────────────────────────────────────────────\n",
    "    # breadth‑first traversal (queue holds tuples)\n",
    "    # (node_id , depth , incoming_branch_id)\n",
    "    # ───────────────────────────────────────────────────────────\n",
    "    queue = collections.deque([(0, 0, None)])\n",
    "\n",
    "    while queue:\n",
    "        nid, depth, via_branch = queue.popleft()\n",
    "        node = tree[nid]\n",
    "\n",
    "        # ---------- headline for the node ----------\n",
    "        indent  = \"  \" * depth\n",
    "        leaf_fl = \"🌿\" if node.is_leaf else \"├\"\n",
    "        heading = f\"{indent}{leaf_fl} node {nid}\"\n",
    "        if via_branch is not None:\n",
    "            heading += f\"  (arrived via branch {via_branch})\"\n",
    "        if node.is_leaf:\n",
    "            heading += f\"  ➜ predict label {node.prediction}\"\n",
    "        print(heading)\n",
    "\n",
    "        # ---------- some sample rows that live in *this* node ----------\n",
    "        for r in sample_rows(nid):\n",
    "            print(f\"{indent}     · {r.asDict()}\")\n",
    "\n",
    "        # ---------- enqueue / optionally show per‑branch samples ----------\n",
    "        for br_id, child_id in sorted(node.children.items()):\n",
    "            if show_branch_rows and not node.is_leaf:\n",
    "                sub_indent = indent + \"  \"\n",
    "                print(f\"{sub_indent}branch {br_id} → child {child_id}\")\n",
    "                for r in sample_rows(nid, br_id):\n",
    "                    print(f\"{sub_indent}   · {r.asDict()}\")\n",
    "            queue.append((child_id, depth + 1, br_id))\n",
    "\n",
    "        print()            # blank line between siblings for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b56bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├ node 0\n",
      "     · {'node_id': 0, 'branch_id': 4, 'true_label': 4, 'dist_calc': 4.449719092257398}\n",
      "     · {'node_id': 0, 'branch_id': 4, 'true_label': 4, 'dist_calc': 0.0}\n",
      "     · {'node_id': 0, 'branch_id': 1, 'true_label': 1, 'dist_calc': 0.22360679774997902}\n",
      "  branch 1 → child 4\n",
      "     · {'node_id': 0, 'branch_id': 1, 'true_label': 1, 'dist_calc': 0.22360679774997902}\n",
      "     · {'node_id': 0, 'branch_id': 1, 'true_label': 1, 'dist_calc': 0.0}\n",
      "     · {'node_id': 0, 'branch_id': 1, 'true_label': 1, 'dist_calc': 0.3872983346207417}\n",
      "  branch 2 → child 1\n",
      "  branch 3 → child 2\n",
      "  branch 4 → child 3\n",
      "     · {'node_id': 0, 'branch_id': 4, 'true_label': 4, 'dist_calc': 4.449719092257398}\n",
      "     · {'node_id': 0, 'branch_id': 4, 'true_label': 4, 'dist_calc': 0.0}\n",
      "     · {'node_id': 0, 'branch_id': 4, 'true_label': 4, 'dist_calc': 4.748684028233506}\n",
      "\n",
      "  🌿 node 4  (arrived via branch 1)  ➜ predict label 1\n",
      "\n",
      "  🌿 node 1  (arrived via branch 2)  ➜ predict label 2\n",
      "       · {'node_id': 1, 'branch_id': 2, 'true_label': 1, 'dist_calc': 0.9746794344808963}\n",
      "       · {'node_id': 1, 'branch_id': 2, 'true_label': 2, 'dist_calc': 2.024845673131659}\n",
      "       · {'node_id': 1, 'branch_id': 2, 'true_label': 2, 'dist_calc': 2.0371548787463363}\n",
      "\n",
      "  🌿 node 2  (arrived via branch 3)  ➜ predict label 1\n",
      "       · {'node_id': 2, 'branch_id': 3, 'true_label': 1, 'dist_calc': 0.6708203932499369}\n",
      "       · {'node_id': 2, 'branch_id': 3, 'true_label': 1, 'dist_calc': 1.3416407864998738}\n",
      "       · {'node_id': 2, 'branch_id': 3, 'true_label': 1, 'dist_calc': 0.6855654600401043}\n",
      "\n",
      "  🌿 node 3  (arrived via branch 4)  ➜ predict label 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tree_with_points(tree, tagged, max_rows=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7543b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree_sideways(tree, node_id=0, indent=0, gap=6):\n",
    "    \"\"\"\n",
    "    Recursively prints the whole Proximity‑Tree sideways.\n",
    "    ─  root is on the *left*, leaves on the *right*.\n",
    "    ─  ‘In‑order’ = visit left‑half children ▸ node ▸ right‑half children\n",
    "      (if you have more than two children we simply take the lower‑index\n",
    "       half as the “left” side).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree     dict {node_id ➜ TreeNode}\n",
    "    node_id  int   id of the current node (start with the root, 0)\n",
    "    indent   int   how many spaces to shift this subtree to the right\n",
    "    gap      int   horizontal spacing between levels (tweak to taste)\n",
    "    \"\"\"\n",
    "    node = tree[node_id]\n",
    "\n",
    "    # sort branch labels so the picture is deterministic\n",
    "    children = sorted(node.children.items())           # [(br, child_id) …]\n",
    "\n",
    "    # **split** children into a ‘left’ and ‘right’ half for in‑order\n",
    "    mid = len(children) // 2\n",
    "    left  = children[:mid]\n",
    "    right = children[mid:]\n",
    "\n",
    "    # ---------- print RIGHT half first (so it ends up *above* the parent)\n",
    "    for br, c_id in right:\n",
    "        print_tree_sideways(tree, c_id, indent + gap, gap)\n",
    "\n",
    "    # ---------- print the current node\n",
    "    label = f\"[{node.node_id}]\"\n",
    "    if node.is_leaf:\n",
    "        label += f\"⟶{node.prediction}\"\n",
    "    print(\" \" * indent + label)\n",
    "\n",
    "    # ---------- print LEFT half afterwards (goes *below* the parent)\n",
    "    for br, c_id in left:\n",
    "        print_tree_sideways(tree, c_id, indent + gap, gap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ee775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884c572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [2]⟶1\n",
      "      [3]⟶4\n",
      "[0]\n",
      "      [4]⟶1\n",
      "      [1]⟶2\n"
     ]
    }
   ],
   "source": [
    "print_tree_sideways(tree)        # `tree` is the dict you built earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              [0]\n",
      "     [4→1]           [1→2]           [2→1]           [3→4]\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import deque\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# pretty‑printer  (root on top, breadth‑first levels)\n",
    "# ------------------------------------------------------------\n",
    "def print_tree_topdown(tree, root_id=0, gap=3):\n",
    "    \"\"\"\n",
    "    Nicely print a Proximity‑Tree in 2‑D (root at top).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree     dict { node_id -> TreeNode }\n",
    "    root_id  int   where to start (default = 0)\n",
    "    gap      int   horizontal space between sibling sub‑trees\n",
    "    \"\"\"\n",
    "\n",
    "    # 1.  Gather each level with a simple BFS\n",
    "    levels = []                       # list of [node_id ...] per depth\n",
    "    q = deque([(root_id, 0)])         # (node_id, depth)\n",
    "    while q:\n",
    "        nid, d = q.popleft()\n",
    "        if len(levels) <= d:\n",
    "            levels.append([])\n",
    "        levels[d].append(nid)\n",
    "\n",
    "        # enqueue children in *branch order* for stability\n",
    "        for _, cid in sorted(tree[nid].children.items()):\n",
    "            q.append((cid, d+1))\n",
    "\n",
    "    # 2.  Determine the printable label for every node once\n",
    "    labels = {}\n",
    "    for nid, node in tree.items():\n",
    "        if node.is_leaf:\n",
    "            labels[nid] = f\"[{nid}→{node.prediction}]\"\n",
    "        else:\n",
    "            labels[nid] = f\"[{nid}]\"\n",
    "\n",
    "    # 3.  Compute width of the bottom layer → overall canvas width\n",
    "    bottom = levels[-1]\n",
    "    max_label_len = max(len(labels[n]) for n in tree)\n",
    "    cell_w = max_label_len + gap                    # width per “slot”\n",
    "    width  = len(bottom) * cell_w\n",
    "\n",
    "    # 4.  Print each level centred in its allotted range\n",
    "    for depth, layer in enumerate(levels):\n",
    "        n_slots = 2 ** depth                        # binary tree assumption\n",
    "        slot_w  = width // n_slots                  # width per logical slot\n",
    "\n",
    "        line = \"\"\n",
    "        for nid in layer:\n",
    "            pos  = levels[depth].index(nid)         # index in this layer\n",
    "            cell = labels[nid].center(slot_w)\n",
    "            line += cell\n",
    "        print(line.rstrip())\n",
    "\n",
    "    # 5.  Optional: print an underline for visual separation\n",
    "    print(\"=\" * width)\n",
    "print_tree_topdown(tree)          # <-- just call it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba16a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth 0: (0)\n",
      "depth 1: (4) (1) (2) (3)\n"
     ]
    }
   ],
   "source": [
    "def print_pyramid(tree, root_id=0):\n",
    "    \"\"\"\n",
    "    Show the tree breadth‑first with the root centred on the first line.\n",
    "\n",
    "    Example for a 3‑layer tree:\n",
    "        (0)\n",
    "      (1) (2)\n",
    "     (3)(4)(5)\n",
    "\n",
    "    Only node‑ids are displayed for brevity; adapt as you like.\n",
    "    \"\"\"\n",
    "    # --- collect nodes per depth ------------------------------------------\n",
    "    by_depth = collections.defaultdict(list)\n",
    "    queue    = collections.deque([(root_id, 0)])          # (node_id, depth)\n",
    "\n",
    "    while queue:\n",
    "        nid, depth = queue.popleft()\n",
    "        by_depth[depth].append(nid)\n",
    "\n",
    "        # enqueue children (order by branch‑id just for nicer output)\n",
    "        for br_id in sorted(tree[nid].children):\n",
    "            queue.append((tree[nid].children[br_id], depth + 1))\n",
    "\n",
    "    # --- pretty‑print ------------------------------------------------------\n",
    "    max_depth   = max(by_depth)\n",
    "    width_first = len(\" \".join(f\"({n})\" for n in by_depth[0]))  # width of top line\n",
    "\n",
    "    for d in range(max_depth + 1):\n",
    "        nodes   = \" \".join(f\"({n})\" for n in by_depth[d])\n",
    "        pad     = \" \" * ((width_first - len(nodes)) // 2)\n",
    "        print(f\"depth {d}: {pad}{nodes}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# call it after the training loop\n",
    "print_pyramid(tree)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
