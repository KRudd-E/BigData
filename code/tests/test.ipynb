{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.distances import distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ceb231",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4168200959.py, line 64)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtrained_tree = build_tree(data, , max_depth=5)\u001b[39m\n                                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from aeon.distances import distance\n",
    "\n",
    "def dtw_distance(ts1, ts2):\n",
    "    \"\"\"\n",
    "    Compute the DTW distance between two time series ts1 and ts2.\n",
    "    Parameters:\n",
    "        ts1, ts2: Arrays or lists of numeric values representing the time series.\n",
    "    Returns:\n",
    "        The DTW distance.\n",
    "    \"\"\"\n",
    "    n, m = len(ts1), len(ts2)\n",
    "    dtw_matrix = np.full((n + 1, m + 1), np.inf)\n",
    "    dtw_matrix[0, 0] = 0.0\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            cost = abs(ts1[i - 1] - ts2[j - 1])\n",
    "            dtw_matrix[i, j] = cost + min(\n",
    "                dtw_matrix[i - 1, j],    # insertion\n",
    "                dtw_matrix[i, j - 1],    # deletion\n",
    "                dtw_matrix[i - 1, j - 1] # match\n",
    "            )\n",
    "    return dtw_matrix[n, m]\n",
    "\n",
    "def calculate_best_split(distance):\n",
    "    # This function will calculate the best split based on DTW distance\n",
    "    # It will return a list of candidate splitters\n",
    "    # Each candidate splitter will be a tuple (feature_index, threshold, impurity)\n",
    "    # The impurity can be calculated using Gini impurity or entropy\n",
    "    \n",
    "    # Placeholder for the best split calculation\n",
    "    # You would implement your logic here to find the best feature and threshold based on DTW distance\n",
    "    pass\n",
    "\n",
    "\n",
    "def find_best_splitter(partitioned_data):\n",
    "    # This function will be called on each partition of the RDD and will be sent back to the driver\n",
    "    \n",
    "    \n",
    "    # This function will find the best splitter for the data in the partition\n",
    "    # It will return a list of candidate splitters\n",
    "    # Each candidate splitter will be a tuple (feature_index, threshold, impurity)\n",
    "    # The impurity can be calculated using Gini impurity or entropy\n",
    "    \n",
    "    exemplars = np.random.choice(partitioned_data, size=5, replace=False)\n",
    "\n",
    "     for ex in exemplars:\n",
    "        # For each exemplar, calculate the DTW distance to every other time series\n",
    "        distances = [dtw_distance(exemplar, d) for d in partitioned_data if not np.array_equal(d, exemplar)]\n",
    "        \n",
    "        best_gini_gain = calculate_best_split(distance)\n",
    "        \n",
    "        # Here you would determine the best feature and threshold based on impurity measures\n",
    "        # For the sake of the example, let's simply store the average distance as the \"impurity\"\n",
    "        avg_distance = np.mean(distances)\n",
    "        # Append a dummy tuple: (feature_index, threshold, impurity)\n",
    "        candidate_splitters.append((None, None, avg_distance))\n",
    "        \n",
    "        candidate_splitters = {\n",
    "            'exemplar': exemplar,\n",
    "            'best_split': None,\n",
    "            'best_gini_gain': None, \n",
    "        }\n",
    "    \n",
    "    return candidate_splitters\n",
    "    \n",
    "def _repartition_data(self, new_parts,  df: DataFrame) -> DataFrame:\n",
    "        if \"num_partitions\" in self.config:           \n",
    "            self.logger.info(f\"Repartitioning data to {new_parts} parts\")\n",
    "            return df.repartition(new_parts)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "def build_global_tree(data,  max_depth):\n",
    "    \n",
    "    number_of_partitions = 5\n",
    "    # Repartition the data if our config says so\n",
    "    df = self._repartition_data(df, number_of_partitions )\n",
    "    candidate_rdd = data.mapPartitions(find_best_splitter() )\n",
    "    all_candidates = candidate_rdd.collect()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def main ():\n",
    "        \n",
    "    data = sc.textFile(\"hdfs:///user/hduser/iris.csv\")\n",
    "\n",
    "    trained_tree = build_global_tree(data, , max_depth=5)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
